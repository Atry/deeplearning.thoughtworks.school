{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is a great tool that helps us efficiently summarize inherent patterns from tons of input data. I'd like to introduce DeepLearning.scala by letting the framework learn the common difference from Arithmetic progression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "**Input**:\n",
    " Arithmetic progression(AP) as:\n",
    "``` val input: INDArray = Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray``` \n",
    "\n",
    "**Output**: \n",
    " Common Difference of the certain AP as: \n",
    "```val expectedOutput: INDArray = Array(Array(1), Array(3), Array(2)).toNDArray```\n",
    "\n",
    "So here we want DeepLearning.scala to learn the common difference from the AP, i.e. ```{1} from {0, 1, 2} ``` \n",
    "in which `2-1 = 1-0 = 1 `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install DeepLearning.scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepLearning.scala is hosted on Maven Central repository.\n",
    "\n",
    "You can use magic imports in [jupyter-scala](https://github.com/alexarchambault/jupyter-scala) or [Ammonite-REPL](http://www.lihaoyi.com/Ammonite/#Ammonite-REPL) to download DeepLearning.scala and its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$plugin.$                                            \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \u001b[39m"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`com.thoughtworks.deeplearning::jupyter-differentiable:2.0.0-M1`\n",
    "import $plugin.$ivy.`org.scalamacros:paradise_2.11.11:2.1.0`\n",
    "\n",
    "import $ivy.`org.nd4j:nd4j-native-platform:0.7.2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use [sbt](http://www.scala-sbt.org), please add the following settings into your `build.sbt`:\n",
    "\n",
    "``` scala\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiable\" % \"latest.release\"\n",
    "\n",
    "addCompilerPlugin(\"org.scalamacros\" % \"paradise\" % \"2.1.0\" cross CrossVersion.full)\n",
    "\n",
    "fork := true\n",
    "\n",
    "scalaVersion := \"2.11.11\"\n",
    "```\n",
    "\n",
    "Note that this example must run on Scala 2.11.11 because [nd4s](http://nd4j.org/scala) does not support Scala 2.12. Make sure there is not a setting like `scalaVersion := \"2.12.x\"` in your `build.sbt`.\n",
    "\n",
    "See [Scaladex](https://index.scala-lang.org/thoughtworksinc/deeplearning.scala) to install DeepLearning.scala in other build tools!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you may want to import classes in DeepLearning.scala and its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.math._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.jupyter.differentiable.Any._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.differentiable.INDArray.{\n",
       "  Optimizer => INDArrayOptimizer\n",
       "}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mINDArrayOptimizer.LearningRate\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.jupyter.differentiable.INDArray.implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.each.Monadic._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.raii.asynchronous.Do\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.jupyter.differentiable.Double._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.jupyter.differentiable.Double.implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Tape\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.jupyter.differentiable\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.factory.Nd4j\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4s.Implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.concurrent.ExecutionContext.Implicits.global\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscalaz.concurrent.Task\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscalaz.{-\\/, \\/, \\/-}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscalaz.std.vector._\u001b[39m"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.thoughtworks.deeplearning.math._\n",
    "import com.thoughtworks.deeplearning.jupyter.differentiable.Any._\n",
    "import com.thoughtworks.deeplearning.jupyter.differentiable.INDArray.{\n",
    "  Optimizer => INDArrayOptimizer\n",
    "}\n",
    "import INDArrayOptimizer.LearningRate\n",
    "import com.thoughtworks.deeplearning.jupyter.differentiable.INDArray.implicits._\n",
    "import com.thoughtworks.each.Monadic._\n",
    "import com.thoughtworks.raii.asynchronous.Do\n",
    "import com.thoughtworks.deeplearning.jupyter.differentiable.Double._\n",
    "import com.thoughtworks.deeplearning.jupyter.differentiable.Double.implicits._\n",
    "import com.thoughtworks.deeplearning.Tape\n",
    "import com.thoughtworks.deeplearning.jupyter.differentiable\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.nd4s.Implicits._\n",
    "import scala.concurrent.ExecutionContext.Implicits.global\n",
    "import scalaz.concurrent.Task\n",
    "import scalaz.{-\\/, \\/, \\/-}\n",
    "import scalaz.std.vector._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design your neural network\n",
    "\n",
    "DeepLearning.scala is also a language that we can use to create complex neural networks.\n",
    "\n",
    "\n",
    "In the following sections, you will learn:\n",
    " * how to create your neural network\n",
    " * how to train your neural network\n",
    " * how to predict your neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your neural network\n",
    "\n",
    "Same as the definition of a normal Scala function, the definition of neural network consists of a type definition for its parameter, a type definition for its return value, and a body that contains mathematical formulas, function-calls, and control flows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight Intialization \n",
    "\n",
    "We will create a trainable neural network.\n",
    "It means that some variables in the neural network can be changed automatically according to some goals. Those variables are called `weight`.\n",
    "You can create weight variables via `toWeight` method, given its initial value.\n",
    "\n",
    "In order to create a weight, you must create an `Optimizer`, which contains the rule that manages how the weight changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36moptimizer\u001b[39m"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit def optimizer: INDArrayOptimizer = new LearningRate {\n",
    "  def currentLearningRate() = 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mweight\u001b[39m: \u001b[32mDo\u001b[39m[\u001b[32mdifferentiable\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mINDArray\u001b[39m.\u001b[32mINDArrayTape\u001b[39m] = Suspend(<function0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weight = (Nd4j.randn(3, 1) / scala.math.sqrt(3.0)).toWeight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define your neural network\n",
    "Your neural network is just a normal scala function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmyNeuralNetwork\u001b[39m"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myNeuralNetwork(input: INDArray): differentiable.INDArray = {\n",
    "  dot(input, weight)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your Neural Network\n",
    "\n",
    "You have learned that weight will be automatically changed due to some goals.\n",
    "\n",
    "In DeepLearning.scala, when we train a neural network, our goal should always be minimizing the absolute of the return value.\n",
    "\n",
    "For example, if someone repeatedly call `train(myNeuralNetwork(Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray))`,\n",
    "the neural network would try to minimize `input dot weight`.\n",
    "Soon `weight` would become an array of zeros in order to make `input dot weight` zeros,\n",
    "and `predict(myNeuralNetwork(Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray))` would return `Array(Array(0), Array(0), Array(0)).toNDArray`.\n",
    "\n",
    "What if you expect `predict(myNeuralNetwork(Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray))` to return `Array(Array(1), Array(3), Array(2)).toNDArray`?\n",
    "\n",
    "You can create another neural network that evaluates how far between the result of `myNeuralNetwork` and your expectation. The new neural network is usually called **loss function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mlossFunction\u001b[39m"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lossFunction(input: INDArray,\n",
    "                 expectOutput: INDArray): differentiable.Double = {\n",
    "  sumT(abs(myNeuralNetwork(input) - expectOutput))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the `lossFunction` get trained continuously, its return value will be close to zero, and the result of  `myNeuralNetwork` must be close to the expected result at the same time.\n",
    "\n",
    "Note the `lossFunction` accepts a `input` and `expectOutput` as its parameter.\n",
    "The first array is the input data used to train the neural network, and the second array is the expected output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a plot to show how the loss changed during iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \u001b[39m"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.plotly-scala::plotly-jupyter-scala:0.3.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.JupyterScala._\u001b[39m"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.JupyterScala._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mpolyLoss\u001b[39m"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def polyLoss(lossSeq: IndexedSeq[Double]): Unit = {\n",
    "  plotly.JupyterScala.init()\n",
    "\n",
    "  val plot = Seq(\n",
    "    Scatter(lossSeq.indices, lossSeq)\n",
    "  )\n",
    "\n",
    "  plot.plot(\n",
    "    title = \"loss by time\"\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we hard-code some data to train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36minput\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 1.00, 2.00],\n",
       " [3.00, 6.00, 9.00],\n",
       " [13.00, 15.00, 17.00]]\n",
       "\u001b[36mexpectedOutput\u001b[39m: \u001b[32mINDArray\u001b[39m = [1.00, 3.00, 2.00]\n",
       "\u001b[36mtrainTask\u001b[39m: \u001b[32mTask\u001b[39m[\u001b[32mUnit\u001b[39m] = scalaz.concurrent.Task@1d67f702"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val input: INDArray = Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray\n",
    "\n",
    "val expectedOutput: INDArray = Array(Array(1), Array(3), Array(2)).toNDArray\n",
    "\n",
    "@monadic[Task]\n",
    "val trainTask: Task[Unit] = {\n",
    "\n",
    "  val lossSeq = for (_ <- (1 to 400).toVector) yield {\n",
    "    train(lossFunction(input, expectedOutput)).each\n",
    "  }\n",
    "\n",
    "  polyLoss(lossSeq)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@monadic` and `throwableMonadic` is a syntax sugar provide by [each](https://github.com/ThoughtWorksInc/each).\n",
    "\n",
    "After those iterations, the loss should close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict  your Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <script type=\"text/javascript\">\n",
       "        require.config({\n",
       "  paths: {\n",
       "    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',\n",
       "    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min'\n",
       "  },\n",
       "\n",
       "  shim: {\n",
       "    plotly: {\n",
       "      deps: ['d3', 'jquery'],\n",
       "      exports: 'plotly'\n",
       "    }\n",
       "  }\n",
       "});\n",
       "        \n",
       "\n",
       "        require(['plotly'], function(Plotly) {\n",
       "          window.Plotly = Plotly;\n",
       "        });\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-1858318957\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "requirejs([\"plotly\"], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"type\":\"scatter\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0],\"y\":[14.311918258666992,12.787918090820312,11.263917922973633,9.739916801452637,8.215916633605957,6.761713981628418,5.529714107513428,4.297715187072754,3.0657150745391846,2.5271148681640625,2.327115058898926,2.127115249633789,1.9271148443222046,1.727115511894226,1.5271151065826416,1.804081916809082,1.8311150074005127,1.631115436553955,1.4311161041259766,1.7920804023742676,1.735115647315979,1.5351160764694214,1.3351163864135742,1.7800776958465576,1.639116644859314,1.4391162395477295,1.2640762329101562,1.7431169748306274,1.5431172847747803,1.3431172370910645,1.2520725727081299,1.647118091583252,1.4471185207366943,1.2471179962158203,1.2400703430175781,1.5511186122894287,1.351119041442871,1.1511194705963135,1.2280678749084473,1.4551200866699219,1.255120038986206,1.0551199913024902,1.2160648107528687,1.3591198921203613,1.1591205596923828,0.9591211080551147,1.204063057899475,1.2631208896636963,1.06312096118927,0.863120436668396,1.192059874534607,1.1671215295791626,0.967121958732605,0.7671216726303101,1.1800568103790283,1.0711220502853394,0.8711215257644653,0.6711228489875793,1.1680548191070557,0.9751230478286743,0.7751227021217346,0.6520535945892334,1.0791231393814087,0.8791242241859436,0.6791236400604248,0.6400508880615234,1.0337433815002441,0.5191233158111572,0.9880499243736267,0.823124885559082,0.623124361038208,0.47204774618148804,1.145744800567627,0.46312415599823,0.8200474977493286,0.8177456259727478,0.6640466451644897,0.9537457227706909,0.5080476403236389,1.0897465944290161,0.38312506675720215,0.8560454845428467,0.7617467045783997,0.7000455856323242,0.8977475166320801,0.5440446734428406,1.0337483882904053,0.38804471492767334,1.1697478294372559,0.34312498569488525,0.7360435724258423,0.8417493104934692,0.580043613910675,0.977749228477478,0.4240427017211914,1.113749623298645,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748]};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"loss by time\"};\n",
       "\n",
       "  Plotly.plot('plot-1858318957', data, layout);\n",
       "})();\n",
       "});\n",
       "      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93, 2.81, 2.00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mpredictResult\u001b[39m: \u001b[32mTask\u001b[39m[\u001b[32mTape\u001b[39m.\u001b[32m<refinement>\u001b[39m.this.type.\u001b[32mData\u001b[39m] = scalaz.concurrent.Task@27fd8d73"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictResult = throwableMonadic[Task] {\n",
    "  trainTask.each\n",
    "  predict(myNeuralNetwork(input)).each\n",
    "}\n",
    "\n",
    "predictResult.unsafePerformSyncAttempt match {\n",
    "  case -\\/(e) => throw e\n",
    "  case \\/-(result) =>\n",
    "    println(result)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this article, you have learned:\n",
    "* to create neural networks dealing with complex data structures like `Double` and `INDArray` like ordinary programming language\n",
    "* to train your neural network\n",
    "* to predict your neural network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
