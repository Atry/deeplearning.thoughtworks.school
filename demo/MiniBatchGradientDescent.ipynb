{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "During large-scale date training, the order of magnitude of data can reach millions. If a parameter is acquired via the computation of the whole training set, the update speed will be too slow. To solve this problem, a common used method is [Mini-Batch Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) which computes mini-batche data in the training set, resulting faster training of parameters in a neural network.\n",
    "\n",
    "In this article, we will first define a softmax classifier, then use the training set of [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) to train this neural network, and finally use the test set to verify the accuracy of the neural network. The difference is that we will use Mini-Batch Gradient Descent, thus the accuracy of the neural network can reach 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## This article is the same as the last article. Import dependencies and build the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling ReadCIFAR10ToNDArray.sc\n",
      "Compiling Utils.sc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$plugin.$                                                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                       \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                       \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                          \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                         \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                         \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                            \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.{FileInputStream, InputStream}\n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableHList._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableDouble._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableAny._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Layer.Tape\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.{\n",
       "  DifferentiableHList,\n",
       "  DifferentiableINDArray,\n",
       "  Layer,\n",
       "  Symbolic\n",
       "}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathFunctions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathMethods./\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathOps\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.factory.Nd4j\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.ops.transforms.Transforms\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4s.Implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mshapeless._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.JupyterScala._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.immutable.IndexedSeq\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$    \n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36msoftmax\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36moptimizer\u001b[39m\n",
       "\u001b[36mNumberOfClasses\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m\n",
       "\u001b[36mNumberOfPixels\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m3072\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcreateMyNeuralNetwork\u001b[39m\n",
       "\u001b[36mmyNeuralNetwork\u001b[39m: (\u001b[32mcom\u001b[39m.\u001b[32mthoughtworks\u001b[39m.\u001b[32mdeeplearning\u001b[39m.\u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m[\u001b[32mINDArray\u001b[39m]{type OutputData = org.nd4j.linalg.api.ndarray.INDArray;type OutputDelta = org.nd4j.linalg.api.ndarray.INDArray;type InputData = org.nd4j.linalg.api.ndarray.INDArray;type InputDelta = org.nd4j.linalg.api.ndarray.INDArray})#\u001b[32m@\u001b[39m = Compose(MultiplyINDArray(Exp(Identity()),Reciprocal(Sum(Exp(Identity()),WrappedArray(1)))),Dot(Identity(),Weight([[-0.00, 0.00, -0.00, -0.00, -0.00, 0.00, 0.00,\u001b[33m...\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mlossFunction\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $plugin.$ivy.`com.thoughtworks.implicit-dependent-type::implicit-dependent-type:2.0.0`\n",
    "\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableany:1.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablenothing:1.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableseq:1.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiabledouble:1.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablefloat:1.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablehlist:1.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablecoproduct:1.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableindarray:1.0.0`\n",
    "import $ivy.`org.nd4j:nd4j-native-platform:0.7.2`\n",
    "import $ivy.`org.rauschig:jarchivelib:0.5.0`\n",
    "\n",
    "import $ivy.`org.plotly-scala::plotly-jupyter-scala:0.3.0`\n",
    "\n",
    "import java.io.{FileInputStream, InputStream}\n",
    "\n",
    "\n",
    "import com.thoughtworks.deeplearning\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import com.thoughtworks.deeplearning.DifferentiableHList._\n",
    "import com.thoughtworks.deeplearning.DifferentiableDouble._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray._\n",
    "import com.thoughtworks.deeplearning.DifferentiableAny._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
    "import com.thoughtworks.deeplearning.Layer.Tape\n",
    "import com.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
    "import com.thoughtworks.deeplearning.Symbolic._\n",
    "import com.thoughtworks.deeplearning.{\n",
    "  DifferentiableHList,\n",
    "  DifferentiableINDArray,\n",
    "  Layer,\n",
    "  Symbolic\n",
    "}\n",
    "import com.thoughtworks.deeplearning.Poly.MathFunctions._\n",
    "import com.thoughtworks.deeplearning.Poly.MathMethods./\n",
    "import com.thoughtworks.deeplearning.Poly.MathOps\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
    "import org.nd4j.linalg.ops.transforms.Transforms\n",
    "import org.nd4s.Implicits._\n",
    "import shapeless._\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.JupyterScala._\n",
    "\n",
    "import scala.collection.immutable.IndexedSeq\n",
    "import scala.util.Random\n",
    "\n",
    "pprintConfig() = pprintConfig().copy(height = 2)\n",
    "\n",
    "import $file.ReadCIFAR10ToNDArray\n",
    "import $file.Utils\n",
    "\n",
    "def softmax(implicit scores: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val expScores = exp(scores)\n",
    "  expScores / expScores.sum(1)\n",
    "}\n",
    "\n",
    "implicit def optimizer: Optimizer = new LearningRate {\n",
    "  def currentLearningRate() = 0.00001\n",
    "}\n",
    "\n",
    "//10 label of CIFAR10 images(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)\n",
    "val NumberOfClasses: Int = 10\n",
    "val NumberOfPixels: Int = 3072\n",
    "\n",
    "def createMyNeuralNetwork(implicit input: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val initialValueOfWeight = Nd4j.randn(NumberOfPixels, NumberOfClasses) * 0.001\n",
    "  val weight: INDArray @Symbolic = initialValueOfWeight.toWeight\n",
    "  val result: INDArray @Symbolic = input dot weight\n",
    "  softmax.compose(result)\n",
    "}\n",
    "val myNeuralNetwork = createMyNeuralNetwork\n",
    "\n",
    "def lossFunction(implicit pair: (INDArray :: INDArray :: HNil) @Symbolic): Double @Symbolic = {\n",
    "  val input = pair.head\n",
    "  val expectedOutput = pair.tail.head\n",
    "  val probabilities = myNeuralNetwork.compose(input)\n",
    "\n",
    "  -(expectedOutput * log(probabilities)).mean\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the previous article, we need to train the neural network. However, the difference is that the training data in this article are read randomly, and in the last article one batch of data set is used for its repeated training. Train the neural network and observe the change of `loss` in each training. The trend of `loss` is decrease, however, we need to find out whether it decreases every time. (The future is bright, the road is tortuous.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and process the data according to the array, and then train the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mMiniBatchSize\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m256\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtrainData\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val MiniBatchSize = 256\n",
    "\n",
    "def trainData(randomIndexArray: Array[Int]): Double = {\n",
    "  val trainNDArray :: expectLabel :: shapeless.HNil =\n",
    "    ReadCIFAR10ToNDArray.getSGDTrainNDArray(randomIndexArray)\n",
    "\n",
    "  val input =\n",
    "    trainNDArray.reshape(MiniBatchSize, NumberOfPixels)\n",
    "\n",
    "  val expectLabelVectorized =\n",
    "    Utils.makeVectorized(expectLabel, NumberOfClasses)\n",
    "\n",
    "  lossFunction.train(input :: expectLabelVectorized :: HNil)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disrupt the order of a sequence once for each [epoch](http://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks), and generate the random arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 1 loss is :0.21291561126708985\n",
      "at epoch 2 loss is :0.20125489234924315\n",
      "at epoch 3 loss is :0.19702608585357667\n",
      "at epoch 4 loss is :0.1977776288986206\n",
      "at epoch 5 loss is :0.19166061878204346\n",
      "at epoch 6 loss is :0.198227858543396\n",
      "at epoch 7 loss is :0.1935807228088379\n",
      "at epoch 8 loss is :0.20156009197235109\n",
      "at epoch 9 loss is :0.18221101760864258\n",
      "at epoch 10 loss is :0.18732290267944335\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <script type=\"text/javascript\">\n",
       "        require.config({\n",
       "  paths: {\n",
       "    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',\n",
       "    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min'\n",
       "  },\n",
       "\n",
       "  shim: {\n",
       "    plotly: {\n",
       "      deps: ['d3', 'jquery'],\n",
       "      exports: 'plotly'\n",
       "    }\n",
       "  }\n",
       "});\n",
       "        \n",
       "\n",
       "        require(['plotly'], function(Plotly) {\n",
       "          window.Plotly = Plotly;\n",
       "        });\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-314683916\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "requirejs([\"plotly\"], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"type\":\"scatter\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0,1001.0,1002.0,1003.0,1004.0,1005.0,1006.0,1007.0,1008.0,1009.0,1010.0,1011.0,1012.0,1013.0,1014.0,1015.0,1016.0,1017.0,1018.0,1019.0,1020.0,1021.0,1022.0,1023.0,1024.0,1025.0,1026.0,1027.0,1028.0,1029.0,1030.0,1031.0,1032.0,1033.0,1034.0,1035.0,1036.0,1037.0,1038.0,1039.0,1040.0,1041.0,1042.0,1043.0,1044.0,1045.0,1046.0,1047.0,1048.0,1049.0,1050.0,1051.0,1052.0,1053.0,1054.0,1055.0,1056.0,1057.0,1058.0,1059.0,1060.0,1061.0,1062.0,1063.0,1064.0,1065.0,1066.0,1067.0,1068.0,1069.0,1070.0,1071.0,1072.0,1073.0,1074.0,1075.0,1076.0,1077.0,1078.0,1079.0,1080.0,1081.0,1082.0,1083.0,1084.0,1085.0,1086.0,1087.0,1088.0,1089.0,1090.0,1091.0,1092.0,1093.0,1094.0,1095.0,1096.0,1097.0,1098.0,1099.0,1100.0,1101.0,1102.0,1103.0,1104.0,1105.0,1106.0,1107.0,1108.0,1109.0,1110.0,1111.0,1112.0,1113.0,1114.0,1115.0,1116.0,1117.0,1118.0,1119.0,1120.0,1121.0,1122.0,1123.0,1124.0,1125.0,1126.0,1127.0,1128.0,1129.0,1130.0,1131.0,1132.0,1133.0,1134.0,1135.0,1136.0,1137.0,1138.0,1139.0,1140.0,1141.0,1142.0,1143.0,1144.0,1145.0,1146.0,1147.0,1148.0,1149.0,1150.0,1151.0,1152.0,1153.0,1154.0,1155.0,1156.0,1157.0,1158.0,1159.0,1160.0,1161.0,1162.0,1163.0,1164.0,1165.0,1166.0,1167.0,1168.0,1169.0,1170.0,1171.0,1172.0,1173.0,1174.0,1175.0,1176.0,1177.0,1178.0,1179.0,1180.0,1181.0,1182.0,1183.0,1184.0,1185.0,1186.0,1187.0,1188.0,1189.0,1190.0,1191.0,1192.0,1193.0,1194.0,1195.0,1196.0,1197.0,1198.0,1199.0,1200.0,1201.0,1202.0,1203.0,1204.0,1205.0,1206.0,1207.0,1208.0,1209.0,1210.0,1211.0,1212.0,1213.0,1214.0,1215.0,1216.0,1217.0,1218.0,1219.0,1220.0,1221.0,1222.0,1223.0,1224.0,1225.0,1226.0,1227.0,1228.0,1229.0,1230.0,1231.0,1232.0,1233.0,1234.0,1235.0,1236.0,1237.0,1238.0,1239.0,1240.0,1241.0,1242.0,1243.0,1244.0,1245.0,1246.0,1247.0,1248.0,1249.0,1250.0,1251.0,1252.0,1253.0,1254.0,1255.0,1256.0,1257.0,1258.0,1259.0,1260.0,1261.0,1262.0,1263.0,1264.0,1265.0,1266.0,1267.0,1268.0,1269.0,1270.0,1271.0,1272.0,1273.0,1274.0,1275.0,1276.0,1277.0,1278.0,1279.0,1280.0,1281.0,1282.0,1283.0,1284.0,1285.0,1286.0,1287.0,1288.0,1289.0,1290.0,1291.0,1292.0,1293.0,1294.0,1295.0,1296.0,1297.0,1298.0,1299.0,1300.0,1301.0,1302.0,1303.0,1304.0,1305.0,1306.0,1307.0,1308.0,1309.0,1310.0,1311.0,1312.0,1313.0,1314.0,1315.0,1316.0,1317.0,1318.0,1319.0,1320.0,1321.0,1322.0,1323.0,1324.0,1325.0,1326.0,1327.0,1328.0,1329.0,1330.0,1331.0,1332.0,1333.0,1334.0,1335.0,1336.0,1337.0,1338.0,1339.0,1340.0,1341.0,1342.0,1343.0,1344.0,1345.0,1346.0,1347.0,1348.0,1349.0,1350.0,1351.0,1352.0,1353.0,1354.0,1355.0,1356.0,1357.0,1358.0,1359.0,1360.0,1361.0,1362.0,1363.0,1364.0,1365.0,1366.0,1367.0,1368.0,1369.0,1370.0,1371.0,1372.0,1373.0,1374.0,1375.0,1376.0,1377.0,1378.0,1379.0,1380.0,1381.0,1382.0,1383.0,1384.0,1385.0,1386.0,1387.0,1388.0,1389.0,1390.0,1391.0,1392.0,1393.0,1394.0,1395.0,1396.0,1397.0,1398.0,1399.0,1400.0,1401.0,1402.0,1403.0,1404.0,1405.0,1406.0,1407.0,1408.0,1409.0,1410.0,1411.0,1412.0,1413.0,1414.0,1415.0,1416.0,1417.0,1418.0,1419.0,1420.0,1421.0,1422.0,1423.0,1424.0,1425.0,1426.0,1427.0,1428.0,1429.0,1430.0,1431.0,1432.0,1433.0,1434.0,1435.0,1436.0,1437.0,1438.0,1439.0,1440.0,1441.0,1442.0,1443.0,1444.0,1445.0,1446.0,1447.0,1448.0,1449.0,1450.0,1451.0,1452.0,1453.0,1454.0,1455.0,1456.0,1457.0,1458.0,1459.0,1460.0,1461.0,1462.0,1463.0,1464.0,1465.0,1466.0,1467.0,1468.0,1469.0,1470.0,1471.0,1472.0,1473.0,1474.0,1475.0,1476.0,1477.0,1478.0,1479.0,1480.0,1481.0,1482.0,1483.0,1484.0,1485.0,1486.0,1487.0,1488.0,1489.0,1490.0,1491.0,1492.0,1493.0,1494.0,1495.0,1496.0,1497.0,1498.0,1499.0,1500.0,1501.0,1502.0,1503.0,1504.0,1505.0,1506.0,1507.0,1508.0,1509.0,1510.0,1511.0,1512.0,1513.0,1514.0,1515.0,1516.0,1517.0,1518.0,1519.0,1520.0,1521.0,1522.0,1523.0,1524.0,1525.0,1526.0,1527.0,1528.0,1529.0,1530.0,1531.0,1532.0,1533.0,1534.0,1535.0,1536.0,1537.0,1538.0,1539.0,1540.0,1541.0,1542.0,1543.0,1544.0,1545.0,1546.0,1547.0,1548.0,1549.0,1550.0,1551.0,1552.0,1553.0,1554.0,1555.0,1556.0,1557.0,1558.0,1559.0,1560.0,1561.0,1562.0,1563.0,1564.0,1565.0,1566.0,1567.0,1568.0,1569.0,1570.0,1571.0,1572.0,1573.0,1574.0,1575.0,1576.0,1577.0,1578.0,1579.0,1580.0,1581.0,1582.0,1583.0,1584.0,1585.0,1586.0,1587.0,1588.0,1589.0,1590.0,1591.0,1592.0,1593.0,1594.0,1595.0,1596.0,1597.0,1598.0,1599.0,1600.0,1601.0,1602.0,1603.0,1604.0,1605.0,1606.0,1607.0,1608.0,1609.0,1610.0,1611.0,1612.0,1613.0,1614.0,1615.0,1616.0,1617.0,1618.0,1619.0,1620.0,1621.0,1622.0,1623.0,1624.0,1625.0,1626.0,1627.0,1628.0,1629.0,1630.0,1631.0,1632.0,1633.0,1634.0,1635.0,1636.0,1637.0,1638.0,1639.0,1640.0,1641.0,1642.0,1643.0,1644.0,1645.0,1646.0,1647.0,1648.0,1649.0,1650.0,1651.0,1652.0,1653.0,1654.0,1655.0,1656.0,1657.0,1658.0,1659.0,1660.0,1661.0,1662.0,1663.0,1664.0,1665.0,1666.0,1667.0,1668.0,1669.0,1670.0,1671.0,1672.0,1673.0,1674.0,1675.0,1676.0,1677.0,1678.0,1679.0,1680.0,1681.0,1682.0,1683.0,1684.0,1685.0,1686.0,1687.0,1688.0,1689.0,1690.0,1691.0,1692.0,1693.0,1694.0,1695.0,1696.0,1697.0,1698.0,1699.0,1700.0,1701.0,1702.0,1703.0,1704.0,1705.0,1706.0,1707.0,1708.0,1709.0,1710.0,1711.0,1712.0,1713.0,1714.0,1715.0,1716.0,1717.0,1718.0,1719.0,1720.0,1721.0,1722.0,1723.0,1724.0,1725.0,1726.0,1727.0,1728.0,1729.0,1730.0,1731.0,1732.0,1733.0,1734.0,1735.0,1736.0,1737.0,1738.0,1739.0,1740.0,1741.0,1742.0,1743.0,1744.0,1745.0,1746.0,1747.0,1748.0,1749.0,1750.0,1751.0,1752.0,1753.0,1754.0,1755.0,1756.0,1757.0,1758.0,1759.0,1760.0,1761.0,1762.0,1763.0,1764.0,1765.0,1766.0,1767.0,1768.0,1769.0,1770.0,1771.0,1772.0,1773.0,1774.0,1775.0,1776.0,1777.0,1778.0,1779.0,1780.0,1781.0,1782.0,1783.0,1784.0,1785.0,1786.0,1787.0,1788.0,1789.0,1790.0,1791.0,1792.0,1793.0,1794.0,1795.0,1796.0,1797.0,1798.0,1799.0,1800.0,1801.0,1802.0,1803.0,1804.0,1805.0,1806.0,1807.0,1808.0,1809.0,1810.0,1811.0,1812.0,1813.0,1814.0,1815.0,1816.0,1817.0,1818.0,1819.0,1820.0,1821.0,1822.0,1823.0,1824.0,1825.0,1826.0,1827.0,1828.0,1829.0,1830.0,1831.0,1832.0,1833.0,1834.0,1835.0,1836.0,1837.0,1838.0,1839.0,1840.0,1841.0,1842.0,1843.0,1844.0,1845.0,1846.0,1847.0,1848.0,1849.0,1850.0,1851.0,1852.0,1853.0,1854.0,1855.0,1856.0,1857.0,1858.0,1859.0,1860.0,1861.0,1862.0,1863.0,1864.0,1865.0,1866.0,1867.0,1868.0,1869.0,1870.0,1871.0,1872.0,1873.0,1874.0,1875.0,1876.0,1877.0,1878.0,1879.0,1880.0,1881.0,1882.0,1883.0,1884.0,1885.0,1886.0,1887.0,1888.0,1889.0,1890.0,1891.0,1892.0,1893.0,1894.0,1895.0,1896.0,1897.0,1898.0,1899.0,1900.0,1901.0,1902.0,1903.0,1904.0,1905.0,1906.0,1907.0,1908.0,1909.0,1910.0,1911.0,1912.0,1913.0,1914.0,1915.0,1916.0,1917.0,1918.0,1919.0,1920.0,1921.0,1922.0,1923.0,1924.0,1925.0,1926.0,1927.0,1928.0,1929.0,1930.0,1931.0,1932.0,1933.0,1934.0,1935.0,1936.0,1937.0,1938.0,1939.0,1940.0,1941.0,1942.0,1943.0,1944.0,1945.0,1946.0,1947.0,1948.0,1949.0,1950.0,1951.0,1952.0,1953.0,1954.0,1955.0,1956.0,1957.0,1958.0,1959.0,1960.0,1961.0,1962.0,1963.0,1964.0,1965.0,1966.0,1967.0,1968.0,1969.0,1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0],\"y\":[0.23041152954101562,0.22966151237487792,0.22985687255859374,0.22950880527496337,0.22805562019348144,0.22826228141784669,0.22903861999511718,0.22889323234558107,0.2285466194152832,0.22891383171081542,0.2275712490081787,0.22746801376342773,0.227822208404541,0.2284019708633423,0.22709949016571046,0.22735304832458497,0.22626101970672607,0.22496967315673827,0.22524852752685548,0.22827372550964356,0.22696254253387452,0.22447118759155274,0.22585208415985109,0.22812600135803224,0.22453861236572265,0.22376036643981934,0.22368693351745605,0.22549469470977784,0.22587032318115235,0.22577569484710694,0.224057936668396,0.2239691734313965,0.2229219913482666,0.22424328327178955,0.22605447769165038,0.22328636646270753,0.22166569232940675,0.22507038116455078,0.22373616695404053,0.22232286930084227,0.22420377731323243,0.22183499336242676,0.22140183448791503,0.22502646446228028,0.2217416763305664,0.22213392257690429,0.22142248153686522,0.22017307281494142,0.22279324531555175,0.22177796363830565,0.22156260013580323,0.22131795883178712,0.22091064453125,0.21990766525268554,0.21934494972229004,0.22305386066436766,0.22160253524780274,0.2200470447540283,0.22060039043426513,0.21936509609222413,0.22086429595947266,0.2183216094970703,0.22052254676818847,0.21881093978881835,0.22002253532409669,0.21929421424865722,0.21932559013366698,0.21937966346740723,0.21911971569061278,0.2167969226837158,0.2172487735748291,0.22199544906616211,0.2165921926498413,0.21884570121765137,0.2198664665222168,0.21859617233276368,0.21813244819641114,0.2197782039642334,0.21945085525512695,0.21909542083740235,0.21892926692962647,0.21829352378845215,0.21447370052337647,0.21531147956848146,0.2189574956893921,0.21786251068115234,0.2168264865875244,0.21638965606689453,0.21607069969177245,0.21845784187316894,0.21718440055847169,0.2175389289855957,0.21668500900268556,0.21684932708740234,0.21564922332763672,0.216916823387146,0.21575417518615722,0.21603703498840332,0.21660149097442627,0.21640393733978272,0.21521141529083251,0.21495916843414306,0.2151423692703247,0.21724722385406495,0.2145561695098877,0.21553571224212648,0.2142655849456787,0.21607937812805175,0.2159944772720337,0.21432628631591796,0.21508636474609374,0.21401565074920653,0.21474802494049072,0.21554107666015626,0.21583158969879152,0.2133927822113037,0.21264250278472902,0.2149144172668457,0.21369309425354005,0.21505429744720458,0.2168877601623535,0.21352238655090333,0.21336767673492432,0.2144439697265625,0.212760329246521,0.21437783241271974,0.21540093421936035,0.21446785926818848,0.21356542110443116,0.2153864860534668,0.21369171142578125,0.21364054679870606,0.21279993057250976,0.2127211093902588,0.21213295459747314,0.2144918918609619,0.21100068092346191,0.21453235149383545,0.2126997232437134,0.20694832801818847,0.2146735668182373,0.21314382553100586,0.21354248523712158,0.21076345443725586,0.21221351623535156,0.21276071071624755,0.21182520389556886,0.21060538291931152,0.21139914989471437,0.21051998138427735,0.21581034660339354,0.2091498613357544,0.21230714321136473,0.21068992614746093,0.21397533416748046,0.20767602920532227,0.21358294486999513,0.21294302940368653,0.21320221424102784,0.21291561126708985,0.21257302761077881,0.21373465061187744,0.20887742042541504,0.2097090482711792,0.21007556915283204,0.21106843948364257,0.20878334045410157,0.21175813674926758,0.20954582691192628,0.21439700126647948,0.20746874809265137,0.209672212600708,0.2090573787689209,0.2077925682067871,0.21179685592651368,0.207771372795105,0.2127681016921997,0.20960621833801268,0.20798416137695314,0.2120222806930542,0.20627069473266602,0.2110264778137207,0.20877809524536134,0.21156821250915528,0.20736913681030272,0.21053838729858398,0.20752809047698975,0.21375558376312256,0.20800118446350097,0.20838704109191894,0.20797264575958252,0.21063625812530518,0.20955500602722169,0.2100313901901245,0.20908031463623047,0.20562372207641602,0.2131293773651123,0.20783514976501466,0.20806922912597656,0.21004395484924315,0.20716099739074706,0.20762476921081544,0.2121950626373291,0.20910971164703368,0.20881004333496095,0.20930113792419433,0.20681934356689452,0.21253180503845215,0.20889806747436523,0.20532748699188233,0.21094803810119628,0.20799269676208496,0.20851306915283202,0.20466089248657227,0.2065141201019287,0.20741887092590333,0.20433273315429687,0.20453212261199952,0.20808143615722657,0.20511376857757568,0.20399718284606932,0.2033461093902588,0.20433378219604492,0.20337514877319335,0.21019043922424316,0.2070923328399658,0.210101318359375,0.2114729404449463,0.20485951900482177,0.20676765441894532,0.20569138526916503,0.20944690704345703,0.20691413879394532,0.20365965366363525,0.2062779426574707,0.21425399780273438,0.2094942569732666,0.20939188003540038,0.20548062324523925,0.2055206298828125,0.2030730962753296,0.209356427192688,0.20838704109191894,0.20654191970825195,0.20786972045898439,0.20663342475891114,0.2029705286026001,0.20190129280090333,0.20805799961090088,0.20499601364135742,0.2084590435028076,0.2081571102142334,0.20609688758850098,0.20342733860015869,0.20469050407409667,0.20183925628662108,0.20794053077697755,0.20637874603271483,0.20293040275573732,0.20516722202301024,0.20312857627868652,0.2063063144683838,0.20750653743743896,0.20576491355895996,0.2095167636871338,0.2057188034057617,0.20828118324279785,0.20457534790039061,0.20125834941864013,0.19930050373077393,0.20677824020385743,0.20815720558166503,0.20583133697509765,0.20565943717956542,0.20400803089141845,0.2023611068725586,0.2013314723968506,0.20810856819152831,0.20326876640319824,0.2046574592590332,0.2035360097885132,0.20262017250061035,0.20259451866149902,0.20729632377624513,0.2038853406906128,0.2036445379257202,0.203277587890625,0.21215486526489258,0.20517218112945557,0.2075216293334961,0.20989084243774414,0.19904897212982178,0.2062448024749756,0.20789568424224852,0.20226480960845947,0.1971317410469055,0.20161724090576172,0.20240306854248047,0.20533447265625,0.19943137168884278,0.20588898658752441,0.20018382072448732,0.20568881034851075,0.19994006156921387,0.2039128303527832,0.20584309101104736,0.20436315536499022,0.19959161281585694,0.20647883415222168,0.20901000499725342,0.20691347122192383,0.20587317943572997,0.20253400802612304,0.20356111526489257,0.20508136749267578,0.20663962364196778,0.20461277961730956,0.2015394926071167,0.20558769702911378,0.20523607730865479,0.1994268536567688,0.20348222255706788,0.19871939420700074,0.20343596935272218,0.20421867370605468,0.20171718597412108,0.1999035358428955,0.20049865245819093,0.20180957317352294,0.20531129837036133,0.20393915176391603,0.2036734104156494,0.20555438995361328,0.19669506549835206,0.20531752109527587,0.19528969526290893,0.20625944137573243,0.2016446590423584,0.20082664489746094,0.20282433032989503,0.204750657081604,0.20525708198547363,0.2006826400756836,0.2002101421356201,0.2016960620880127,0.2041799545288086,0.20215435028076173,0.202138090133667,0.20202863216400146,0.20216851234436034,0.20298891067504882,0.19789955615997315,0.201529598236084,0.2030715227127075,0.20125489234924315,0.19934003353118895,0.2001512050628662,0.20199332237243653,0.1996180534362793,0.20108144283294677,0.19729983806610107,0.2073805809020996,0.20228657722473145,0.1971285343170166,0.2040026903152466,0.20690817832946778,0.20151088237762452,0.20018372535705567,0.19958642721176148,0.20553336143493653,0.20354392528533935,0.2032081127166748,0.1984960436820984,0.19933894872665406,0.20134124755859376,0.19857176542282104,0.20032932758331298,0.20622830390930175,0.2025980234146118,0.20113167762756348,0.19435651302337648,0.20311350822448732,0.19855964183807373,0.20604329109191893,0.20434536933898925,0.19862154722213746,0.19613323211669922,0.20340166091918946,0.20089857578277587,0.19636180400848388,0.20567102432250978,0.2046414852142334,0.2026759624481201,0.2056055784225464,0.2018980026245117,0.19922196865081787,0.19957319498062134,0.20386092662811278,0.20503978729248046,0.1976151466369629,0.195359468460083,0.19740582704544068,0.19780240058898926,0.2006345272064209,0.20034470558166503,0.20115118026733397,0.19814538955688477,0.20043954849243165,0.20062520503997802,0.19555895328521727,0.20887293815612792,0.19283828735351563,0.19656116962432862,0.2007152557373047,0.20458145141601564,0.19992103576660156,0.2010420322418213,0.1980022072792053,0.19754656553268432,0.2024837017059326,0.198584246635437,0.20186328887939453,0.20305416584014893,0.20812878608703614,0.20292959213256836,0.19970017671585083,0.19664300680160524,0.20002641677856445,0.20116667747497557,0.1984174966812134,0.20114316940307617,0.19861216545104982,0.19836493730545043,0.20467123985290528,0.20078916549682618,0.19915974140167236,0.2030266523361206,0.2013627529144287,0.20379786491394042,0.20356464385986328,0.19943017959594728,0.19939122200012208,0.19674060344696045,0.19764323234558107,0.2010646343231201,0.1975589394569397,0.19732096195220947,0.1994655966758728,0.19699807167053224,0.2000121831893921,0.19719502925872803,0.19821879863739014,0.2010516881942749,0.20236489772796631,0.19949711561203004,0.1987929344177246,0.2002171754837036,0.19491504430770873,0.19653624296188354,0.19632716178894044,0.19973406791687012,0.2009672164916992,0.19616655111312867,0.1982303738594055,0.19772908687591553,0.19766098260879517,0.1919854998588562,0.204121732711792,0.19656732082366943,0.19666286706924438,0.20085291862487792,0.20219733715057372,0.19591197967529297,0.19711830615997314,0.20090181827545167,0.2057487726211548,0.19736602306365966,0.2002784252166748,0.198200523853302,0.19568135738372802,0.20364868640899658,0.19413033723831177,0.19821912050247192,0.20633141994476317,0.20146524906158447,0.2067267656326294,0.19645966291427613,0.1969730496406555,0.1980576992034912,0.19700547456741332,0.1959166407585144,0.20053787231445314,0.19693243503570557,0.19414258003234863,0.20148544311523436,0.19799160957336426,0.20076494216918944,0.19553208351135254,0.20332138538360595,0.19206877946853637,0.19984874725341797,0.1986790895462036,0.20218584537506104,0.20143723487854004,0.20021390914916992,0.19854594469070436,0.19984334707260132,0.20019667148590087,0.194150972366333,0.20333366394042968,0.1932230830192566,0.19524184465408326,0.19584259986877442,0.1990111231803894,0.2027461051940918,0.19045712947845458,0.2026155948638916,0.19370830059051514,0.1964118242263794,0.19292157888412476,0.19419113397598267,0.1979370355606079,0.20173273086547852,0.19749301671981812,0.19720275402069093,0.19908041954040528,0.1976740002632141,0.20413975715637206,0.19526474475860595,0.19567254781723023,0.19874629974365235,0.20324885845184326,0.20104341506958007,0.196350622177124,0.2025383949279785,0.19337908029556275,0.2056819438934326,0.19366536140441895,0.1982577323913574,0.19499454498291016,0.19450403451919557,0.19995193481445311,0.19762004613876344,0.19534584283828735,0.19294127225875854,0.1926533341407776,0.1945825457572937,0.19699229001998902,0.19399937391281127,0.19702608585357667,0.1911820650100708,0.19743114709854126,0.2023477554321289,0.19643809795379638,0.19404025077819825,0.20082299709320067,0.2033339262008667,0.19247901439666748,0.1964005708694458,0.19506081342697143,0.19256476163864136,0.19383766651153564,0.19539391994476318,0.20242300033569335,0.19776225090026855,0.19723764657974244,0.19488338232040406,0.20038652420043945,0.19737937450408935,0.20184454917907715,0.19674261808395385,0.19570350646972656,0.1949848413467407,0.19781216382980346,0.19935312271118164,0.1959500789642334,0.19606211185455322,0.1935110092163086,0.20197658538818358,0.20087478160858155,0.1932897686958313,0.20058057308197022,0.196791934967041,0.19522202014923096,0.19788846969604493,0.19295493364334107,0.19052766561508178,0.1955332040786743,0.19453370571136475,0.1956099271774292,0.19680689573287963,0.20256414413452148,0.191987943649292,0.19807398319244385,0.1939086675643921,0.19811915159225463,0.19878337383270264,0.19483551979064942,0.194974946975708,0.19211363792419434,0.19517698287963867,0.20054261684417723,0.19657014608383178,0.1981923460960388,0.18854492902755737,0.19577087163925172,0.19889073371887206,0.19730110168457032,0.19960662126541137,0.1959874749183655,0.19284944534301757,0.19313247203826905,0.19856957197189332,0.19561843872070311,0.1989089846611023,0.19050772190093995,0.19374710321426392,0.19636449813842774,0.19332956075668334,0.19442436695098878,0.19842748641967772,0.19710795879364013,0.1942052125930786,0.19476112127304077,0.1979992628097534,0.19157077074050904,0.19166829586029052,0.19216593503952026,0.19425809383392334,0.19876729249954223,0.1940173864364624,0.1915189504623413,0.189930260181427,0.20135197639465333,0.19647619724273682,0.18997554779052733,0.1950342297554016,0.20033502578735352,0.18636208772659302,0.19193825721740723,0.19464497566223143,0.20047564506530763,0.18985471725463868,0.19627251625061035,0.20104856491088868,0.196610689163208,0.19748499393463134,0.19399768114089966,0.20466246604919433,0.19353957176208497,0.19319391250610352,0.1920873999595642,0.19411311149597169,0.18494473695755004,0.19491249322891235,0.19172401428222657,0.19788365364074706,0.2006335973739624,0.19723401069641114,0.19526686668395996,0.20092339515686036,0.19850330352783202,0.19493014812469484,0.19137148857116698,0.197292697429657,0.1964585304260254,0.19902358055114747,0.1927799701690674,0.18585777282714844,0.19103120565414428,0.19702322483062745,0.1996596097946167,0.19604723453521727,0.19394396543502807,0.19375112056732177,0.19503769874572754,0.19255759716033935,0.19067492485046386,0.2049569606781006,0.19625755548477172,0.19309501647949218,0.20177195072174073,0.19279589653015136,0.19162073135375976,0.19407451152801514,0.1962890863418579,0.19320833683013916,0.1926971197128296,0.1963214874267578,0.1993536353111267,0.19387367963790894,0.19213999509811402,0.19247236251831054,0.20093250274658203,0.19712677001953124,0.19168384075164796,0.20049805641174318,0.19428868293762208,0.20588841438293456,0.19274280071258545,0.20396780967712402,0.19482465982437133,0.19268712997436524,0.190080988407135,0.1968017578125,0.19585778713226318,0.1968191981315613,0.19745945930480957,0.19412857294082642,0.19804264307022096,0.1965605616569519,0.19390366077423096,0.1940150260925293,0.19354336261749266,0.19436995983123778,0.1935224413871765,0.19514682292938232,0.19265118837356568,0.19332342147827147,0.20286636352539061,0.19728058576583862,0.18989650011062623,0.19304335117340088,0.1918110728263855,0.1967527151107788,0.19636085033416747,0.19374185800552368,0.191591215133667,0.19320075511932372,0.20240590572357178,0.20103073120117188,0.1934803009033203,0.19523919820785524,0.18884460926055907,0.19056661128997804,0.20049500465393066,0.19708628654479982,0.19161589145660402,0.1957789659500122,0.19756194353103637,0.19353253841400148,0.19722263813018798,0.1910954475402832,0.19466593265533447,0.1977776288986206,0.19154938459396362,0.19009113311767578,0.18596645593643188,0.19249571561813356,0.19133563041687013,0.19816222190856933,0.1913384199142456,0.2005692958831787,0.1942049026489258,0.19847023487091064,0.19130382537841797,0.19373472929000854,0.19259096384048463,0.19097676277160644,0.19373267889022827,0.1940972089767456,0.20475399494171143,0.20027530193328857,0.19579579830169677,0.19551924467086793,0.19557937383651733,0.19837615489959717,0.19052335023880004,0.1945006728172302,0.18955516815185547,0.18626928329467773,0.19323134422302246,0.1902538299560547,0.19567523002624512,0.19339298009872435,0.18916423320770265,0.20088915824890136,0.19057425260543823,0.18839659690856933,0.19056110382080077,0.18855456113815308,0.19471921920776367,0.19358453750610352,0.19415657520294188,0.19388186931610107,0.19329277276992798,0.1960562586784363,0.1938401699066162,0.19204578399658204,0.19126720428466798,0.19129695892333984,0.188506555557251,0.1968790054321289,0.19799268245697021,0.19573098421096802,0.19441479444503784,0.19287328720092772,0.19496326446533202,0.1946513295173645,0.19005590677261353,0.19038642644882203,0.19207658767700195,0.1957583785057068,0.19262526035308838,0.18910146951675416,0.19891929626464844,0.19589837789535522,0.1991619348526001,0.1932670831680298,0.19703454971313478,0.1885772466659546,0.1931411623954773,0.18900561332702637,0.19311375617980958,0.1870710015296936,0.19202094078063964,0.1936310887336731,0.19358222484588622,0.19335484504699707,0.1990436792373657,0.19387950897216796,0.1983208417892456,0.19607324600219728,0.1910618305206299,0.18996798992156982,0.20051274299621583,0.18682100772857665,0.18784523010253906,0.1896904706954956,0.19220021963119507,0.1884592056274414,0.18698034286499024,0.19005699157714845,0.1922318935394287,0.19366023540496827,0.19191967248916625,0.1905111312866211,0.18905291557312012,0.19709941148757934,0.1843506097793579,0.18925462961196898,0.19786813259124755,0.19487614631652833,0.19363479614257811,0.19120259284973146,0.19451601505279542,0.1853623390197754,0.18959589004516603,0.19855165481567383,0.1997591018676758,0.19386641979217528,0.1924576759338379,0.19511996507644652,0.19806236028671265,0.19046597480773925,0.1901276707649231,0.19314820766448976,0.19157190322875978,0.1917415976524353,0.19869446754455566,0.19176539182662963,0.1957041621208191,0.18315498828887938,0.19114243984222412,0.19007714986801147,0.19271955490112305,0.19240413904190062,0.19109364748001098,0.18934212923049926,0.1976291298866272,0.18603332042694093,0.19022717475891113,0.19144259691238402,0.19553096294403077,0.1920685052871704,0.19474798440933228,0.19678385257720948,0.19385416507720948,0.18858059644699096,0.19805498123168946,0.19057897329330445,0.18694422245025635,0.19183008670806884,0.19188371896743775,0.20026755332946777,0.19283478260040282,0.18589816093444825,0.1908862352371216,0.1963710904121399,0.20196781158447266,0.19078423976898193,0.19141297340393065,0.18554139137268066,0.19420771598815917,0.1894770860671997,0.1979604721069336,0.1953284740447998,0.18990919589996338,0.19725738763809203,0.1872364640235901,0.18738423585891723,0.18885903358459472,0.18060455322265626,0.1963315486907959,0.19033076763153076,0.1882482051849365,0.19601054191589357,0.1964940309524536,0.19453349113464355,0.18823845386505128,0.18533189296722413,0.19180104732513428,0.18704469203948976,0.1953023910522461,0.19113738536834718,0.1912081480026245,0.18613951206207274,0.18917102813720704,0.19612911939620972,0.1889204740524292,0.19636560678482057,0.19402767419815065,0.1964421272277832,0.1818813681602478,0.1920785903930664,0.18752771615982056,0.1895347237586975,0.1930857539176941,0.1935318350791931,0.19447622299194336,0.2000274419784546,0.19080007076263428,0.19607086181640626,0.18672677278518676,0.19556658267974852,0.1926856279373169,0.19255629777908326,0.19150278568267823,0.18850585222244262,0.19166061878204346,0.18687360286712645,0.19032372236251832,0.19865585565567018,0.19597532749176025,0.18922953605651854,0.1953974723815918,0.18423519134521485,0.1953425168991089,0.19629119634628295,0.18719558715820311,0.19111881256103516,0.1992905020713806,0.1942017436027527,0.19870051145553588,0.19989025592803955,0.18951690196990967,0.18865084648132324,0.19094221591949462,0.19147212505340577,0.1891934871673584,0.19812219142913817,0.20167958736419678,0.1903291702270508,0.2008206844329834,0.1861424446105957,0.18956854343414306,0.18645234107971193,0.1966625452041626,0.18948287963867189,0.19141532182693483,0.1886974811553955,0.18575598001480104,0.19539159536361694,0.19615161418914795,0.18819935321807862,0.19958181381225587,0.18850820064544677,0.19560883045196534,0.19387813806533813,0.18998795747756958,0.19309872388839722,0.18668112754821778,0.19604768753051757,0.1872417688369751,0.19279180765151976,0.18697189092636107,0.1886747360229492,0.19536798000335692,0.19188178777694703,0.1928114891052246,0.18451454639434814,0.1865036129951477,0.18909258842468263,0.19023189544677735,0.18886369466781616,0.19544554948806764,0.18963850736618043,0.1920941114425659,0.19794336557388306,0.18709179162979125,0.18665411472320556,0.18993139266967773,0.19217100143432617,0.18754055500030517,0.1901463747024536,0.19448466300964357,0.18065934181213378,0.19320344924926758,0.1872791528701782,0.19044864177703857,0.18973383903503419,0.1883123278617859,0.18679317235946655,0.19832096099853516,0.19033209085464478,0.1881783962249756,0.19252235889434816,0.18718827962875367,0.19261476993560792,0.19634177684783935,0.1965261936187744,0.1880625605583191,0.19676201343536376,0.18112618923187257,0.18955844640731812,0.19709726572036743,0.19221212863922119,0.19668688774108886,0.185080623626709,0.19576996564865112,0.2034156084060669,0.19129959344863892,0.18954403400421144,0.18798842430114746,0.1936357021331787,0.1923619508743286,0.19145591259002687,0.20094776153564453,0.18587868213653563,0.18820540904998778,0.18648099899291992,0.1933639168739319,0.1949467897415161,0.1827818512916565,0.18892393112182618,0.19930360317230225,0.1897887945175171,0.19021177291870117,0.18332695960998535,0.18985424041748047,0.1887291669845581,0.18624632358551024,0.18731943368911744,0.19574609994888306,0.1862303614616394,0.1851893424987793,0.1932004928588867,0.1912033438682556,0.18722169399261473,0.19026857614517212,0.19196544885635375,0.18871095180511474,0.1891303300857544,0.1853506088256836,0.19025028944015504,0.1901978373527527,0.19032838344573974,0.1838165283203125,0.19722675085067748,0.19359222650527955,0.1924736976623535,0.19186363220214844,0.18942456245422362,0.19145510196685792,0.19025905132293702,0.18892732858657837,0.19606221914291383,0.18891525268554688,0.19730744361877442,0.18700909614562988,0.18403432369232178,0.18549107313156127,0.19004456996917723,0.18663092851638793,0.19303354024887084,0.1874943494796753,0.19216848611831666,0.19072396755218507,0.1901723861694336,0.18826375007629395,0.18480693101882933,0.1886831998825073,0.1918389081954956,0.1839098811149597,0.19213663339614867,0.19277344942092894,0.19430748224258423,0.1925365686416626,0.18672568798065187,0.19764599800109864,0.19412083625793458,0.18811722993850707,0.18153396844863892,0.18834611177444457,0.19209003448486328,0.19157425165176392,0.19029462337493896,0.19167352914810182,0.19315500259399415,0.1955755829811096,0.1908428430557251,0.200309419631958,0.18923879861831666,0.18548061847686767,0.18900561332702637,0.19452544450759887,0.19693570137023925,0.19578249454498292,0.18959879875183105,0.19799221754074098,0.1866327404975891,0.19154572486877441,0.19041669368743896,0.1962360620498657,0.18894436359405517,0.19624615907669068,0.18536465167999266,0.1945273160934448,0.18244471549987792,0.18440619707107545,0.1982906460762024,0.19238992929458618,0.1846062183380127,0.18663763999938965,0.198227858543396,0.19193934202194213,0.1890270709991455,0.1943354606628418,0.18724750280380248,0.18929840326309205,0.18910491466522217,0.18826283216476442,0.19297399520874023,0.19056544303894044,0.18001146316528321,0.19432487487792968,0.19389872550964354,0.19628405570983887,0.19219008684158326,0.18520356416702272,0.19131778478622435,0.19342918395996095,0.19471564292907714,0.19218443632125853,0.19802526235580445,0.18730236291885377,0.18600765466690064,0.19400662183761597,0.19270930290222169,0.18821872472763063,0.19312620162963867,0.18727291822433473,0.18937448263168336,0.18946704864501954,0.18571219444274903,0.1852409601211548,0.18587641716003417,0.18019869327545165,0.18963179588317872,0.1895521402359009,0.1939329147338867,0.18956499099731444,0.19563432931900024,0.1953148603439331,0.18524955511093139,0.18949687480926514,0.1988348126411438,0.1978180766105652,0.19196289777755737,0.19030191898345947,0.18847860097885133,0.19187309741973876,0.19371278285980226,0.1949850082397461,0.18825945854187012,0.18994272947311402,0.196518874168396,0.18302161693573,0.1967369794845581,0.19546397924423217,0.18789348602294922,0.1840426206588745,0.18544546365737916,0.19452037811279296,0.19246960878372193,0.1922210931777954,0.18856798410415648,0.19393085241317748,0.1964670181274414,0.19418532848358155,0.19301316738128663,0.1969343900680542,0.18506065607070923,0.18592267036437987,0.182355797290802,0.1894136428833008,0.19086875915527343,0.18739391565322877,0.18717806339263915,0.19139405488967895,0.1908212423324585,0.1901954174041748,0.18605445623397826,0.18554097414016724,0.19068217277526855,0.18814258575439452,0.18372340202331544,0.19114208221435547,0.18420498371124266,0.18542519807815552,0.18630994558334352,0.1853158950805664,0.19181596040725707,0.19472894668579102,0.18927426338195802,0.19294699430465698,0.18954083919525147,0.196122944355011,0.18669350147247316,0.19063736200332643,0.19694772958755494,0.19308722019195557,0.18933360576629638,0.18708276748657227,0.19506356716156006,0.18527252674102784,0.1893537998199463,0.18662455081939697,0.19120471477508544,0.19595928192138673,0.1863157033920288,0.18090558052062988,0.1924802303314209,0.18810787200927734,0.1832815885543823,0.1916595697402954,0.19291508197784424,0.18994265794754028,0.19422248601913453,0.19375901222229003,0.18768494129180907,0.18419333696365356,0.18517007827758789,0.19243786334991456,0.19543373584747314,0.18921420574188233,0.18234938383102417,0.18806029558181764,0.19313671588897705,0.18922406435012817,0.19071249961853026,0.18880558013916016,0.1878953218460083,0.19093806743621827,0.18455135822296143,0.18497831821441652,0.1887230634689331,0.192393159866333,0.1928386926651001,0.1898323893547058,0.19041674137115477,0.18192946910858154,0.19159135818481446,0.18804303407669068,0.19296703338623047,0.1873565912246704,0.18482627868652343,0.18620831966400148,0.18598682880401612,0.18726422786712646,0.18512871265411376,0.18874831199645997,0.19282908439636232,0.183274507522583,0.18783649206161498,0.1857409119606018,0.19130959510803222,0.18729195594787598,0.18985419273376464,0.18400589227676392,0.19232782125473022,0.19415441751480103,0.19688326120376587,0.19336451292037965,0.19502384662628175,0.1968568444252014,0.1976460576057434,0.1873149871826172,0.18382163047790528,0.1904497742652893,0.18820115327835082,0.19387288093566896,0.18539736270904542,0.1910571575164795,0.18688002824783326,0.19036173820495605,0.19482760429382323,0.18725173473358153,0.18215714693069457,0.1858302593231201,0.18782997131347656,0.1903012990951538,0.18581830263137816,0.1893308162689209,0.18331754207611084,0.1954108476638794,0.1911712884902954,0.18916656970977783,0.19116315841674805,0.18552230596542357,0.18678085803985595,0.18900978565216064,0.18627697229385376,0.19113316535949706,0.19223637580871583,0.19233341217041017,0.19210160970687867,0.1853581428527832,0.18367331027984618,0.1935807228088379,0.19361262321472167,0.18433015346527098,0.1851298451423645,0.19168260097503662,0.1846048951148987,0.19078536033630372,0.1940680742263794,0.18558982610702515,0.19501063823699952,0.1891087293624878,0.19218264818191527,0.1863972544670105,0.18602845668792725,0.1818768262863159,0.19026074409484864,0.18798656463623048,0.1852177858352661,0.19104948043823242,0.19168788194656372,0.1941044569015503,0.19882644414901735,0.18833893537521362,0.18652344942092897,0.19253315925598144,0.191520357131958,0.1865563154220581,0.18521910905838013,0.1874630331993103,0.18729982376098633,0.18213733434677123,0.19213392734527587,0.19104630947113038,0.1841121196746826,0.18913798332214354,0.18078749179840087,0.18875926733016968,0.18824228048324584,0.18141531944274902,0.18898916244506836,0.18612899780273437,0.18749209642410278,0.18259696960449218,0.1898144006729126,0.1949749231338501,0.1887001395225525,0.19441709518432618,0.19425588846206665,0.19673352241516112,0.18663697242736815,0.18781864643096924,0.1939152956008911,0.18544199466705322,0.18404102325439453,0.18755229711532592,0.1905585527420044,0.17732584476470947,0.18859548568725587,0.18593840599060057,0.19171538352966308,0.18658828735351562,0.18751199245452882,0.18563255071640014,0.19230400323867797,0.1885086178779602,0.1919191598892212,0.18088337182998657,0.19088296890258788,0.18465886116027833,0.1895774483680725,0.19221057891845703,0.18307726383209227,0.18909621238708496,0.1927879571914673,0.18781440258026122,0.18752284049987794,0.195839262008667,0.18233342170715333,0.1874840259552002,0.18862016201019288,0.19856537580490113,0.18744518756866455,0.1912424921989441,0.19417567253112794,0.18804702758789063,0.18960036039352418,0.1892099618911743,0.18278019428253173,0.19060087203979492,0.1882917046546936,0.18463637828826904,0.18391557931900024,0.1840383768081665,0.18138406276702881,0.18482760190963746,0.18779408931732178,0.18076696395874023,0.1840897798538208,0.1854395866394043,0.18517603874206542,0.19596490859985352,0.18926732540130614,0.1879154086112976,0.1864022731781006,0.18699216842651367,0.193166184425354,0.18485220670700073,0.1934115171432495,0.19852712154388427,0.18742421865463257,0.1895381450653076,0.20144281387329102,0.18539202213287354,0.1851613402366638,0.18938353061676025,0.18891313076019287,0.19022644758224488,0.18133805990219115,0.17982293367385865,0.18877183198928832,0.1815778613090515,0.18188297748565674,0.1946721076965332,0.185334312915802,0.2005312442779541,0.18361942768096923,0.18817425966262818,0.19000811576843263,0.1872851848602295,0.18708343505859376,0.19192636013031006,0.19236998558044432,0.18515709638595582,0.1840669631958008,0.19162048101425172,0.1898055076599121,0.18534475564956665,0.19293701648712158,0.19096953868865968,0.19248945713043214,0.18749091625213624,0.19236843585968016,0.19320030212402345,0.19907052516937257,0.19055076837539672,0.18050622940063477,0.18364028930664061,0.1866110682487488,0.18274407386779784,0.1847195029258728,0.18573077917098998,0.19259310960769654,0.18361024856567382,0.18520731925964357,0.191886043548584,0.19449933767318725,0.19321722984313966,0.1845884919166565,0.19078397750854492,0.1867359161376953,0.1811407208442688,0.19494091272354125,0.19388142824172974,0.1886976480484009,0.18675684928894043,0.1961364984512329,0.18668677806854247,0.19069280624389648,0.18299978971481323,0.1890865683555603,0.1851749062538147,0.19031591415405275,0.18951572179794313,0.18420807123184205,0.1870121479034424,0.19064998626708984,0.1872793912887573,0.1948191285133362,0.18946518898010253,0.181245756149292,0.1821429967880249,0.18814254999160768,0.18286454677581787,0.19236629009246825,0.1864396333694458,0.1796095371246338,0.1809898853302002,0.1874490737915039,0.19428012371063233,0.18106765747070314,0.18861632347106932,0.19573835134506226,0.18145558834075928,0.18318521976470947,0.18395830392837526,0.20156009197235109,0.1882399559020996,0.1895775318145752,0.19116830825805664,0.1931416392326355,0.1890297770500183,0.18983330726623535,0.18297706842422484,0.1900489091873169,0.18526335954666137,0.17361242771148683,0.194976282119751,0.1823350667953491,0.17835712432861328,0.1845254898071289,0.18391587734222412,0.18847126960754396,0.1820929765701294,0.19253662824630738,0.18884751796722413,0.18964238166809083,0.1869335651397705,0.19543864727020263,0.18370288610458374,0.19684573411941528,0.18883224725723266,0.18539539575576783,0.1852148413658142,0.18570029735565186,0.18485939502716064,0.1855777382850647,0.18698549270629883,0.19134408235549927,0.1879220962524414,0.18727167844772338,0.18170973062515258,0.18384460210800171,0.18451402187347413,0.18382160663604735,0.18273268938064574,0.18497107028961182,0.18057453632354736,0.19246292114257812,0.18428789377212523,0.18684548139572144,0.18520890474319457,0.18800859451293944,0.18100993633270263,0.18663980960845947,0.18546990156173707,0.19533278942108154,0.1942903995513916,0.1899596095085144,0.18642494678497315,0.18009612560272217,0.1938685178756714,0.19076722860336304,0.18357454538345336,0.1804833173751831,0.18150887489318848,0.18223530054092407,0.1852982759475708,0.18341143131256105,0.1912745475769043,0.19030660390853882,0.18901140689849855,0.18258713483810424,0.19666703939437866,0.18954325914382936,0.18601762056350707,0.1859381079673767,0.18746638298034668,0.18441177606582643,0.1884304404258728,0.18794840574264526,0.18370083570480347,0.18329402208328247,0.19246805906295777,0.19761985540390015,0.18648185729980468,0.18442846536636354,0.18883249759674073,0.18755390644073486,0.18775804042816163,0.19628543853759767,0.19113655090332032,0.17766584157943727,0.18800305128097533,0.18393285274505616,0.1872567653656006,0.19305050373077393,0.1908832311630249,0.18346216678619384,0.189313805103302,0.19329323768615722,0.1889270544052124,0.20206861495971679,0.1878188133239746,0.18359627723693847,0.18927857875823975,0.18846917152404785,0.18600627183914184,0.18524975776672364,0.18200808763504028,0.18574810028076172,0.1858210802078247,0.18628814220428466,0.18309932947158813,0.18542184829711914,0.18306541442871094,0.1889711856842041,0.19369950294494628,0.19199355840682983,0.1951218605041504,0.18984203338623046,0.18722933530807495,0.18491729497909545,0.18658823966979982,0.19029417037963867,0.18697869777679443,0.18747049570083618,0.18129440546035766,0.1849219560623169,0.19741055965423585,0.18887994289398194,0.18078057765960692,0.19265960454940795,0.19313573837280273,0.19157614707946777,0.19345569610595703,0.1857084274291992,0.19399260282516478,0.18358203172683715,0.18207314014434814,0.19119666814804076,0.1795099973678589,0.18225882053375245,0.19110207557678222,0.18465015888214112,0.1843013882637024,0.18672144412994385,0.18221508264541625,0.18186438083648682,0.19051336050033568,0.18407869338989258,0.18017598390579223,0.18786610364913942,0.19445481300354003,0.18042625188827516,0.18548028469085692,0.1849851131439209,0.19405558109283447,0.18257124423980714,0.1897247314453125,0.18226755857467652,0.18280322551727296,0.18584558963775635,0.18324341773986816,0.18823723793029784,0.1911710262298584,0.1861047625541687,0.1955500602722168,0.18503987789154053,0.17992637157440186,0.1833125352859497,0.17912778854370118,0.18313711881637573,0.18691734075546265,0.18197400569915773,0.1800553321838379,0.1804336428642273,0.18611785173416137,0.19325249195098876,0.18116507530212403,0.18152931928634644,0.18486323356628417,0.18697853088378907,0.17690938711166382,0.18796956539154053,0.17829608917236328,0.18866434097290039,0.18203060626983641,0.1943874478340149,0.1884239912033081,0.18326257467269896,0.18687615394592286,0.1831578016281128,0.1950751781463623,0.19437682628631592,0.18356139659881593,0.18639841079711914,0.18908652067184448,0.18909330368041993,0.18866827487945556,0.19046918153762818,0.18221101760864258,0.18313851356506347,0.1840352773666382,0.18710339069366455,0.18683323860168458,0.19116131067276002,0.18423731327056886,0.1831698536872864,0.18561713695526122,0.18075921535491943,0.1888325810432434,0.18242874145507812,0.18283313512802124,0.18597745895385742,0.18025310039520265,0.18856606483459473,0.1948791742324829,0.18890669345855712,0.18690049648284912,0.18792173862457276,0.17863874435424804,0.18873319625854493,0.19012821912765504,0.18507639169692994,0.18840575218200684,0.18189671039581298,0.18676047325134276,0.18720282316207887,0.18690118789672852,0.18702335357666017,0.18488526344299316,0.18598852157592774,0.19041455984115602,0.18448114395141602,0.1852874279022217,0.18474785089492798,0.19010987281799316,0.18106995820999144,0.1937916874885559,0.1958409309387207,0.18297715187072755,0.19223146438598632,0.189192795753479,0.1837487578392029,0.18692830801010132,0.18839694261550904,0.18603057861328126,0.1857043743133545,0.190155029296875,0.1859783411026001,0.18125128746032715,0.18578262329101564,0.1810155391693115,0.18667500019073485,0.18111586570739746,0.1839182734489441,0.19305450916290284,0.17545298337936402,0.1882922410964966,0.18195465803146363,0.18449877500534057,0.19032312631607057,0.18695712089538574,0.18961470127105712,0.18242669105529785,0.18118150234222413,0.19138679504394532,0.19329304695129396,0.18727457523345947,0.18566596508026123,0.18189281225204468,0.18939005136489867,0.18944504261016845,0.18965705633163452,0.18519606590270996,0.18848763704299926,0.18420307636260985,0.18781312704086303,0.1847814440727234,0.18730587959289552,0.18513325452804566,0.18062832355499267,0.18678535223007203,0.18925349712371825,0.1913140296936035,0.1875452756881714,0.18362851142883302,0.18358471393585205,0.18600906133651735,0.19074825048446656,0.18971219062805175,0.18602778911590576,0.18454400300979615,0.1862824559211731,0.18297209739685058,0.19244731664657594,0.18919825553894043,0.18893790245056152,0.1948665499687195,0.18265480995178224,0.18533824682235717,0.17947287559509278,0.18898999691009521,0.18901230096817018,0.1919054388999939,0.1780595064163208,0.18755698204040527,0.19132670164108276,0.18871111869812013,0.18245081901550292,0.18371611833572388,0.1843736171722412,0.18172271251678468,0.18959354162216185,0.18594623804092408,0.18277790546417236,0.18592371940612792,0.18827954530715943,0.1840360403060913,0.19203834533691405,0.18410940170288087,0.18935941457748412,0.1780565619468689,0.18088297843933104,0.18704123497009278,0.18272027969360352,0.18327974081039428,0.18658008575439453,0.1848936438560486,0.19002383947372437,0.18069987297058104,0.18800795078277588,0.18528918027877808,0.184519624710083,0.180821692943573,0.18561290502548217,0.18773143291473388,0.1964272141456604,0.1863034725189209,0.18891699314117433,0.18731695413589478,0.18305834531784057,0.18880709409713745,0.1848128318786621,0.19129621982574463,0.18813049793243408,0.19253859519958497,0.193440842628479,0.1820469617843628,0.18686579465866088,0.17884933948516846,0.18454713821411134,0.19244909286499023,0.18123557567596435,0.18644059896469117,0.1873775005340576,0.18580098152160646,0.184196138381958,0.18395228385925294,0.18644146919250487,0.1842687487602234,0.18156847953796387,0.18330055475234985,0.19250936508178712,0.18588247299194335,0.18966517448425294,0.18108172416687013,0.18321363925933837,0.1807041049003601,0.1876041293144226,0.17830376625061034,0.18786296844482422,0.18031641244888305,0.19020336866378784,0.18758275508880615,0.18991363048553467,0.1863860607147217,0.18263390064239501,0.1813308358192444,0.1821671724319458,0.18976584672927857,0.19619122743606568,0.18834609985351564,0.18511631488800048,0.1895317554473877,0.1905284881591797,0.19249863624572755,0.18747947216033936,0.18984148502349854,0.1856817364692688,0.18552500009536743,0.18494729995727538,0.1950596809387207,0.18362631797790527,0.18277920484542848,0.18732290267944335,0.17822189331054689,0.1885928750038147,0.18198935985565184,0.19685615301132203,0.1844036102294922,0.18258403539657592,0.18366990089416504,0.1895283579826355,0.17608253955841063,0.18498727083206176,0.18569526672363282,0.1800051212310791,0.18111128807067872,0.19201194047927855,0.1813047170639038,0.1937783718109131,0.18262298107147218,0.19047296047210693,0.18913645744323732,0.18087176084518433,0.19065163135528565,0.19190514087677002,0.19216259717941284,0.18591296672821045,0.18895090818405152,0.19043780565261842,0.1840517997741699,0.1820217490196228,0.181601619720459,0.18430721759796143,0.1849672555923462,0.18888790607452394,0.18383089303970337,0.18436205387115479,0.19181153774261475,0.18424843549728392,0.18122975826263427,0.1897030472755432,0.18994537591934205,0.18735532760620116,0.1843078017234802,0.19315778017044066,0.1838558793067932,0.18779032230377196,0.18902170658111572,0.18584439754486085,0.18223495483398439,0.18592679500579834,0.18456001281738282,0.1861008882522583,0.18437643051147462,0.18411978483200073,0.18726115226745604,0.1812160611152649,0.1856173038482666,0.18437951803207397,0.18516364097595214,0.17964496612548828,0.1786663055419922,0.18295325040817262,0.19431633949279786,0.17363646030426025,0.18454508781433104,0.19846113920211791,0.1805626392364502,0.19159235954284667,0.17771919965744018,0.1827751636505127,0.1887717843055725,0.18462145328521729,0.17589941024780273,0.18175871372222902,0.1863415479660034,0.18547663688659669]};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"loss by time\"};\n",
       "\n",
       "  Plotly.plot('plot-314683916', data, layout);\n",
       "})();\n",
       "});\n",
       "      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrandom\u001b[39m: \u001b[32mRandom\u001b[39m = scala.util.Random@320757a\n",
       "\u001b[36mlossSeq\u001b[39m: \u001b[32mIndexedSeq\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m0.23041152954101562\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mplot\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mScatter\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mScatter\u001b[39m(\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mres2_4\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-314683916\"\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val random = new Random\n",
    "\n",
    "val lossSeq =\n",
    "  (\n",
    "    for (iteration <- 0 to 50) yield {\n",
    "      val randomIndex = random\n",
    "        .shuffle[Int, IndexedSeq](0 until 10000) //https://issues.scala-lang.org/browse/SI-6948\n",
    "        .toArray\n",
    "      for (times <- 0 until 10000 / MiniBatchSize) yield {\n",
    "        val randomIndexArray =\n",
    "          randomIndex.slice(times * MiniBatchSize,\n",
    "                            (times + 1) * MiniBatchSize)\n",
    "          val loss = trainData(randomIndexArray)\n",
    "          if(times == 3 & iteration % 5 == 4){\n",
    "            println(\"at epoch \" + (iteration / 5 + 1) + \" loss is :\" + loss)\n",
    "          }\n",
    "          loss\n",
    "      }\n",
    "    }\n",
    "  ).flatten\n",
    "\n",
    "plotly.JupyterScala.init()\n",
    "\n",
    "val plot = Seq(\n",
    "  Scatter(lossSeq.indices, lossSeq)\n",
    ")\n",
    "\n",
    "plot.plot(\n",
    "  title = \"loss by time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and process the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like [the previous article](https://thoughtworksinc.github.io/DeepLearning.scala/demo/SoftmaxLinearClassifier.html), we read the images and corresponding label information for test data from CIFAR10 database and process them. However, here we only read the test set, and the training set is randomly read during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtestNDArray\u001b[39m: \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mHNil\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestData\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [3.00, 8.00, 8.00, 0.00, 6.00, 6.00, 1.00, 6.00, 3.00, 1.00, 0.00, 9.00, 5.00, 7.00, 9.00, 8.00, 5.00, 7.00, 8.00, 6.00, 7.00, 0.00, 4.00, 9.00, 5.00, 2.00, 4.0\u001b[33m...\u001b[39m\n",
       "\u001b[36mvectorizedTestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testNDArray =\n",
    "   ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/test_batch.bin\", 100)\n",
    "\n",
    "val testData = testNDArray.head\n",
    "\n",
    "val testExpectResult = testNDArray.tail.head\n",
    "\n",
    "val vectorizedTestExpectResult = Utils.makeVectorized(testExpectResult, NumberOfClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the neural network and predict the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the last article, we use the test data to verify the prediction result of the neural network and compute the accuracy. This time, the accuracy may increase to about 40%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the result is 37.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mright\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m37.0\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val right = Utils.getAccuracy(myNeuralNetwork.predict(testData), testExpectResult)\n",
    "println(s\"the result is $right %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have learned the follows in this article:\n",
    "\n",
    "* Mini-Batch Gradient Descent\n",
    "* epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Complete code](https://github.com/izhangzhihao/deeplearning-tutorial/blob/master/src/main/scala/com/thoughtworks/deeplearning/tutorial/MiniBatchGradientDescent.scala)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
