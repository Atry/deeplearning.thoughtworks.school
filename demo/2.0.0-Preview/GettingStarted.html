---
layout: demo
title: GettingStarted
download_path: demo_download/./2.0.0-Preview
filename: GettingStarted.ipynb
---
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Deep learning is a great tool that helps us efficiently summarize inherent patterns from tons of input data. I'd like to introduce DeepLearning.scala by letting the framework learn the common difference from Arithmetic progression.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Background">Background<a class="anchor-link" href="#Background">&#182;</a></h2><p><strong>Input</strong>:
 Arithmetic progression(AP) as:
<code>val input: INDArray = Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray</code></p>
<p><strong>Output</strong>: 
 Common Difference of the certain AP as: 
<code>val expectedOutput: INDArray = Array(Array(1), Array(3), Array(2)).toNDArray</code></p>
<p>So here we want DeepLearning.scala to learn the common difference from the AP, i.e. <code>{1} from {0, 1, 2}</code> 
in which <code>2-1 = 1-0 = 1</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Install-DeepLearning.scala">Install DeepLearning.scala<a class="anchor-link" href="#Install-DeepLearning.scala">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>DeepLearning.scala is hosted on Maven Central repository.</p>
<p>You can use magic imports in <a href="https://github.com/alexarchambault/jupyter-scala">jupyter-scala</a> or <a href="http://www.lihaoyi.com/Ammonite/#Ammonite-REPL">Ammonite-REPL</a> to download DeepLearning.scala and its dependencies.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::jupyter-differentiable:2.0.0-M1`</span>
<span class="n">interp</span><span class="o">.</span><span class="n">load</span><span class="o">.</span><span class="n">plugin</span><span class="o">.</span><span class="n">ivy</span><span class="o">(</span><span class="s">&quot;org.scalamacros&quot;</span> <span class="o">%</span> <span class="s">&quot;paradise_2.11.11&quot;</span> <span class="o">%</span> <span class="s">&quot;2.1.0&quot;</span><span class="o">)</span>

<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`org.nd4j:nd4j-native-platform:0.7.2`</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[12]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                               
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                    </span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you use <a href="http://www.scala-sbt.org">sbt</a>, please add the following settings into your <code>build.sbt</code>:</p>
<div class="highlight"><pre><span></span><span class="n">libraryDependencies</span> <span class="o">+=</span> <span class="s">&quot;com.thoughtworks.deeplearning&quot;</span> <span class="o">%%</span> <span class="s">&quot;differentiable&quot;</span> <span class="o">%</span> <span class="s">&quot;latest.release&quot;</span>

<span class="n">addCompilerPlugin</span><span class="o">(</span><span class="s">&quot;org.scalamacros&quot;</span> <span class="o">%</span> <span class="s">&quot;paradise&quot;</span> <span class="o">%</span> <span class="s">&quot;2.1.0&quot;</span> <span class="n">cross</span> <span class="nc">CrossVersion</span><span class="o">.</span><span class="n">full</span><span class="o">)</span>

<span class="n">fork</span> <span class="o">:=</span> <span class="kc">true</span>

<span class="n">scalaVersion</span> <span class="o">:=</span> <span class="s">&quot;2.11.11&quot;</span>
</pre></div>
<p>Note that this example must run on Scala 2.11.11 because <a href="http://nd4j.org/scala">nd4s</a> does not support Scala 2.12. Make sure there is not a setting like <code>scalaVersion := "2.12.x"</code> in your <code>build.sbt</code>.</p>
<p>See <a href="https://index.scala-lang.org/thoughtworksinc/deeplearning.scala">Scaladex</a> to install DeepLearning.scala in other build tools!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then, you may want to import classes in DeepLearning.scala and its dependencies.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.math._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable.Any._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.differentiable.INDArray.</span><span class="o">{</span>
  <span class="nc">Optimizer</span> <span class="k">=&gt;</span> <span class="nc">INDArrayOptimizer</span>
<span class="o">}</span>
<span class="k">import</span> <span class="nn">INDArrayOptimizer.LearningRate</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable.INDArray.implicits._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.each.Monadic._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.raii.asynchronous.Do</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable.Double._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable.Double.implicits._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Tape</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.api.ndarray.INDArray</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.factory.Nd4j</span>
<span class="k">import</span> <span class="nn">org.nd4s.Implicits._</span>
<span class="k">import</span> <span class="nn">scala.concurrent.ExecutionContext.Implicits.global</span>
<span class="k">import</span> <span class="nn">scalaz.concurrent.Task</span>
<span class="k">import</span> <span class="nn">scalaz.</span><span class="o">{-\/,</span> <span class="o">\/,</span> <span class="o">\/-}</span>
<span class="k">import</span> <span class="nn">scalaz.std.vector._</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[13]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.math._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable.Any._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.differentiable.INDArray.{
  Optimizer =&gt; INDArrayOptimizer
}
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">INDArrayOptimizer.LearningRate
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable.INDArray.implicits._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.each.Monadic._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.raii.asynchronous.Do
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable.Double._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable.Double.implicits._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Tape
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.api.ndarray.INDArray
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.factory.Nd4j
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4s.Implicits._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scala.concurrent.ExecutionContext.Implicits.global
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scalaz.concurrent.Task
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scalaz.{-\/, \/, \/-}
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scalaz.std.vector._</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Design-your-neural-network">Design your neural network<a class="anchor-link" href="#Design-your-neural-network">&#182;</a></h2><p>DeepLearning.scala is also a language that we can use to create complex neural networks.</p>
<p>In the following sections, you will learn:</p>
<ul>
<li>how to create your neural network</li>
<li>how to train your neural network</li>
<li>how to predict your neural network</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Create-your-neural-network">Create your neural network<a class="anchor-link" href="#Create-your-neural-network">&#182;</a></h3><p>Same as the definition of a normal Scala function, the definition of neural network consists of a type definition for its parameter, a type definition for its return value, and a body that contains mathematical formulas, function-calls, and control flows.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Weight-Intialization">Weight Intialization<a class="anchor-link" href="#Weight-Intialization">&#182;</a></h4><p>We will create a trainable neural network.
It means that some variables in the neural network can be changed automatically according to some goals. Those variables are called <code>weight</code>.
You can create weight variables via <code>toWeight</code> method, given its initial value.</p>
<p>In order to create a weight, you must create an <code>Optimizer</code>, which contains the rule that manages how the weight changes.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">implicit</span> <span class="k">def</span> <span class="n">optimizer</span><span class="k">:</span> <span class="kt">INDArrayOptimizer</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">LearningRate</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">currentLearningRate</span><span class="o">()</span> <span class="k">=</span> <span class="mf">0.001</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[14]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">optimizer</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">weight</span> <span class="k">=</span> <span class="o">(</span><span class="nc">Nd4j</span><span class="o">.</span><span class="n">randn</span><span class="o">(</span><span class="mi">3</span><span class="o">,</span> <span class="mi">1</span><span class="o">)</span> <span class="o">/</span> <span class="n">scala</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="o">(</span><span class="mf">3.0</span><span class="o">)).</span><span class="n">toWeight</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[15]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">weight</span>: <span class="ansi-green-fg">Do</span>[<span class="ansi-green-fg">differentiable</span>.<span class="ansi-green-fg">package</span>.<span class="ansi-green-fg">INDArray</span>.<span class="ansi-green-fg">INDArrayTape</span>] = Suspend(&lt;function0&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="define-your-neural-network">define your neural network<a class="anchor-link" href="#define-your-neural-network">&#182;</a></h4><p>Your neural network is just a normal scala function:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">def</span> <span class="n">myNeuralNetwork</span><span class="o">(</span><span class="n">input</span><span class="k">:</span> <span class="kt">INDArray</span><span class="o">)</span><span class="k">:</span> <span class="kt">differentiable.INDArray</span> <span class="o">=</span> <span class="o">{</span>
  <span class="n">dot</span><span class="o">(</span><span class="n">input</span><span class="o">,</span> <span class="n">weight</span><span class="o">)</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[16]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">myNeuralNetwork</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Train-your-Neural-Network">Train your Neural Network<a class="anchor-link" href="#Train-your-Neural-Network">&#182;</a></h2><p>You have learned that weight will be automatically changed due to some goals.</p>
<p>In DeepLearning.scala, when we train a neural network, our goal should always be minimizing the absolute of the return value.</p>
<p>For example, if someone repeatedly call <code>train(myNeuralNetwork(Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray))</code>,
the neural network would try to minimize <code>input dot weight</code>.
Soon <code>weight</code> would become an array of zeros in order to make <code>input dot weight</code> zeros,
and <code>predict(myNeuralNetwork(Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray))</code> would return <code>Array(Array(0), Array(0), Array(0)).toNDArray</code>.</p>
<p>What if you expect <code>predict(myNeuralNetwork(Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray))</code> to return <code>Array(Array(1), Array(3), Array(2)).toNDArray</code>?</p>
<p>You can create another neural network that evaluates how far between the result of <code>myNeuralNetwork</code> and your expectation. The new neural network is usually called <strong>loss function</strong>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">def</span> <span class="n">lossFunction</span><span class="o">(</span><span class="n">input</span><span class="k">:</span> <span class="kt">INDArray</span><span class="o">,</span>
                 <span class="n">expectOutput</span><span class="k">:</span> <span class="kt">INDArray</span><span class="o">)</span><span class="k">:</span> <span class="kt">differentiable.Double</span> <span class="o">=</span> <span class="o">{</span>
  <span class="n">sumT</span><span class="o">(</span><span class="n">abs</span><span class="o">(</span><span class="n">myNeuralNetwork</span><span class="o">(</span><span class="n">input</span><span class="o">)</span> <span class="o">-</span> <span class="n">expectOutput</span><span class="o">))</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[17]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">lossFunction</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When the <code>lossFunction</code> get trained continuously, its return value will be close to zero, and the result of  <code>myNeuralNetwork</code> must be close to the expected result at the same time.</p>
<p>Note the <code>lossFunction</code> accepts a <code>input</code> and <code>expectOutput</code> as its parameter.
The first array is the input data used to train the neural network, and the second array is the expected output.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then we create a plot to show how the loss changed during iterations.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[18]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`org.plotly-scala::plotly-jupyter-scala:0.3.2`</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[18]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                             </span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[19]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">import</span> <span class="nn">plotly._</span>
<span class="k">import</span> <span class="nn">plotly.element._</span>
<span class="k">import</span> <span class="nn">plotly.layout._</span>
<span class="k">import</span> <span class="nn">plotly.JupyterScala._</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[19]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.element._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.layout._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.JupyterScala._</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[20]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">def</span> <span class="n">polyLoss</span><span class="o">(</span><span class="n">lossSeq</span><span class="k">:</span> <span class="kt">IndexedSeq</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
  <span class="n">plotly</span><span class="o">.</span><span class="nc">JupyterScala</span><span class="o">.</span><span class="n">init</span><span class="o">()</span>

  <span class="k">val</span> <span class="n">plot</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span>
    <span class="nc">Scatter</span><span class="o">(</span><span class="n">lossSeq</span><span class="o">.</span><span class="n">indices</span><span class="o">,</span> <span class="n">lossSeq</span><span class="o">)</span>
  <span class="o">)</span>

  <span class="n">plot</span><span class="o">.</span><span class="n">plot</span><span class="o">(</span>
    <span class="n">title</span> <span class="k">=</span> <span class="s">&quot;loss by time&quot;</span>
  <span class="o">)</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[20]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">polyLoss</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, we hard-code some data to train the network:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[21]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="o">=</span> <span class="nc">Array</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">),</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">3</span><span class="o">,</span> <span class="mi">6</span><span class="o">,</span> <span class="mi">9</span><span class="o">),</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">13</span><span class="o">,</span> <span class="mi">15</span><span class="o">,</span> <span class="mi">17</span><span class="o">)).</span><span class="n">toNDArray</span>

<span class="k">val</span> <span class="n">expectedOutput</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="o">=</span> <span class="nc">Array</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">),</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">3</span><span class="o">),</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">2</span><span class="o">)).</span><span class="n">toNDArray</span>

<span class="nd">@monadic</span><span class="o">[</span><span class="kt">Task</span><span class="o">]</span>
<span class="k">val</span> <span class="n">trainTask</span><span class="k">:</span> <span class="kt">Task</span><span class="o">[</span><span class="kt">Unit</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>

  <span class="k">val</span> <span class="n">lossSeq</span> <span class="k">=</span> <span class="k">for</span> <span class="o">(</span><span class="k">_</span> <span class="k">&lt;-</span> <span class="o">(</span><span class="mi">1</span> <span class="n">to</span> <span class="mi">400</span><span class="o">).</span><span class="n">toVector</span><span class="o">)</span> <span class="k">yield</span> <span class="o">{</span>
    <span class="n">train</span><span class="o">(</span><span class="n">lossFunction</span><span class="o">(</span><span class="n">input</span><span class="o">,</span> <span class="n">expectedOutput</span><span class="o">)).</span><span class="n">each</span>
  <span class="o">}</span>

  <span class="n">polyLoss</span><span class="o">(</span><span class="n">lossSeq</span><span class="o">)</span>

<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[21]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">input</span>: <span class="ansi-green-fg">INDArray</span> = [[0.00, 1.00, 2.00],
 [3.00, 6.00, 9.00],
 [13.00, 15.00, 17.00]]
<span class="ansi-cyan-fg">expectedOutput</span>: <span class="ansi-green-fg">INDArray</span> = [1.00, 3.00, 2.00]
<span class="ansi-cyan-fg">trainTask</span>: <span class="ansi-green-fg">Task</span>[<span class="ansi-green-fg">Unit</span>] = scalaz.concurrent.Task@7670223f</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>@monadic</code> and <code>throwableMonadic</code> is a syntax sugar provide by <a href="https://github.com/ThoughtWorksInc/each">each</a>.</p>
<p>After those iterations, the loss should close to zero.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Predict--your-Neural-Network">Predict  your Neural Network<a class="anchor-link" href="#Predict--your-Neural-Network">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[22]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">predictResult</span> <span class="k">=</span> <span class="n">throwableMonadic</span><span class="o">[</span><span class="kt">Task</span><span class="o">]</span> <span class="o">{</span>
  <span class="n">trainTask</span><span class="o">.</span><span class="n">each</span>
  <span class="n">predict</span><span class="o">(</span><span class="n">myNeuralNetwork</span><span class="o">(</span><span class="n">input</span><span class="o">)).</span><span class="n">each</span>
<span class="o">}</span>

<span class="n">predictResult</span><span class="o">.</span><span class="n">unsafePerformSyncAttempt</span> <span class="k">match</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">-\/(</span><span class="n">e</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="k">throw</span> <span class="n">e</span>
  <span class="k">case</span> <span class="o">\/-(</span><span class="n">result</span><span class="o">)</span> <span class="k">=&gt;</span>
    <span class="n">println</span><span class="o">(</span><span class="n">result</span><span class="o">)</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>


<div class="output_html rendered_html output_subarea ">

      <script type="text/javascript">
        require.config({
  paths: {
    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',
    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min'
  },

  shim: {
    plotly: {
      deps: ['d3', 'jquery'],
      exports: 'plotly'
    }
  }
});
        

        require(['plotly'], function(Plotly) {
          window.Plotly = Plotly;
        });
      </script>
    
</div>

</div>

<div class="output_area">
<div class="prompt"></div>


<div class="output_html rendered_html output_subarea ">
<div class="chart" id="plot-159765941"></div>
</div>

</div>

<div class="output_area">
<div class="prompt"></div>




<div id="a7ade390-bd7d-46da-b3d7-e4f87612baef"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#a7ade390-bd7d-46da-b3d7-e4f87612baef');
requirejs(["plotly"], function(Plotly) {
  (function () {
  var data0 = {"type":"scatter","x":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0],"y":[27.02153778076172,25.497535705566406,23.973533630371094,22.449535369873047,20.925535202026367,19.401535034179688,17.877534866333008,16.353534698486328,14.829535484313965,13.305536270141602,11.781536102294922,10.257535934448242,8.733536720275879,7.209536552429199,5.685537338256836,4.16153621673584,2.63753604888916,1.2547836303710938,0.6380471587181091,0.7584630250930786,0.9420473575592041,0.742048442363739,0.5420476198196411,0.7464603185653687,0.9027873873710632,0.5904604196548462,1.038786768913269,0.43446046113967896,1.174787163734436,0.4620492458343506,0.7824592590332031,0.846788227558136,0.6264584064483643,0.9827895164489746,0.4704577326774597,1.1187894344329834,0.38204896450042725,0.8184572458267212,0.7907909750938416,0.6624563932418823,0.9267903566360474,0.5064564347267151,1.0627902746200562,0.35045528411865234,1.1987920999526978,0.34204983711242676,0.6984553337097168,0.8707921504974365,0.5424546599388123,1.0067930221557617,0.3864542245864868,1.1427924633026123,0.2620505094528198,0.7344523668289185,0.8147939443588257,0.5784524083137512,0.9507938623428345,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049,1.1015483140945435,0.4224522113800049]};

  var data = [data0];
  var layout = {"title":"loss by time"};

  Plotly.plot('plot-159765941', data, layout);
})();
});
      
</script>
</div>

</div>

<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>[1.01, 3.22, 2.87]
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt output_prompt">Out[22]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">predictResult</span>: <span class="ansi-green-fg">Task</span>[<span class="ansi-green-fg">Tape</span>.<span class="ansi-green-fg">&lt;refinement&gt;</span>.this.type.<span class="ansi-green-fg">Data</span>] = scalaz.concurrent.Task@235bbd14</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Summary">Summary<a class="anchor-link" href="#Summary">&#182;</a></h2><p>In this article, you have learned:</p>
<ul>
<li>to create neural networks dealing with complex data structures like <code>Double</code> and <code>INDArray</code> like ordinary programming language</li>
<li>to train your neural network</li>
<li>to predict your neural network</li>
</ul>

</div>
</div>
</div>
 

