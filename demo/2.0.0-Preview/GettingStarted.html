---
layout: demo
title: GettingStarted
download_path: demo_download/./2.0.0-Preview
filename: GettingStarted.ipynb
---
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Deep learning is a great tool that helps us efficiently summarize inherent patterns from tons of input data. I'd like to introduce DeepLearning.scala by letting the framework learn the common difference from Arithmetic progression.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Background">Background<a class="anchor-link" href="#Background">&#182;</a></h2><p><strong>Input</strong>:
 Arithmetic progression(AP) as:
<code>val input: INDArray = Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray</code></p>
<p><strong>Output</strong>: 
 Common Difference of the certain AP as: 
<code>val expectedOutput: INDArray = Array(Array(1), Array(3), Array(2)).toNDArray</code></p>
<p>So here we want DeepLearning.scala to learn the common difference from the AP, i.e. <code>{1} from {0, 1, 2}</code> 
in which <code>2-1 = 1-0 = 1</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Install-DeepLearning.scala">Install DeepLearning.scala<a class="anchor-link" href="#Install-DeepLearning.scala">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>DeepLearning.scala is hosted on Maven Central repository.</p>
<p>You can use magic imports in <a href="https://github.com/alexarchambault/jupyter-scala">jupyter-scala</a> or <a href="http://www.lihaoyi.com/Ammonite/#Ammonite-REPL">Ammonite-REPL</a> to download DeepLearning.scala and its dependencies.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[34]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::jupyter-differentiable:2.0.0-M1`</span>
<span class="k">import</span> <span class="nn">$plugin.$ivy.</span><span class="n">`org.scalamacros:paradise_2.11.11:2.1.0`</span>

<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`org.nd4j:nd4j-native-platform:0.7.2`</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[34]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                               
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$plugin.$                                            

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                    </span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you use <a href="http://www.scala-sbt.org">sbt</a>, please add the following settings into your <code>build.sbt</code>:</p>
<div class="highlight"><pre><span></span><span class="n">libraryDependencies</span> <span class="o">+=</span> <span class="s">&quot;com.thoughtworks.deeplearning&quot;</span> <span class="o">%%</span> <span class="s">&quot;differentiable&quot;</span> <span class="o">%</span> <span class="s">&quot;latest.release&quot;</span>

<span class="n">addCompilerPlugin</span><span class="o">(</span><span class="s">&quot;org.scalamacros&quot;</span> <span class="o">%</span> <span class="s">&quot;paradise&quot;</span> <span class="o">%</span> <span class="s">&quot;2.1.0&quot;</span> <span class="n">cross</span> <span class="nc">CrossVersion</span><span class="o">.</span><span class="n">full</span><span class="o">)</span>

<span class="n">fork</span> <span class="o">:=</span> <span class="kc">true</span>

<span class="n">scalaVersion</span> <span class="o">:=</span> <span class="s">&quot;2.11.11&quot;</span>
</pre></div>
<p>Note that this example must run on Scala 2.11.11 because <a href="http://nd4j.org/scala">nd4s</a> does not support Scala 2.12. Make sure there is not a setting like <code>scalaVersion := "2.12.x"</code> in your <code>build.sbt</code>.</p>
<p>See <a href="https://index.scala-lang.org/thoughtworksinc/deeplearning.scala">Scaladex</a> to install DeepLearning.scala in other build tools!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then, you may want to import classes in DeepLearning.scala and its dependencies.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[35]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.math._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable.Any._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.differentiable.INDArray.</span><span class="o">{</span>
  <span class="nc">Optimizer</span> <span class="k">=&gt;</span> <span class="nc">INDArrayOptimizer</span>
<span class="o">}</span>
<span class="k">import</span> <span class="nn">INDArrayOptimizer.LearningRate</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable.INDArray.implicits._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.each.Monadic._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.raii.asynchronous.Do</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable.Double._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable.Double.implicits._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Tape</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.api.ndarray.INDArray</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.factory.Nd4j</span>
<span class="k">import</span> <span class="nn">org.nd4s.Implicits._</span>
<span class="k">import</span> <span class="nn">scala.concurrent.ExecutionContext.Implicits.global</span>
<span class="k">import</span> <span class="nn">scalaz.concurrent.Task</span>
<span class="k">import</span> <span class="nn">scalaz.</span><span class="o">{-\/,</span> <span class="o">\/,</span> <span class="o">\/-}</span>
<span class="k">import</span> <span class="nn">scalaz.std.vector._</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[35]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.math._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable.Any._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.differentiable.INDArray.{
  Optimizer =&gt; INDArrayOptimizer
}
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">INDArrayOptimizer.LearningRate
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable.INDArray.implicits._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.each.Monadic._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.raii.asynchronous.Do
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable.Double._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable.Double.implicits._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Tape
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.api.ndarray.INDArray
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.factory.Nd4j
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4s.Implicits._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scala.concurrent.ExecutionContext.Implicits.global
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scalaz.concurrent.Task
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scalaz.{-\/, \/, \/-}
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scalaz.std.vector._</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Design-your-neural-network">Design your neural network<a class="anchor-link" href="#Design-your-neural-network">&#182;</a></h2><p>DeepLearning.scala is also a language that we can use to create complex neural networks.</p>
<p>In the following sections, you will learn:</p>
<ul>
<li>how to create your neural network</li>
<li>how to train your neural network</li>
<li>how to predict your neural network</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Create-your-neural-network">Create your neural network<a class="anchor-link" href="#Create-your-neural-network">&#182;</a></h3><p>Same as the definition of a normal Scala function, the definition of neural network consists of a type definition for its parameter, a type definition for its return value, and a body that contains mathematical formulas, function-calls, and control flows.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Weight-Intialization">Weight Intialization<a class="anchor-link" href="#Weight-Intialization">&#182;</a></h4><p>We will create a trainable neural network.
It means that some variables in the neural network can be changed automatically according to some goals. Those variables are called <code>weight</code>.
You can create weight variables via <code>toWeight</code> method, given its initial value.</p>
<p>In order to create a weight, you must create an <code>Optimizer</code>, which contains the rule that manages how the weight changes.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[36]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">implicit</span> <span class="k">def</span> <span class="n">optimizer</span><span class="k">:</span> <span class="kt">INDArrayOptimizer</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">LearningRate</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">currentLearningRate</span><span class="o">()</span> <span class="k">=</span> <span class="mf">0.001</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[36]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">optimizer</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[37]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">weight</span> <span class="k">=</span> <span class="o">(</span><span class="nc">Nd4j</span><span class="o">.</span><span class="n">randn</span><span class="o">(</span><span class="mi">3</span><span class="o">,</span> <span class="mi">1</span><span class="o">)</span> <span class="o">/</span> <span class="n">scala</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="o">(</span><span class="mf">3.0</span><span class="o">)).</span><span class="n">toWeight</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[37]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">weight</span>: <span class="ansi-green-fg">Do</span>[<span class="ansi-green-fg">differentiable</span>.<span class="ansi-green-fg">package</span>.<span class="ansi-green-fg">INDArray</span>.<span class="ansi-green-fg">INDArrayTape</span>] = Suspend(&lt;function0&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="define-your-neural-network">define your neural network<a class="anchor-link" href="#define-your-neural-network">&#182;</a></h4><p>Your neural network is just a normal scala function:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[38]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">def</span> <span class="n">myNeuralNetwork</span><span class="o">(</span><span class="n">input</span><span class="k">:</span> <span class="kt">INDArray</span><span class="o">)</span><span class="k">:</span> <span class="kt">differentiable.INDArray</span> <span class="o">=</span> <span class="o">{</span>
  <span class="n">dot</span><span class="o">(</span><span class="n">input</span><span class="o">,</span> <span class="n">weight</span><span class="o">)</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[38]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">myNeuralNetwork</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Train-your-Neural-Network">Train your Neural Network<a class="anchor-link" href="#Train-your-Neural-Network">&#182;</a></h2><p>You have learned that weight will be automatically changed due to some goals.</p>
<p>In DeepLearning.scala, when we train a neural network, our goal should always be minimizing the absolute of the return value.</p>
<p>For example, if someone repeatedly call <code>train(myNeuralNetwork(Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray))</code>,
the neural network would try to minimize <code>input dot weight</code>.
Soon <code>weight</code> would become an array of zeros in order to make <code>input dot weight</code> zeros,
and <code>predict(myNeuralNetwork(Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray))</code> would return <code>Array(Array(0), Array(0), Array(0)).toNDArray</code>.</p>
<p>What if you expect <code>predict(myNeuralNetwork(Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray))</code> to return <code>Array(Array(1), Array(3), Array(2)).toNDArray</code>?</p>
<p>You can create another neural network that evaluates how far between the result of <code>myNeuralNetwork</code> and your expectation. The new neural network is usually called <strong>loss function</strong>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[39]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">def</span> <span class="n">lossFunction</span><span class="o">(</span><span class="n">input</span><span class="k">:</span> <span class="kt">INDArray</span><span class="o">,</span>
                 <span class="n">expectOutput</span><span class="k">:</span> <span class="kt">INDArray</span><span class="o">)</span><span class="k">:</span> <span class="kt">differentiable.Double</span> <span class="o">=</span> <span class="o">{</span>
  <span class="n">sumT</span><span class="o">(</span><span class="n">abs</span><span class="o">(</span><span class="n">myNeuralNetwork</span><span class="o">(</span><span class="n">input</span><span class="o">)</span> <span class="o">-</span> <span class="n">expectOutput</span><span class="o">))</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[39]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">lossFunction</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When the <code>lossFunction</code> get trained continuously, its return value will be close to zero, and the result of  <code>myNeuralNetwork</code> must be close to the expected result at the same time.</p>
<p>Note the <code>lossFunction</code> accepts a <code>input</code> and <code>expectOutput</code> as its parameter.
The first array is the input data used to train the neural network, and the second array is the expected output.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then we create a plot to show how the loss changed during iterations.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[40]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`org.plotly-scala::plotly-jupyter-scala:0.3.2`</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[40]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                             </span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[41]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">import</span> <span class="nn">plotly._</span>
<span class="k">import</span> <span class="nn">plotly.element._</span>
<span class="k">import</span> <span class="nn">plotly.layout._</span>
<span class="k">import</span> <span class="nn">plotly.JupyterScala._</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[41]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.element._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.layout._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.JupyterScala._</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[42]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">def</span> <span class="n">polyLoss</span><span class="o">(</span><span class="n">lossSeq</span><span class="k">:</span> <span class="kt">IndexedSeq</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
  <span class="n">plotly</span><span class="o">.</span><span class="nc">JupyterScala</span><span class="o">.</span><span class="n">init</span><span class="o">()</span>

  <span class="k">val</span> <span class="n">plot</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span>
    <span class="nc">Scatter</span><span class="o">(</span><span class="n">lossSeq</span><span class="o">.</span><span class="n">indices</span><span class="o">,</span> <span class="n">lossSeq</span><span class="o">)</span>
  <span class="o">)</span>

  <span class="n">plot</span><span class="o">.</span><span class="n">plot</span><span class="o">(</span>
    <span class="n">title</span> <span class="k">=</span> <span class="s">&quot;loss by time&quot;</span>
  <span class="o">)</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[42]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">polyLoss</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, we hard-code some data to train the network:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[43]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="o">=</span> <span class="nc">Array</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">),</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">3</span><span class="o">,</span> <span class="mi">6</span><span class="o">,</span> <span class="mi">9</span><span class="o">),</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">13</span><span class="o">,</span> <span class="mi">15</span><span class="o">,</span> <span class="mi">17</span><span class="o">)).</span><span class="n">toNDArray</span>

<span class="k">val</span> <span class="n">expectedOutput</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="o">=</span> <span class="nc">Array</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">),</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">3</span><span class="o">),</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">2</span><span class="o">)).</span><span class="n">toNDArray</span>

<span class="nd">@monadic</span><span class="o">[</span><span class="kt">Task</span><span class="o">]</span>
<span class="k">val</span> <span class="n">trainTask</span><span class="k">:</span> <span class="kt">Task</span><span class="o">[</span><span class="kt">Unit</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>

  <span class="k">val</span> <span class="n">lossSeq</span> <span class="k">=</span> <span class="k">for</span> <span class="o">(</span><span class="k">_</span> <span class="k">&lt;-</span> <span class="o">(</span><span class="mi">1</span> <span class="n">to</span> <span class="mi">400</span><span class="o">).</span><span class="n">toVector</span><span class="o">)</span> <span class="k">yield</span> <span class="o">{</span>
    <span class="n">train</span><span class="o">(</span><span class="n">lossFunction</span><span class="o">(</span><span class="n">input</span><span class="o">,</span> <span class="n">expectedOutput</span><span class="o">)).</span><span class="n">each</span>
  <span class="o">}</span>

  <span class="n">polyLoss</span><span class="o">(</span><span class="n">lossSeq</span><span class="o">)</span>

<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[43]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">input</span>: <span class="ansi-green-fg">INDArray</span> = [[0.00, 1.00, 2.00],
 [3.00, 6.00, 9.00],
 [13.00, 15.00, 17.00]]
<span class="ansi-cyan-fg">expectedOutput</span>: <span class="ansi-green-fg">INDArray</span> = [1.00, 3.00, 2.00]
<span class="ansi-cyan-fg">trainTask</span>: <span class="ansi-green-fg">Task</span>[<span class="ansi-green-fg">Unit</span>] = scalaz.concurrent.Task@1d67f702</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>@monadic</code> and <code>throwableMonadic</code> is a syntax sugar provide by <a href="https://github.com/ThoughtWorksInc/each">each</a>.</p>
<p>After those iterations, the loss should close to zero.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Predict--your-Neural-Network">Predict  your Neural Network<a class="anchor-link" href="#Predict--your-Neural-Network">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[44]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">predictResult</span> <span class="k">=</span> <span class="n">throwableMonadic</span><span class="o">[</span><span class="kt">Task</span><span class="o">]</span> <span class="o">{</span>
  <span class="n">trainTask</span><span class="o">.</span><span class="n">each</span>
  <span class="n">predict</span><span class="o">(</span><span class="n">myNeuralNetwork</span><span class="o">(</span><span class="n">input</span><span class="o">)).</span><span class="n">each</span>
<span class="o">}</span>

<span class="n">predictResult</span><span class="o">.</span><span class="n">unsafePerformSyncAttempt</span> <span class="k">match</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">-\/(</span><span class="n">e</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="k">throw</span> <span class="n">e</span>
  <span class="k">case</span> <span class="o">\/-(</span><span class="n">result</span><span class="o">)</span> <span class="k">=&gt;</span>
    <span class="n">println</span><span class="o">(</span><span class="n">result</span><span class="o">)</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>


<div class="output_html rendered_html output_subarea ">

      <script type="text/javascript">
        require.config({
  paths: {
    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',
    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min'
  },

  shim: {
    plotly: {
      deps: ['d3', 'jquery'],
      exports: 'plotly'
    }
  }
});
        

        require(['plotly'], function(Plotly) {
          window.Plotly = Plotly;
        });
      </script>
    
</div>

</div>

<div class="output_area">
<div class="prompt"></div>


<div class="output_html rendered_html output_subarea ">
<div class="chart" id="plot-1858318957"></div>
</div>

</div>

<div class="output_area">
<div class="prompt"></div>




<div id="114c377d-7eb7-48e8-9a25-9069b07f6473"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#114c377d-7eb7-48e8-9a25-9069b07f6473');
requirejs(["plotly"], function(Plotly) {
  (function () {
  var data0 = {"type":"scatter","x":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0],"y":[14.311918258666992,12.787918090820312,11.263917922973633,9.739916801452637,8.215916633605957,6.761713981628418,5.529714107513428,4.297715187072754,3.0657150745391846,2.5271148681640625,2.327115058898926,2.127115249633789,1.9271148443222046,1.727115511894226,1.5271151065826416,1.804081916809082,1.8311150074005127,1.631115436553955,1.4311161041259766,1.7920804023742676,1.735115647315979,1.5351160764694214,1.3351163864135742,1.7800776958465576,1.639116644859314,1.4391162395477295,1.2640762329101562,1.7431169748306274,1.5431172847747803,1.3431172370910645,1.2520725727081299,1.647118091583252,1.4471185207366943,1.2471179962158203,1.2400703430175781,1.5511186122894287,1.351119041442871,1.1511194705963135,1.2280678749084473,1.4551200866699219,1.255120038986206,1.0551199913024902,1.2160648107528687,1.3591198921203613,1.1591205596923828,0.9591211080551147,1.204063057899475,1.2631208896636963,1.06312096118927,0.863120436668396,1.192059874534607,1.1671215295791626,0.967121958732605,0.7671216726303101,1.1800568103790283,1.0711220502853394,0.8711215257644653,0.6711228489875793,1.1680548191070557,0.9751230478286743,0.7751227021217346,0.6520535945892334,1.0791231393814087,0.8791242241859436,0.6791236400604248,0.6400508880615234,1.0337433815002441,0.5191233158111572,0.9880499243736267,0.823124885559082,0.623124361038208,0.47204774618148804,1.145744800567627,0.46312415599823,0.8200474977493286,0.8177456259727478,0.6640466451644897,0.9537457227706909,0.5080476403236389,1.0897465944290161,0.38312506675720215,0.8560454845428467,0.7617467045783997,0.7000455856323242,0.8977475166320801,0.5440446734428406,1.0337483882904053,0.38804471492767334,1.1697478294372559,0.34312498569488525,0.7360435724258423,0.8417493104934692,0.580043613910675,0.977749228477478,0.4240427017211914,1.113749623298645,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748,0.26804280281066895,1.255958080291748]};

  var data = [data0];
  var layout = {"title":"loss by time"};

  Plotly.plot('plot-1858318957', data, layout);
})();
});
      
</script>
</div>

</div>

<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>[0.93, 2.81, 2.00]
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt output_prompt">Out[44]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">predictResult</span>: <span class="ansi-green-fg">Task</span>[<span class="ansi-green-fg">Tape</span>.<span class="ansi-green-fg">&lt;refinement&gt;</span>.this.type.<span class="ansi-green-fg">Data</span>] = scalaz.concurrent.Task@27fd8d73</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Summary">Summary<a class="anchor-link" href="#Summary">&#182;</a></h2><p>In this article, you have learned:</p>
<ul>
<li>to create neural networks dealing with complex data structures like <code>Double</code> and <code>INDArray</code> like ordinary programming language</li>
<li>to train your neural network</li>
<li>to predict your neural network</li>
</ul>

</div>
</div>
</div>
 

