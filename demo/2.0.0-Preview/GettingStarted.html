---
layout: demo
title: GettingStarted
download_path: demo_download/./2.0.0-Preview
filename: GettingStarted.ipynb
---
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Deep learning is a great tool that helps us efficiently summarize inherent patterns from tons of input data. I'd like to introduce DeepLearning.scala by letting the framework learn the common difference from Arithmetic progression.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Background">Background<a class="anchor-link" href="#Background">&#182;</a></h2><p><strong>Input</strong>:
 Arithmetic progression(AP) as:
<code>val input: INDArray = Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray</code></p>
<p><strong>Output</strong>: 
 Common Difference of the certain AP as: 
<code>val expectedOutput: INDArray = Array(Array(1), Array(3), Array(2)).toNDArray</code></p>
<p>So here we want DeepLearning.scala to learn the common difference from the AP, i.e. <code>{1} from {0, 1, 2}</code> 
in which <code>2-1 = 1-0 = 1</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Install-DeepLearning.scala">Install DeepLearning.scala<a class="anchor-link" href="#Install-DeepLearning.scala">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>DeepLearning.scala is hosted on Maven Central repository.</p>
<p>You can use magic imports in <a href="https://github.com/alexarchambault/jupyter-scala">jupyter-scala</a> or <a href="http://www.lihaoyi.com/Ammonite/#Ammonite-REPL">Ammonite-REPL</a> to download DeepLearning.scala and its dependencies.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::jupyter-differentiable:2.0.0-M1`</span>
<span class="k">import</span> <span class="nn">$plugin.$ivy.</span><span class="n">`org.scalamacros:paradise_2.11.11:2.1.0`</span>

<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`org.nd4j:nd4j-native-platform:0.7.2`</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[1]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                               
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$plugin.$                                            

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                    </span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you use <a href="http://www.scala-sbt.org">sbt</a>, please add the following settings into your <code>build.sbt</code>:</p>
<div class="highlight"><pre><span></span><span class="n">libraryDependencies</span> <span class="o">+=</span> <span class="s">&quot;com.thoughtworks.deeplearning&quot;</span> <span class="o">%%</span> <span class="s">&quot;differentiable&quot;</span> <span class="o">%</span> <span class="s">&quot;latest.release&quot;</span>

<span class="n">addCompilerPlugin</span><span class="o">(</span><span class="s">&quot;org.scalamacros&quot;</span> <span class="o">%</span> <span class="s">&quot;paradise&quot;</span> <span class="o">%</span> <span class="s">&quot;2.1.0&quot;</span> <span class="n">cross</span> <span class="nc">CrossVersion</span><span class="o">.</span><span class="n">full</span><span class="o">)</span>

<span class="n">fork</span> <span class="o">:=</span> <span class="kc">true</span>

<span class="n">scalaVersion</span> <span class="o">:=</span> <span class="s">&quot;2.11.11&quot;</span>
</pre></div>
<p>Note that this example must run on Scala 2.11.11 because <a href="http://nd4j.org/scala">nd4s</a> does not support Scala 2.12. Make sure there is not a setting like <code>scalaVersion := "2.12.x"</code> in your <code>build.sbt</code>.</p>
<p>See <a href="https://index.scala-lang.org/thoughtworksinc/deeplearning.scala">Scaladex</a> to install DeepLearning.scala in other build tools!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then, you may want to import classes in DeepLearning.scala and its dependencies.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.math._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable.Any._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable.INDArray.</span><span class="o">{</span>
  <span class="nc">Optimizer</span> <span class="k">=&gt;</span> <span class="nc">INDArrayOptimizer</span>
<span class="o">}</span>
<span class="k">import</span> <span class="nn">INDArrayOptimizer.LearningRate</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable.INDArray.implicits._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.each.Monadic._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.raii.asynchronous.Do</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable.Double._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable.Double.implicits._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Tape</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.api.ndarray.INDArray</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.factory.Nd4j</span>
<span class="k">import</span> <span class="nn">org.nd4s.Implicits._</span>
<span class="k">import</span> <span class="nn">scala.concurrent.ExecutionContext.Implicits.global</span>
<span class="k">import</span> <span class="nn">scalaz.concurrent.Task</span>
<span class="k">import</span> <span class="nn">scalaz.</span><span class="o">{-\/,</span> <span class="o">\/,</span> <span class="o">\/-}</span>
<span class="k">import</span> <span class="nn">scalaz.std.vector._</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[2]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.math._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable.Any._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable.INDArray.{
  Optimizer =&gt; INDArrayOptimizer
}
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">INDArrayOptimizer.LearningRate
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable.INDArray.implicits._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.each.Monadic._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.raii.asynchronous.Do
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable.Double._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable.Double.implicits._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Tape
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.api.ndarray.INDArray
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.factory.Nd4j
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4s.Implicits._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scala.concurrent.ExecutionContext.Implicits.global
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scalaz.concurrent.Task
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scalaz.{-\/, \/, \/-}
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scalaz.std.vector._</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Design-your-neural-network">Design your neural network<a class="anchor-link" href="#Design-your-neural-network">&#182;</a></h2><p>DeepLearning.scala is also a language that we can use to create complex neural networks.</p>
<p>In the following sections, you will learn:</p>
<ul>
<li>how to create your neural network</li>
<li>how to train your neural network</li>
<li>how to predict your neural network</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Create-your-neural-network">Create your neural network<a class="anchor-link" href="#Create-your-neural-network">&#182;</a></h3><p>Same as the definition of a normal Scala function, the definition of neural network consists of a type definition for its parameter, a type definition for its return value, and a body that contains mathematical formulas, function-calls, and control flows.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Weight-Intialization">Weight Intialization<a class="anchor-link" href="#Weight-Intialization">&#182;</a></h4><p>We will create a trainable neural network.
It means that some variables in the neural network can be changed automatically according to some goals. Those variables are called <code>weight</code>.
You can create weight variables via <code>toWeight</code> method, given its initial value.</p>
<p>In order to create a weight, you must create an <code>Optimizer</code>, which contains the rule that manages how the weight changes.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">implicit</span> <span class="k">def</span> <span class="n">optimizer</span><span class="k">:</span> <span class="kt">INDArrayOptimizer</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">LearningRate</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">currentLearningRate</span><span class="o">()</span> <span class="k">=</span> <span class="mf">0.001</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[3]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">optimizer</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">weight</span> <span class="k">=</span> <span class="o">(</span><span class="nc">Nd4j</span><span class="o">.</span><span class="n">randn</span><span class="o">(</span><span class="mi">3</span><span class="o">,</span> <span class="mi">1</span><span class="o">)</span> <span class="o">/</span> <span class="n">scala</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="o">(</span><span class="mf">3.0</span><span class="o">)).</span><span class="n">toWeight</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stderr output_text">
<pre>SLF4J: Failed to load class &#34;org.slf4j.impl.StaticLoggerBinder&#34;.
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt output_prompt">Out[4]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">weight</span>: <span class="ansi-green-fg">Do</span>[<span class="ansi-green-fg">differentiable</span>.<span class="ansi-green-fg">package</span>.<span class="ansi-green-fg">INDArray</span>.<span class="ansi-green-fg">INDArrayTape</span>] = Suspend(&lt;function0&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="define-your-neural-network">define your neural network<a class="anchor-link" href="#define-your-neural-network">&#182;</a></h4><p>Your neural network is just a normal scala function:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">def</span> <span class="n">myNeuralNetwork</span><span class="o">(</span><span class="n">input</span><span class="k">:</span> <span class="kt">INDArray</span><span class="o">)</span><span class="k">:</span> <span class="kt">differentiable.INDArray</span> <span class="o">=</span> <span class="o">{</span>
  <span class="n">dot</span><span class="o">(</span><span class="n">input</span><span class="o">,</span> <span class="n">weight</span><span class="o">)</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[5]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">myNeuralNetwork</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Train-your-Neural-Network">Train your Neural Network<a class="anchor-link" href="#Train-your-Neural-Network">&#182;</a></h2><p>You have learned that weight will be automatically changed due to some goals.</p>
<p>In DeepLearning.scala, when we train a neural network, our goal should always be minimizing the absolute of the return value.</p>
<p>For example, if someone repeatedly call <code>train(myNeuralNetwork(Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray))</code>,
the neural network would try to minimize <code>input dot weight</code>.
Soon <code>weight</code> would become an array of zeros in order to make <code>input dot weight</code> zeros,
and <code>predict(myNeuralNetwork(Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray))</code> would return <code>Array(Array(0), Array(0), Array(0)).toNDArray</code>.</p>
<p>What if you expect <code>predict(myNeuralNetwork(Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray))</code> to return <code>Array(Array(1), Array(3), Array(2)).toNDArray</code>?</p>
<p>You can create another neural network that evaluates how far between the result of <code>myNeuralNetwork</code> and your expectation. The new neural network is usually called <strong>loss function</strong>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">def</span> <span class="n">lossFunction</span><span class="o">(</span><span class="n">input</span><span class="k">:</span> <span class="kt">INDArray</span><span class="o">,</span>
                 <span class="n">expectOutput</span><span class="k">:</span> <span class="kt">INDArray</span><span class="o">)</span><span class="k">:</span> <span class="kt">differentiable.Double</span> <span class="o">=</span> <span class="o">{</span>
  <span class="n">sumT</span><span class="o">(</span><span class="n">abs</span><span class="o">(</span><span class="n">myNeuralNetwork</span><span class="o">(</span><span class="n">input</span><span class="o">)</span> <span class="o">-</span> <span class="n">expectOutput</span><span class="o">))</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[6]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">lossFunction</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When the <code>lossFunction</code> get trained continuously, its return value will be close to zero, and the result of  <code>myNeuralNetwork</code> must be close to the expected result at the same time.</p>
<p>Note the <code>lossFunction</code> accepts a <code>input</code> and <code>expectOutput</code> as its parameter.
The first array is the input data used to train the neural network, and the second array is the expected output.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then we create a plot to show how the loss changed during iterations.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`org.plotly-scala::plotly-jupyter-scala:0.3.2`</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[7]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                             </span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">import</span> <span class="nn">plotly._</span>
<span class="k">import</span> <span class="nn">plotly.element._</span>
<span class="k">import</span> <span class="nn">plotly.layout._</span>
<span class="k">import</span> <span class="nn">plotly.JupyterScala._</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[8]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.element._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.layout._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.JupyterScala._</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">def</span> <span class="n">polyLoss</span><span class="o">(</span><span class="n">lossSeq</span><span class="k">:</span> <span class="kt">IndexedSeq</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
  <span class="n">plotly</span><span class="o">.</span><span class="nc">JupyterScala</span><span class="o">.</span><span class="n">init</span><span class="o">()</span>

  <span class="k">val</span> <span class="n">plot</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span>
    <span class="nc">Scatter</span><span class="o">(</span><span class="n">lossSeq</span><span class="o">.</span><span class="n">indices</span><span class="o">,</span> <span class="n">lossSeq</span><span class="o">)</span>
  <span class="o">)</span>

  <span class="n">plot</span><span class="o">.</span><span class="n">plot</span><span class="o">(</span>
    <span class="n">title</span> <span class="k">=</span> <span class="s">&quot;loss by time&quot;</span>
  <span class="o">)</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[9]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">polyLoss</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, we hard-code some data to train the network:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="o">=</span> <span class="nc">Array</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">),</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">3</span><span class="o">,</span> <span class="mi">6</span><span class="o">,</span> <span class="mi">9</span><span class="o">),</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">13</span><span class="o">,</span> <span class="mi">15</span><span class="o">,</span> <span class="mi">17</span><span class="o">)).</span><span class="n">toNDArray</span>

<span class="k">val</span> <span class="n">expectedOutput</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="o">=</span> <span class="nc">Array</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">),</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">3</span><span class="o">),</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">2</span><span class="o">)).</span><span class="n">toNDArray</span>

<span class="nd">@monadic</span><span class="o">[</span><span class="kt">Task</span><span class="o">]</span>
<span class="k">val</span> <span class="n">trainTask</span><span class="k">:</span> <span class="kt">Task</span><span class="o">[</span><span class="kt">Unit</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>

  <span class="k">val</span> <span class="n">lossSeq</span> <span class="k">=</span> <span class="k">for</span> <span class="o">(</span><span class="k">_</span> <span class="k">&lt;-</span> <span class="o">(</span><span class="mi">1</span> <span class="n">to</span> <span class="mi">400</span><span class="o">).</span><span class="n">toVector</span><span class="o">)</span> <span class="k">yield</span> <span class="o">{</span>
    <span class="n">train</span><span class="o">(</span><span class="n">lossFunction</span><span class="o">(</span><span class="n">input</span><span class="o">,</span> <span class="n">expectedOutput</span><span class="o">)).</span><span class="n">each</span>
  <span class="o">}</span>

  <span class="n">polyLoss</span><span class="o">(</span><span class="n">lossSeq</span><span class="o">)</span>

<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[10]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">input</span>: <span class="ansi-green-fg">INDArray</span> = [[0.00, 1.00, 2.00],
 [3.00, 6.00, 9.00],
 [13.00, 15.00, 17.00]]
<span class="ansi-cyan-fg">expectedOutput</span>: <span class="ansi-green-fg">INDArray</span> = [1.00, 3.00, 2.00]
<span class="ansi-cyan-fg">trainTask</span>: <span class="ansi-green-fg">Task</span>[<span class="ansi-green-fg">Unit</span>] = scalaz.concurrent.Task@a0e5dd</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>@monadic</code> and <code>throwableMonadic</code> is a syntax sugar provide by <a href="https://github.com/ThoughtWorksInc/each">each</a>.</p>
<p>After those iterations, the loss should close to zero.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Predict--your-Neural-Network">Predict  your Neural Network<a class="anchor-link" href="#Predict--your-Neural-Network">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">predictResult</span> <span class="k">=</span> <span class="n">throwableMonadic</span><span class="o">[</span><span class="kt">Task</span><span class="o">]</span> <span class="o">{</span>
  <span class="n">trainTask</span><span class="o">.</span><span class="n">each</span>
  <span class="n">predict</span><span class="o">(</span><span class="n">myNeuralNetwork</span><span class="o">(</span><span class="n">input</span><span class="o">)).</span><span class="n">each</span>
<span class="o">}</span>

<span class="n">predictResult</span><span class="o">.</span><span class="n">unsafePerformSyncAttempt</span> <span class="k">match</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">-\/(</span><span class="n">e</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="k">throw</span> <span class="n">e</span>
  <span class="k">case</span> <span class="o">\/-(</span><span class="n">result</span><span class="o">)</span> <span class="k">=&gt;</span>
    <span class="n">println</span><span class="o">(</span><span class="n">result</span><span class="o">)</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>


<div class="output_html rendered_html output_subarea ">

      <script type="text/javascript">
        require.config({
  paths: {
    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',
    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min'
  },

  shim: {
    plotly: {
      deps: ['d3', 'jquery'],
      exports: 'plotly'
    }
  }
});
        

        require(['plotly'], function(Plotly) {
          window.Plotly = Plotly;
        });
      </script>
    
</div>

</div>

<div class="output_area">
<div class="prompt"></div>


<div class="output_html rendered_html output_subarea ">
<div class="chart" id="plot-1342930159"></div>
</div>

</div>

<div class="output_area">
<div class="prompt"></div>




<div id="bb8d20c6-bea0-465b-b6f4-d292d06080d6"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#bb8d20c6-bea0-465b-b6f4-d292d06080d6');
requirejs(["plotly"], function(Plotly) {
  (function () {
  var data0 = {"type":"scatter","x":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0],"y":[36.33150863647461,34.80751037597656,33.28350830078125,31.759510040283203,30.235511779785156,28.711509704589844,27.187511444091797,25.663509368896484,24.139509201049805,22.615509033203125,21.091508865356445,19.567508697509766,18.043508529663086,16.519508361816406,14.995509147644043,13.471508026123047,11.947509765625,10.423508644104004,8.89950942993164,7.375508785247803,5.851509094238281,4.327508926391602,2.8035082817077637,1.573883295059204,1.7835090160369873,1.8778831958770752,1.6778831481933594,1.4778826236724854,1.771510362625122,1.7818831205368042,1.5818829536437988,1.381882667541504,1.7595109939575195,1.6858826875686646,1.4858826398849487,1.2858827114105225,1.7475119829177856,1.5898823738098145,1.3898829221725464,1.2315120697021484,1.693882703781128,1.493882656097412,1.2938826084136963,1.2195130586624146,1.5978827476501465,1.3978826999664307,1.1978826522827148,1.2075132131576538,1.5018830299377441,1.3018829822540283,1.1018831729888916,1.1955137252807617,1.405882716178894,1.2058830261230469,1.005882740020752,1.1835148334503174,1.3098832368850708,1.109883189201355,0.9098832011222839,1.1715154647827148,1.2138831615447998,1.0138832330703735,0.8138832449913025,1.1595158576965332,1.1178832054138184,0.9178836941719055,0.7178837060928345,1.1475164890289307,1.021883249282837,0.8218832612037659,0.631517231464386,1.1258831024169922,0.9258832335472107,0.7258830070495605,0.6195171475410461,1.066076397895813,0.5658833980560303,0.9675174355506897,0.8698834180831909,0.669883668422699,0.46988391876220703,0.9555180072784424,0.7738839387893677,0.5738834142684937,0.43951839208602905,1.1540753841400146,0.41388386487960815,0.7875179648399353,0.8260751366615295,0.6315178275108337,0.96207594871521,0.4755181670188904,1.0980761051177979,0.3338843584060669,0.8235175013542175,0.7700760364532471,0.6675180196762085,0.9060771465301514,0.5115171670913696,1.0420763492584229,0.35551726818084717,1.1780767440795898,0.2938838601112366,0.7035170793533325,0.8500768542289734,0.5475171208381653,0.9860772490501404,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637,1.132482886314392,0.3915169835090637]};

  var data = [data0];
  var layout = {"title":"loss by time"};

  Plotly.plot('plot-1342930159', data, layout);
})();
});
      
</script>
</div>

</div>

<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>[1.01, 3.22, 2.91]
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt output_prompt">Out[11]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">predictResult</span>: <span class="ansi-green-fg">Task</span>[<span class="ansi-green-fg">Tape</span>.<span class="ansi-green-fg">&lt;refinement&gt;</span>.this.type.<span class="ansi-green-fg">Data</span>] = scalaz.concurrent.Task@1476e4d2</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Summary">Summary<a class="anchor-link" href="#Summary">&#182;</a></h2><p>In this article, you have learned:</p>
<ul>
<li>to create neural networks dealing with complex data structures like <code>Double</code> and <code>INDArray</code> like ordinary programming language</li>
<li>to train your neural network</li>
<li>to predict your neural network</li>
</ul>

</div>
</div>
</div>
 

