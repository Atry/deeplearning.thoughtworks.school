<!DOCTYPE HTML>
<!--
DeepLearning.scala by ThoughtWorks
Released for free under the Apache 2.0 license (https://github.com/ThoughtWorksInc/DeepLearning.scala/blob/1.0.x/LICENSE)
-->
<html>
<head>
	<title>DeepLearning.scala by ThoughtWorks</title>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<meta name="description" content="DeepLearning.Scala" />
	<meta name="keywords" content="DeepLearning,Scala" />
	<link href='https://fonts.googleapis.com/css?family=Roboto:400,100,300,700,500,900' rel='stylesheet' type='text/css'>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js" type="text/javascript"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js" type="text/javascript"></script>
	<script src="/assets/js/skel.min.js"> </script>
	<script src="/assets/js/skel-panels.min.js"></script>
	<script src="/assets/js/init.js"> </script>
	<script>
		init_init("/assets/css/style");
	</script>
	<noscript>
		<link rel="stylesheet" href="/assets/css/skel-noscript.css" />
		<link rel="stylesheet" href="/assets/css/style.css" />
		<link rel="stylesheet" href="/assets/css/style-desktop.css" />
	</noscript>
	<style>
		
	</style>
</head>



<body >
	<!-- Header -->
	<div id="header">
		<div id="nav-wrapper"><div class="container">
	<!-- Nav -->
	<nav id="nav">
		<ul>
			
			<li >
				<a href="/index.html">
					<img alt="DeepLearning.scala"
					     src="/assets/images/logo-text-white-opacity.png" 
					     style="height: 1.5em; vertical-align: text-bottom;">
				</a>
			</li>
			<li ><a href="/doc">Documentation</a></li>
			<li ><a href="/news">News</a></li>
			<li ><a href="/get-involved">Get Involved</a></li>
		</ul>
	</nav>
</div></div>

		<div class="container">

			<!-- Logo -->
			<div id="logo">
				<h1><a><img width="65%" alt="DeepLearning.scala" src="/assets/images/logo-text-white-opacity.png"/></a></h1>
				<span class="tag">By ThoughtWorks</span>
			</div>
		</div>
	</div>
	<!-- Header -->
	<div id="main">
    <div class="container">
        <div class="row">
            <div class="col-md-3">
                <nav class="toc" role="navigation">
                    
<ul class="default">
    
    <li>
        <a href="/doc/">
            
            
            Introduction
            
        </a>
    </li>
    
    
<ul class="default">
    
    <li>
        <a href="/demo/GettingStarted.html">
            
            
            GettingStarted
            
        </a>
    </li>
    
    
    <li>
        <a href="/demo/Debug.html">
            
            
            Debug
            
        </a>
    </li>
    
    
    <li>
        <a href="/demo/SoftmaxLinearClassifier.html">
            
            
            SoftmaxLinearClassifier
            
        </a>
    </li>
    
    
    <li>
        <a href="/demo/MiniBatchGradientDescent.html">
            
            
            MiniBatchGradientDescent
            
        </a>
    </li>
    
    
    <li>
        <a href="/demo/TwoLayerNet.html">
            
            
            TwoLayerNet
            
        </a>
    </li>
    
    
    <li>
        <a href="/demo/CNNs.html">
            
            
            CNNs
            
        </a>
    </li>
    
    
</ul>

    
    
    <li>
        <a href="/doc/">
            
            
            2.0.0-Preview
            
        </a>
    </li>
    
    
<ul class="default">
    
    <li>
        <a href="/demo/2.0.0-Preview/GettingStarted.html">
            
            
            GettingStarted
            
        </a>
    </li>
    
    
    <li>
        <a href="/demo/2.0.0-Preview/SoftmaxLinearClassifier.html">
            
            
            SoftmaxLinearClassifier
            
        </a>
    </li>
    
    
    <li>
        <a href="/demo/2.0.0-Preview/MiniBatchGradientDescent.html">
            
            
            <b>MiniBatchGradientDescent</b>
            
        </a>
    </li>
    
    
</ul>

    
    
</ul>

                </nav>
            </div>
            <div id="content" class="8u skel-cell-important">
                <section>
                    <div class="col-md-9 normal-head">
                        <div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Background">Background<a class="anchor-link" href="#Background">&#182;</a></h2><p>During large-scale date training, the order of magnitude of data can reach millions. If a parameter is acquired via the computation of the whole training set, the update speed will be too slow. To solve this problem, a common used method is <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Mini-Batch Gradient Descent</a> which computes mini-batche data in the training set, resulting faster training of parameters in a neural network.</p>
<p>In this article, we will first define a softmax classifier, then use the training set of <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR10</a> to train this neural network, and finally use the test set to verify the accuracy of the neural network. The difference is that we will use Mini-Batch Gradient Descent, thus the accuracy of the neural network can reach 40%.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Import-dependencies-&amp;-build-your-own-neural-network.">Import dependencies &amp; build your own neural network.<a class="anchor-link" href="#Import-dependencies-&amp;-build-your-own-neural-network.">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Like the <a href="http://deeplearning.thoughtworks.school/demo/2.0.0-Preview/SoftmaxLinearClassifier.html">previous course</a>, we need to introduce each class of DeepLearning.scala.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">import</span> <span class="nn">$plugin.$ivy.</span><span class="n">`org.scalamacros:paradise_2.11.11:2.1.0`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::jupyter-differentiable:2.0.0-M1`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`org.nd4j:nd4j-native-platform:0.7.2`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`org.rauschig:jarchivelib:0.5.0`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`org.plotly-scala::plotly-jupyter-scala:0.3.2`</span>
<span class="k">import</span> <span class="nn">$url.</span><span class="o">{</span><span class="n">`https://raw.githubusercontent.com/ThoughtWorksInc/DeepLearning.scala-website/master/ipynbs/ReadCIFAR10ToNDArray.sc`</span> <span class="k">=&gt;</span> <span class="nc">ReadCIFAR10ToNDArray</span><span class="o">}</span>
<span class="k">import</span> <span class="nn">$url.</span><span class="o">{</span><span class="n">`https://raw.githubusercontent.com/ThoughtWorksInc/DeepLearning.scala-website/master/ipynbs/Utils.sc`</span> <span class="k">=&gt;</span> <span class="nc">Utils</span><span class="o">}</span>


<span class="k">import</span> <span class="nn">java.io.</span><span class="o">{</span><span class="nc">FileInputStream</span><span class="o">,</span> <span class="nc">InputStream</span><span class="o">}</span>

<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.math._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable.Any._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable.INDArray.</span><span class="o">{</span>
  <span class="nc">Optimizer</span> <span class="k">=&gt;</span> <span class="nc">INDArrayOptimizer</span>
<span class="o">}</span>
<span class="k">import</span> <span class="nn">INDArrayOptimizer.LearningRate</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable.INDArray.implicits._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.each.Monadic._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.raii.asynchronous.Do</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable.Double._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable.Double.implicits._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Tape</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.jupyter.differentiable</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.api.ndarray.INDArray</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.factory.Nd4j</span>
<span class="k">import</span> <span class="nn">org.nd4s.Implicits._</span>
<span class="k">import</span> <span class="nn">scala.concurrent.ExecutionContext.Implicits.global</span>
<span class="k">import</span> <span class="nn">scalaz.concurrent.Task</span>
<span class="k">import</span> <span class="nn">scalaz.</span><span class="o">{-\/,</span> <span class="o">\/,</span> <span class="o">\/-}</span>
<span class="k">import</span> <span class="nn">scalaz.std.vector._</span>
<span class="k">import</span> <span class="nn">shapeless._</span>
<span class="k">import</span> <span class="nn">plotly._</span>
<span class="k">import</span> <span class="nn">plotly.element._</span>
<span class="k">import</span> <span class="nn">plotly.layout._</span>
<span class="k">import</span> <span class="nn">plotly.JupyterScala._</span>

<span class="k">import</span> <span class="nn">scala.collection.immutable.IndexedSeq</span>

<span class="n">pprintConfig</span><span class="o">()</span> <span class="k">=</span> <span class="n">pprintConfig</span><span class="o">().</span><span class="n">copy</span><span class="o">(</span><span class="n">height</span> <span class="k">=</span> <span class="mi">2</span><span class="o">)</span>

<span class="k">implicit</span> <span class="k">def</span> <span class="n">optimizer</span><span class="k">:</span> <span class="kt">INDArrayOptimizer</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">LearningRate</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">currentLearningRate</span><span class="o">()</span> <span class="k">=</span> <span class="mf">0.00001</span>
<span class="o">}</span>

<span class="k">def</span> <span class="n">softmax</span><span class="o">(</span><span class="n">scores</span><span class="k">:</span> <span class="kt">differentiable.INDArray</span><span class="o">)</span><span class="k">:</span> <span class="kt">differentiable.INDArray</span> <span class="o">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">expScores</span> <span class="k">=</span> <span class="n">exp</span><span class="o">(</span><span class="n">scores</span><span class="o">)</span>
  <span class="n">expScores</span> <span class="o">/</span> <span class="n">sum</span><span class="o">(</span><span class="n">expScores</span><span class="o">,</span> <span class="mi">1</span><span class="o">)</span>
<span class="o">}</span>

<span class="c1">//10 label of CIFAR10 images(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)</span>
<span class="k">val</span> <span class="nc">NumberOfClasses</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">val</span> <span class="nc">NumberOfPixels</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="mi">3072</span>

<span class="k">val</span> <span class="n">weight</span><span class="k">:</span> <span class="kt">differentiable.INDArray</span> <span class="o">=</span>
  <span class="o">(</span><span class="nc">Nd4j</span><span class="o">.</span><span class="n">randn</span><span class="o">(</span><span class="nc">NumberOfPixels</span><span class="o">,</span> <span class="nc">NumberOfClasses</span><span class="o">)</span> <span class="o">*</span> <span class="mf">0.001</span><span class="o">).</span><span class="n">toWeight</span>

<span class="k">def</span> <span class="n">myNeuralNetwork</span><span class="o">(</span><span class="n">input</span><span class="k">:</span> <span class="kt">INDArray</span><span class="o">)</span><span class="k">:</span> <span class="kt">differentiable.INDArray</span> <span class="o">=</span> <span class="o">{</span>
  <span class="n">softmax</span><span class="o">(</span><span class="n">dot</span><span class="o">(</span><span class="n">input</span><span class="o">,</span> <span class="n">weight</span><span class="o">))</span>
<span class="o">}</span>

<span class="k">def</span> <span class="n">lossFunction</span><span class="o">(</span><span class="n">input</span><span class="k">:</span> <span class="kt">INDArray</span><span class="o">,</span>
                 <span class="n">expectOutput</span><span class="k">:</span> <span class="kt">INDArray</span><span class="o">)</span><span class="k">:</span> <span class="kt">differentiable.Double</span> <span class="o">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">probabilities</span> <span class="k">=</span> <span class="n">myNeuralNetwork</span><span class="o">(</span><span class="n">input</span><span class="o">)</span>
  <span class="o">-</span><span class="n">mean</span><span class="o">(</span><span class="n">log</span><span class="o">(</span><span class="n">probabilities</span><span class="o">)</span> <span class="o">*</span> <span class="n">expectOutput</span><span class="o">)</span>
<span class="o">}</span>
             
<span class="n">plotly</span><span class="o">.</span><span class="nc">JupyterScala</span><span class="o">.</span><span class="n">init</span><span class="o">()</span>
<span class="k">def</span> <span class="n">polyLoss</span><span class="o">(</span><span class="n">lossSeq</span><span class="k">:</span> <span class="kt">IndexedSeq</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
  <span class="n">plotly</span><span class="o">.</span><span class="nc">JupyterScala</span><span class="o">.</span><span class="n">init</span><span class="o">()</span>

  <span class="k">val</span> <span class="n">plot</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span>
    <span class="nc">Scatter</span><span class="o">(</span><span class="n">lossSeq</span><span class="o">.</span><span class="n">indices</span><span class="o">,</span> <span class="n">lossSeq</span><span class="o">)</span>
  <span class="o">)</span>

  <span class="n">plot</span><span class="o">.</span><span class="n">plot</span><span class="o">(</span>
    <span class="n">title</span> <span class="k">=</span> <span class="s">&quot;loss by time&quot;</span>
  <span class="o">)</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stderr output_text">
<pre>SLF4J: Failed to load class &#34;org.slf4j.impl.StaticLoggerBinder&#34;.
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt"></div>


<div class="output_html rendered_html output_subarea ">

      <script type="text/javascript">
        require.config({
  paths: {
    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',
    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min'
  },

  shim: {
    plotly: {
      deps: ['d3', 'jquery'],
      exports: 'plotly'
    }
  }
});
        

        require(['plotly'], function(Plotly) {
          window.Plotly = Plotly;
        });
      </script>
    
</div>

</div>

<div class="output_area">
<div class="prompt output_prompt">Out[1]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$plugin.$                                            
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                               
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                    
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                               
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                             
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$url.$                                                                                                                                             
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$url.$                                                                                                               


</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">java.io.{FileInputStream, InputStream}

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.math._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable.Any._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable.INDArray.{
  Optimizer =&gt; INDArrayOptimizer
}
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">INDArrayOptimizer.LearningRate
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable.INDArray.implicits._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.each.Monadic._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.raii.asynchronous.Do
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable.Double._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable.Double.implicits._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Tape
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.jupyter.differentiable
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.api.ndarray.INDArray
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.factory.Nd4j
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4s.Implicits._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scala.concurrent.ExecutionContext.Implicits.global
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scalaz.concurrent.Task
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scalaz.{-\/, \/, \/-}
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scalaz.std.vector._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">shapeless._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.element._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.layout._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.JupyterScala._

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scala.collection.immutable.IndexedSeq

</span>
defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">optimizer</span>
defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">softmax</span>
<span class="ansi-cyan-fg">NumberOfClasses</span>: <span class="ansi-green-fg">Int</span> = <span class="ansi-green-fg">10</span>
<span class="ansi-cyan-fg">NumberOfPixels</span>: <span class="ansi-green-fg">Int</span> = <span class="ansi-green-fg">3072</span>
<span class="ansi-cyan-fg">weight</span>: <span class="ansi-green-fg">com</span>.<span class="ansi-green-fg">thoughtworks</span>.<span class="ansi-green-fg">deeplearning</span>.<span class="ansi-green-fg">jupyter</span>.<span class="ansi-green-fg">differentiable</span>.<span class="ansi-green-fg">package</span>.<span class="ansi-green-fg">INDArray</span> = Suspend(&lt;function0&gt;)
defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">myNeuralNetwork</span>
defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">lossFunction</span>
defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">polyLoss</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Disrupt-the-order-of-a-sequence-once-for-each-epoch,-and-generate-the-random-arrays.">Disrupt the order of a sequence once for each <a href="http://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks">epoch</a>, and generate the random arrays.<a class="anchor-link" href="#Disrupt-the-order-of-a-sequence-once-for-each-epoch,-and-generate-the-random-arrays.">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="nd">@monadic</span><span class="o">[</span><span class="kt">Task</span><span class="o">]</span>
<span class="k">val</span> <span class="n">trainTask</span><span class="k">:</span> <span class="kt">Task</span><span class="o">[</span><span class="kt">Unit</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>

  <span class="k">val</span> <span class="n">random</span> <span class="k">=</span> <span class="k">new</span> <span class="n">scala</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="nc">Random</span>

  <span class="k">val</span> <span class="nc">MiniBatchSize</span> <span class="k">=</span> <span class="mi">256</span>

  <span class="k">val</span> <span class="n">lossSeq</span> <span class="k">=</span>
    <span class="o">(</span>
      <span class="k">for</span> <span class="o">(</span><span class="n">iteration</span> <span class="k">&lt;-</span> <span class="o">(</span><span class="mi">0</span> <span class="n">to</span> <span class="mi">50</span><span class="o">).</span><span class="n">toVector</span><span class="o">)</span> <span class="k">yield</span> <span class="o">{</span>
        <span class="k">val</span> <span class="n">randomIndex</span> <span class="k">=</span> <span class="n">random</span>
          <span class="o">.</span><span class="n">shuffle</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">IndexedSeq</span><span class="o">](</span><span class="mi">0</span> <span class="n">until</span> <span class="mi">10000</span><span class="o">)</span> <span class="c1">//https://issues.scala-lang.org/browse/SI-6948</span>
          <span class="o">.</span><span class="n">toArray</span>
        <span class="k">for</span> <span class="o">(</span><span class="n">times</span> <span class="k">&lt;-</span> <span class="o">(</span><span class="mi">0</span> <span class="n">until</span> <span class="mi">10000</span> <span class="o">/</span> <span class="nc">MiniBatchSize</span><span class="o">).</span><span class="n">toVector</span><span class="o">)</span> <span class="k">yield</span> <span class="o">{</span>
          <span class="k">val</span> <span class="n">randomIndexArray</span> <span class="k">=</span>
            <span class="n">randomIndex</span><span class="o">.</span><span class="n">slice</span><span class="o">(</span><span class="n">times</span> <span class="o">*</span> <span class="nc">MiniBatchSize</span><span class="o">,</span>
                              <span class="o">(</span><span class="n">times</span> <span class="o">+</span> <span class="mi">1</span><span class="o">)</span> <span class="o">*</span> <span class="nc">MiniBatchSize</span><span class="o">)</span>
          <span class="k">val</span> <span class="n">trainNDArray</span> <span class="o">::</span> <span class="n">expectLabel</span> <span class="o">::</span> <span class="n">shapeless</span><span class="o">.</span><span class="nc">HNil</span> <span class="k">=</span>
            <span class="nc">ReadCIFAR10ToNDArray</span><span class="o">.</span><span class="n">getSGDTrainNDArray</span><span class="o">(</span><span class="n">randomIndexArray</span><span class="o">)</span>
          <span class="k">val</span> <span class="n">input</span> <span class="k">=</span>
            <span class="n">trainNDArray</span><span class="o">.</span><span class="n">reshape</span><span class="o">(</span><span class="nc">MiniBatchSize</span><span class="o">,</span> <span class="mi">3072</span><span class="o">)</span>

          <span class="k">val</span> <span class="n">expectLabelVectorized</span> <span class="k">=</span>
            <span class="nc">Utils</span><span class="o">.</span><span class="n">makeVectorized</span><span class="o">(</span><span class="n">expectLabel</span><span class="o">,</span> <span class="nc">NumberOfClasses</span><span class="o">)</span>
          <span class="k">val</span> <span class="n">loss</span> <span class="k">=</span> <span class="n">train</span><span class="o">(</span><span class="n">lossFunction</span><span class="o">(</span><span class="n">input</span><span class="o">,</span> <span class="n">expectLabelVectorized</span><span class="o">)).</span><span class="n">each</span>
          <span class="k">if</span><span class="o">(</span><span class="n">times</span> <span class="o">==</span> <span class="mi">3</span> <span class="o">&amp;</span> <span class="n">iteration</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">4</span><span class="o">){</span>
            <span class="n">println</span><span class="o">(</span><span class="s">&quot;at epoch &quot;</span> <span class="o">+</span> <span class="o">(</span><span class="n">iteration</span> <span class="o">/</span> <span class="mi">5</span> <span class="o">+</span> <span class="mi">1</span><span class="o">)</span> <span class="o">+</span> <span class="s">&quot; loss is :&quot;</span> <span class="o">+</span> <span class="n">loss</span><span class="o">)</span>
          <span class="o">}</span>
          <span class="n">loss</span>
        <span class="o">}</span>
      <span class="o">}</span>
    <span class="o">).</span><span class="n">flatten</span>

  <span class="n">polyLoss</span><span class="o">(</span><span class="n">lossSeq</span><span class="o">)</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[2]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">trainTask</span>: <span class="ansi-green-fg">Task</span>[<span class="ansi-green-fg">Unit</span>] = scalaz.concurrent.Task@396c2d6b</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Prepare-and-process-the-test-set">Prepare and process the test set<a class="anchor-link" href="#Prepare-and-process-the-test-set">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Like <a href="http://deeplearning.thoughtworks.school/demo/2.0.0-Preview/SoftmaxLinearClassifier.html">the previous article</a>, we read the images and corresponding label information for test data from CIFAR10 database and process them. However, here we only read the test set, and the training set is randomly read during training.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">testNDArray</span> <span class="k">=</span>
   <span class="nc">ReadCIFAR10ToNDArray</span><span class="o">.</span><span class="n">readFromResource</span><span class="o">(</span><span class="s">&quot;/cifar-10-batches-bin/test_batch.bin&quot;</span><span class="o">,</span> <span class="mi">100</span><span class="o">)</span>

<span class="k">val</span> <span class="n">testData</span> <span class="k">=</span> <span class="n">testNDArray</span><span class="o">.</span><span class="n">head</span>

<span class="k">val</span> <span class="n">testExpectResult</span> <span class="k">=</span> <span class="n">testNDArray</span><span class="o">.</span><span class="n">tail</span><span class="o">.</span><span class="n">head</span>

<span class="k">val</span> <span class="n">vectorizedTestExpectResult</span> <span class="k">=</span> <span class="nc">Utils</span><span class="o">.</span><span class="n">makeVectorized</span><span class="o">(</span><span class="n">testExpectResult</span><span class="o">,</span> <span class="nc">NumberOfClasses</span><span class="o">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[3]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">testNDArray</span>: <span class="ansi-green-fg">INDArray</span> <span class="ansi-green-fg">::</span> <span class="ansi-green-fg">INDArray</span> <span class="ansi-green-fg">::</span> <span class="ansi-green-fg">HNil</span> = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">testData</span>: <span class="ansi-green-fg">INDArray</span> = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">testExpectResult</span>: <span class="ansi-green-fg">INDArray</span> = [3.00, 8.00, 8.00, 0.00, 6.00, 6.00, 1.00, 6.00, 3.00, 1.00, 0.00, 9.00, 5.00, 7.00, 9.00, 8.00, 5.00, 7.00, 8.00, 6.00, 7.00, 0.00, 4.00, 9.00, 5.00, 2.00, 4.0<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">vectorizedTestExpectResult</span>: <span class="ansi-green-fg">INDArray</span> = [[0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
 [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],
<span class="ansi-yellow-fg">...</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Train-&amp;-Predict-your-Neural-Network">Train &amp; Predict your Neural Network<a class="anchor-link" href="#Train-&amp;-Predict-your-Neural-Network">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">predictResult</span> <span class="k">=</span> <span class="n">throwableMonadic</span><span class="o">[</span><span class="kt">Task</span><span class="o">]</span> <span class="o">{</span>
  <span class="n">trainTask</span><span class="o">.</span><span class="n">each</span>
  <span class="n">predict</span><span class="o">(</span><span class="n">myNeuralNetwork</span><span class="o">(</span><span class="n">testData</span><span class="o">)).</span><span class="n">each</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[4]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">predictResult</span>: <span class="ansi-green-fg">Task</span>[<span class="ansi-green-fg">Tape</span>.<span class="ansi-green-fg">&lt;refinement&gt;</span>.this.type.<span class="ansi-green-fg">Data</span>] = scalaz.concurrent.Task@4c5663bd</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Verify-the-accuracy">Verify the accuracy<a class="anchor-link" href="#Verify-the-accuracy">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Just like the last article, we use the test data to verify the prediction result of the neural network and compute the accuracy. This time, the accuracy may increase to about 41%.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="n">predictResult</span><span class="o">.</span><span class="n">unsafePerformSyncAttempt</span> <span class="k">match</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">-\/(</span><span class="n">e</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">{</span>
    <span class="k">throw</span> <span class="n">e</span>
  <span class="o">}</span>
  <span class="k">case</span> <span class="o">\/-(</span><span class="n">result</span><span class="o">)</span> <span class="k">=&gt;</span>
    <span class="n">println</span><span class="o">(</span><span class="s">&quot;The accuracy is &quot;</span> <span class="o">+</span> <span class="nc">Utils</span><span class="o">.</span><span class="n">getAccuracy</span><span class="o">(</span><span class="n">result</span><span class="o">,</span><span class="n">testExpectResult</span><span class="o">)</span> <span class="o">+</span> <span class="s">&quot;%&quot;</span><span class="o">)</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>at epoch 1 loss is :0.2140218734741211
at epoch 2 loss is :0.19828615188598633
at epoch 3 loss is :0.1983615279197693
at epoch 4 loss is :0.19120612144470214
at epoch 5 loss is :0.19014278650283814
at epoch 6 loss is :0.19041664600372316
at epoch 7 loss is :0.17840851545333863
at epoch 8 loss is :0.18262848854064942
at epoch 9 loss is :0.18263672590255736
at epoch 10 loss is :0.189388644695282
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt"></div>


<div class="output_html rendered_html output_subarea ">

      <script type="text/javascript">
        require.config({
  paths: {
    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',
    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min'
  },

  shim: {
    plotly: {
      deps: ['d3', 'jquery'],
      exports: 'plotly'
    }
  }
});
        

        require(['plotly'], function(Plotly) {
          window.Plotly = Plotly;
        });
      </script>
    
</div>

</div>

<div class="output_area">
<div class="prompt"></div>


<div class="output_html rendered_html output_subarea ">
<div class="chart" id="plot-1915618569"></div>
</div>

</div>

<div class="output_area">
<div class="prompt"></div>




<div id="123e4dc4-5a5d-4de7-8147-be8722a2ad22"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#123e4dc4-5a5d-4de7-8147-be8722a2ad22');
requirejs(["plotly"], function(Plotly) {
  (function () {
  var data0 = {"type":"scatter","x":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0,1001.0,1002.0,1003.0,1004.0,1005.0,1006.0,1007.0,1008.0,1009.0,1010.0,1011.0,1012.0,1013.0,1014.0,1015.0,1016.0,1017.0,1018.0,1019.0,1020.0,1021.0,1022.0,1023.0,1024.0,1025.0,1026.0,1027.0,1028.0,1029.0,1030.0,1031.0,1032.0,1033.0,1034.0,1035.0,1036.0,1037.0,1038.0,1039.0,1040.0,1041.0,1042.0,1043.0,1044.0,1045.0,1046.0,1047.0,1048.0,1049.0,1050.0,1051.0,1052.0,1053.0,1054.0,1055.0,1056.0,1057.0,1058.0,1059.0,1060.0,1061.0,1062.0,1063.0,1064.0,1065.0,1066.0,1067.0,1068.0,1069.0,1070.0,1071.0,1072.0,1073.0,1074.0,1075.0,1076.0,1077.0,1078.0,1079.0,1080.0,1081.0,1082.0,1083.0,1084.0,1085.0,1086.0,1087.0,1088.0,1089.0,1090.0,1091.0,1092.0,1093.0,1094.0,1095.0,1096.0,1097.0,1098.0,1099.0,1100.0,1101.0,1102.0,1103.0,1104.0,1105.0,1106.0,1107.0,1108.0,1109.0,1110.0,1111.0,1112.0,1113.0,1114.0,1115.0,1116.0,1117.0,1118.0,1119.0,1120.0,1121.0,1122.0,1123.0,1124.0,1125.0,1126.0,1127.0,1128.0,1129.0,1130.0,1131.0,1132.0,1133.0,1134.0,1135.0,1136.0,1137.0,1138.0,1139.0,1140.0,1141.0,1142.0,1143.0,1144.0,1145.0,1146.0,1147.0,1148.0,1149.0,1150.0,1151.0,1152.0,1153.0,1154.0,1155.0,1156.0,1157.0,1158.0,1159.0,1160.0,1161.0,1162.0,1163.0,1164.0,1165.0,1166.0,1167.0,1168.0,1169.0,1170.0,1171.0,1172.0,1173.0,1174.0,1175.0,1176.0,1177.0,1178.0,1179.0,1180.0,1181.0,1182.0,1183.0,1184.0,1185.0,1186.0,1187.0,1188.0,1189.0,1190.0,1191.0,1192.0,1193.0,1194.0,1195.0,1196.0,1197.0,1198.0,1199.0,1200.0,1201.0,1202.0,1203.0,1204.0,1205.0,1206.0,1207.0,1208.0,1209.0,1210.0,1211.0,1212.0,1213.0,1214.0,1215.0,1216.0,1217.0,1218.0,1219.0,1220.0,1221.0,1222.0,1223.0,1224.0,1225.0,1226.0,1227.0,1228.0,1229.0,1230.0,1231.0,1232.0,1233.0,1234.0,1235.0,1236.0,1237.0,1238.0,1239.0,1240.0,1241.0,1242.0,1243.0,1244.0,1245.0,1246.0,1247.0,1248.0,1249.0,1250.0,1251.0,1252.0,1253.0,1254.0,1255.0,1256.0,1257.0,1258.0,1259.0,1260.0,1261.0,1262.0,1263.0,1264.0,1265.0,1266.0,1267.0,1268.0,1269.0,1270.0,1271.0,1272.0,1273.0,1274.0,1275.0,1276.0,1277.0,1278.0,1279.0,1280.0,1281.0,1282.0,1283.0,1284.0,1285.0,1286.0,1287.0,1288.0,1289.0,1290.0,1291.0,1292.0,1293.0,1294.0,1295.0,1296.0,1297.0,1298.0,1299.0,1300.0,1301.0,1302.0,1303.0,1304.0,1305.0,1306.0,1307.0,1308.0,1309.0,1310.0,1311.0,1312.0,1313.0,1314.0,1315.0,1316.0,1317.0,1318.0,1319.0,1320.0,1321.0,1322.0,1323.0,1324.0,1325.0,1326.0,1327.0,1328.0,1329.0,1330.0,1331.0,1332.0,1333.0,1334.0,1335.0,1336.0,1337.0,1338.0,1339.0,1340.0,1341.0,1342.0,1343.0,1344.0,1345.0,1346.0,1347.0,1348.0,1349.0,1350.0,1351.0,1352.0,1353.0,1354.0,1355.0,1356.0,1357.0,1358.0,1359.0,1360.0,1361.0,1362.0,1363.0,1364.0,1365.0,1366.0,1367.0,1368.0,1369.0,1370.0,1371.0,1372.0,1373.0,1374.0,1375.0,1376.0,1377.0,1378.0,1379.0,1380.0,1381.0,1382.0,1383.0,1384.0,1385.0,1386.0,1387.0,1388.0,1389.0,1390.0,1391.0,1392.0,1393.0,1394.0,1395.0,1396.0,1397.0,1398.0,1399.0,1400.0,1401.0,1402.0,1403.0,1404.0,1405.0,1406.0,1407.0,1408.0,1409.0,1410.0,1411.0,1412.0,1413.0,1414.0,1415.0,1416.0,1417.0,1418.0,1419.0,1420.0,1421.0,1422.0,1423.0,1424.0,1425.0,1426.0,1427.0,1428.0,1429.0,1430.0,1431.0,1432.0,1433.0,1434.0,1435.0,1436.0,1437.0,1438.0,1439.0,1440.0,1441.0,1442.0,1443.0,1444.0,1445.0,1446.0,1447.0,1448.0,1449.0,1450.0,1451.0,1452.0,1453.0,1454.0,1455.0,1456.0,1457.0,1458.0,1459.0,1460.0,1461.0,1462.0,1463.0,1464.0,1465.0,1466.0,1467.0,1468.0,1469.0,1470.0,1471.0,1472.0,1473.0,1474.0,1475.0,1476.0,1477.0,1478.0,1479.0,1480.0,1481.0,1482.0,1483.0,1484.0,1485.0,1486.0,1487.0,1488.0,1489.0,1490.0,1491.0,1492.0,1493.0,1494.0,1495.0,1496.0,1497.0,1498.0,1499.0,1500.0,1501.0,1502.0,1503.0,1504.0,1505.0,1506.0,1507.0,1508.0,1509.0,1510.0,1511.0,1512.0,1513.0,1514.0,1515.0,1516.0,1517.0,1518.0,1519.0,1520.0,1521.0,1522.0,1523.0,1524.0,1525.0,1526.0,1527.0,1528.0,1529.0,1530.0,1531.0,1532.0,1533.0,1534.0,1535.0,1536.0,1537.0,1538.0,1539.0,1540.0,1541.0,1542.0,1543.0,1544.0,1545.0,1546.0,1547.0,1548.0,1549.0,1550.0,1551.0,1552.0,1553.0,1554.0,1555.0,1556.0,1557.0,1558.0,1559.0,1560.0,1561.0,1562.0,1563.0,1564.0,1565.0,1566.0,1567.0,1568.0,1569.0,1570.0,1571.0,1572.0,1573.0,1574.0,1575.0,1576.0,1577.0,1578.0,1579.0,1580.0,1581.0,1582.0,1583.0,1584.0,1585.0,1586.0,1587.0,1588.0,1589.0,1590.0,1591.0,1592.0,1593.0,1594.0,1595.0,1596.0,1597.0,1598.0,1599.0,1600.0,1601.0,1602.0,1603.0,1604.0,1605.0,1606.0,1607.0,1608.0,1609.0,1610.0,1611.0,1612.0,1613.0,1614.0,1615.0,1616.0,1617.0,1618.0,1619.0,1620.0,1621.0,1622.0,1623.0,1624.0,1625.0,1626.0,1627.0,1628.0,1629.0,1630.0,1631.0,1632.0,1633.0,1634.0,1635.0,1636.0,1637.0,1638.0,1639.0,1640.0,1641.0,1642.0,1643.0,1644.0,1645.0,1646.0,1647.0,1648.0,1649.0,1650.0,1651.0,1652.0,1653.0,1654.0,1655.0,1656.0,1657.0,1658.0,1659.0,1660.0,1661.0,1662.0,1663.0,1664.0,1665.0,1666.0,1667.0,1668.0,1669.0,1670.0,1671.0,1672.0,1673.0,1674.0,1675.0,1676.0,1677.0,1678.0,1679.0,1680.0,1681.0,1682.0,1683.0,1684.0,1685.0,1686.0,1687.0,1688.0,1689.0,1690.0,1691.0,1692.0,1693.0,1694.0,1695.0,1696.0,1697.0,1698.0,1699.0,1700.0,1701.0,1702.0,1703.0,1704.0,1705.0,1706.0,1707.0,1708.0,1709.0,1710.0,1711.0,1712.0,1713.0,1714.0,1715.0,1716.0,1717.0,1718.0,1719.0,1720.0,1721.0,1722.0,1723.0,1724.0,1725.0,1726.0,1727.0,1728.0,1729.0,1730.0,1731.0,1732.0,1733.0,1734.0,1735.0,1736.0,1737.0,1738.0,1739.0,1740.0,1741.0,1742.0,1743.0,1744.0,1745.0,1746.0,1747.0,1748.0,1749.0,1750.0,1751.0,1752.0,1753.0,1754.0,1755.0,1756.0,1757.0,1758.0,1759.0,1760.0,1761.0,1762.0,1763.0,1764.0,1765.0,1766.0,1767.0,1768.0,1769.0,1770.0,1771.0,1772.0,1773.0,1774.0,1775.0,1776.0,1777.0,1778.0,1779.0,1780.0,1781.0,1782.0,1783.0,1784.0,1785.0,1786.0,1787.0,1788.0,1789.0,1790.0,1791.0,1792.0,1793.0,1794.0,1795.0,1796.0,1797.0,1798.0,1799.0,1800.0,1801.0,1802.0,1803.0,1804.0,1805.0,1806.0,1807.0,1808.0,1809.0,1810.0,1811.0,1812.0,1813.0,1814.0,1815.0,1816.0,1817.0,1818.0,1819.0,1820.0,1821.0,1822.0,1823.0,1824.0,1825.0,1826.0,1827.0,1828.0,1829.0,1830.0,1831.0,1832.0,1833.0,1834.0,1835.0,1836.0,1837.0,1838.0,1839.0,1840.0,1841.0,1842.0,1843.0,1844.0,1845.0,1846.0,1847.0,1848.0,1849.0,1850.0,1851.0,1852.0,1853.0,1854.0,1855.0,1856.0,1857.0,1858.0,1859.0,1860.0,1861.0,1862.0,1863.0,1864.0,1865.0,1866.0,1867.0,1868.0,1869.0,1870.0,1871.0,1872.0,1873.0,1874.0,1875.0,1876.0,1877.0,1878.0,1879.0,1880.0,1881.0,1882.0,1883.0,1884.0,1885.0,1886.0,1887.0,1888.0,1889.0,1890.0,1891.0,1892.0,1893.0,1894.0,1895.0,1896.0,1897.0,1898.0,1899.0,1900.0,1901.0,1902.0,1903.0,1904.0,1905.0,1906.0,1907.0,1908.0,1909.0,1910.0,1911.0,1912.0,1913.0,1914.0,1915.0,1916.0,1917.0,1918.0,1919.0,1920.0,1921.0,1922.0,1923.0,1924.0,1925.0,1926.0,1927.0,1928.0,1929.0,1930.0,1931.0,1932.0,1933.0,1934.0,1935.0,1936.0,1937.0,1938.0,1939.0,1940.0,1941.0,1942.0,1943.0,1944.0,1945.0,1946.0,1947.0,1948.0,1949.0,1950.0,1951.0,1952.0,1953.0,1954.0,1955.0,1956.0,1957.0,1958.0,1959.0,1960.0,1961.0,1962.0,1963.0,1964.0,1965.0,1966.0,1967.0,1968.0,1969.0,1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0],"y":[0.2300715446472168,0.22974910736083984,0.22962684631347657,0.22991640567779542,0.2293985366821289,0.2290342092514038,0.22930574417114258,0.22903108596801758,0.22836718559265137,0.22857089042663575,0.2287527561187744,0.22787694931030272,0.22724299430847167,0.22818727493286134,0.2262643337249756,0.22722480297088624,0.22741217613220216,0.2266237497329712,0.22569715976715088,0.22725212574005127,0.22453532218933106,0.2264793634414673,0.22601211071014404,0.22473835945129395,0.2250814199447632,0.22585339546203614,0.22619452476501464,0.223974347114563,0.22497334480285644,0.22531638145446778,0.2242985725402832,0.22473726272583008,0.22336525917053224,0.22368457317352294,0.22359302043914794,0.22381997108459473,0.2250349760055542,0.2237466096878052,0.22329435348510743,0.2215416431427002,0.22360692024230958,0.22063772678375243,0.22308011054992677,0.22252559661865234,0.22259035110473632,0.22229409217834473,0.223671293258667,0.2226329565048218,0.22244844436645508,0.22336301803588868,0.22162246704101562,0.22302722930908203,0.22135422229766846,0.221152925491333,0.22121694087982177,0.22085094451904297,0.22198078632354737,0.2201235294342041,0.22245521545410157,0.2182159900665283,0.22043452262878419,0.21910312175750732,0.22217869758605957,0.218798828125,0.21951525211334227,0.21999764442443848,0.21959424018859863,0.21787831783294678,0.21851656436920167,0.22005267143249513,0.21871933937072754,0.2178213119506836,0.21864080429077148,0.2187519073486328,0.2153780460357666,0.21988353729248047,0.21850996017456054,0.22007064819335936,0.21969983577728272,0.2182913064956665,0.2179234504699707,0.220192813873291,0.21690676212310792,0.21429529190063476,0.21628327369689943,0.21885721683502196,0.21640849113464355,0.21589107513427735,0.2151495933532715,0.21761908531188964,0.21447396278381348,0.2185309648513794,0.21657848358154297,0.21358613967895507,0.21588053703308105,0.21549053192138673,0.21780722141265868,0.21592917442321777,0.21509599685668945,0.21278877258300782,0.21682844161987305,0.21352238655090333,0.2198000431060791,0.21614158153533936,0.21409907341003417,0.21297926902770997,0.2182009696960449,0.21213040351867676,0.21496107578277587,0.2122546672821045,0.21939327716827392,0.21634247303009033,0.21526782512664794,0.21202688217163085,0.2152923583984375,0.21680874824523927,0.2123029947280884,0.21198458671569825,0.2176053047180176,0.21620793342590333,0.21530575752258302,0.21287174224853517,0.2128905773162842,0.21268763542175292,0.21167774200439454,0.2138397216796875,0.21289267539978027,0.21223044395446777,0.21494781970977783,0.21198792457580568,0.21520681381225587,0.21161289215087892,0.21488046646118164,0.20858113765716552,0.21647944450378417,0.21278860569000244,0.2108428955078125,0.2099531650543213,0.21174936294555663,0.21097123622894287,0.21328504085540773,0.21365270614624024,0.21291344165802,0.2087337017059326,0.2147317886352539,0.21131467819213867,0.2144176959991455,0.2133336067199707,0.21125340461730957,0.21068935394287108,0.21030652523040771,0.21135330200195312,0.21030635833740235,0.21272354125976561,0.2111952543258667,0.20776019096374512,0.21257514953613282,0.21081185340881348,0.20775692462921141,0.2140218734741211,0.20991430282592774,0.21170442104339598,0.2136309862136841,0.20783686637878418,0.21129837036132812,0.2132906436920166,0.20918216705322265,0.21068854331970216,0.20849852561950682,0.20882532596588135,0.20822455883026122,0.212471866607666,0.21144695281982423,0.2069375991821289,0.20931413173675537,0.21129682064056396,0.20989348888397216,0.20972211360931398,0.21032025814056396,0.2043825387954712,0.2097320795059204,0.21003503799438478,0.21104164123535157,0.20688986778259277,0.21354939937591552,0.20669982433319092,0.2109067440032959,0.2105936050415039,0.21156203746795654,0.20721466541290284,0.21037678718566893,0.20879528522491456,0.20660109519958497,0.20679407119750975,0.2075120449066162,0.2043677568435669,0.20690679550170898,0.2096999168395996,0.20800716876983644,0.21172606945037842,0.2058267593383789,0.20644221305847169,0.20650925636291503,0.20715851783752443,0.20757503509521485,0.20653033256530762,0.21055662631988525,0.20784327983856202,0.2099748134613037,0.20578718185424805,0.20918588638305663,0.21090283393859863,0.20779075622558593,0.2079761505126953,0.20749180316925048,0.2064741373062134,0.20890257358551026,0.2105933904647827,0.20777010917663574,0.21145944595336913,0.20624876022338867,0.20617716312408446,0.20539407730102538,0.2079171657562256,0.20641605854034423,0.21059095859527588,0.20718934535980224,0.21131932735443115,0.20766282081604004,0.20574264526367186,0.21042296886444092,0.20762720108032226,0.2086054801940918,0.2083409070968628,0.20827114582061768,0.20942239761352538,0.20643553733825684,0.20917716026306152,0.20637135505676268,0.20778450965881348,0.207381534576416,0.20493860244750978,0.20475413799285888,0.2081087350845337,0.20842378139495848,0.20183048248291016,0.2018674850463867,0.21123437881469725,0.20779633522033691,0.20548603534698487,0.20756514072418214,0.2073678493499756,0.20315804481506347,0.20720152854919432,0.2090679883956909,0.203064227104187,0.2023077964782715,0.2052597999572754,0.20668327808380127,0.20396647453308106,0.207741641998291,0.20529279708862305,0.20603570938110352,0.20593559741973877,0.2054598808288574,0.20767683982849122,0.2077383041381836,0.21036686897277831,0.2055518627166748,0.19999207258224488,0.2047590970993042,0.20440409183502198,0.20252602100372313,0.20515875816345214,0.2091146945953369,0.20495131015777587,0.20496048927307128,0.20174217224121094,0.1983409643173218,0.20111162662506105,0.2007617473602295,0.20622758865356444,0.2042081356048584,0.2103114604949951,0.20310633182525634,0.20025577545166015,0.2035912036895752,0.20547609329223632,0.20900530815124513,0.20351896286010743,0.2054394006729126,0.2000713348388672,0.20088646411895753,0.20567498207092286,0.2020465612411499,0.20632390975952147,0.2073850154876709,0.2059776782989502,0.2077712297439575,0.20224528312683104,0.20578665733337403,0.2005776882171631,0.20566074848175048,0.20487699508666993,0.20678577423095704,0.2060706377029419,0.20680580139160157,0.20796604156494142,0.20697598457336425,0.20302987098693848,0.20274677276611328,0.19957486391067505,0.20795822143554688,0.20141592025756835,0.20249249935150146,0.20118849277496337,0.20132913589477539,0.20637156963348388,0.19827125072479249,0.20328865051269532,0.20177826881408692,0.2000171184539795,0.20068607330322266,0.2034374713897705,0.19995131492614746,0.20325057506561278,0.20393164157867433,0.19967796802520751,0.2042644739151001,0.20940790176391602,0.20201926231384276,0.20070643424987794,0.20700762271881104,0.2106424331665039,0.20317952632904052,0.20283064842224122,0.20295567512512208,0.20216050148010253,0.1991642713546753,0.20426712036132813,0.20179336071014403,0.2042987823486328,0.20318331718444824,0.19918603897094728,0.20493438243865966,0.2038012981414795,0.20241622924804686,0.20604300498962402,0.20163040161132811,0.2082606554031372,0.20322782993316652,0.19819989204406738,0.19969496726989747,0.20479993820190429,0.19828615188598633,0.20081894397735595,0.2014101505279541,0.19951652288436889,0.20270051956176757,0.20255966186523439,0.19914498329162597,0.20201196670532226,0.2037278175354004,0.20022106170654297,0.20186161994934082,0.19728643894195558,0.19712038040161134,0.199621319770813,0.20051305294036864,0.2003793716430664,0.2085104465484619,0.2010108470916748,0.20031917095184326,0.19789695739746094,0.196482253074646,0.19945664405822755,0.20296745300292968,0.20228946208953857,0.2016446828842163,0.20305333137512208,0.2012399196624756,0.2029980182647705,0.20221354961395263,0.19927079677581788,0.19822070598602295,0.20005016326904296,0.20600695610046388,0.2036379337310791,0.19992762804031372,0.20137314796447753,0.19676663875579833,0.2035996437072754,0.19838457107543944,0.19751213788986205,0.20056095123291015,0.2066265106201172,0.20495877265930176,0.201090669631958,0.20436592102050782,0.20327887535095215,0.19794365167617797,0.2028512716293335,0.19977765083312987,0.20088725090026854,0.20515837669372558,0.19963645935058594,0.20029959678649903,0.1974647045135498,0.19929399490356445,0.20258057117462158,0.1980404496192932,0.20201447010040283,0.20557503700256347,0.1978912830352783,0.2010343313217163,0.2006401538848877,0.19654276371002197,0.1974376320838928,0.19990096092224122,0.20051343441009523,0.19762766361236572,0.19801955223083495,0.19754812717437745,0.20071799755096437,0.19695568084716797,0.20030856132507324,0.1980691909790039,0.20099778175354005,0.19820754528045653,0.19671850204467772,0.1939978241920471,0.19990040063858033,0.201324462890625,0.19835915565490722,0.2007061004638672,0.19664623737335205,0.20304570198059083,0.1956164598464966,0.20092270374298096,0.1980313777923584,0.19788445234298707,0.2007509231567383,0.2024974822998047,0.19580096006393433,0.2039482593536377,0.19396004676818848,0.19907636642456056,0.20746026039123536,0.19617573022842408,0.19735852479934693,0.1987444519996643,0.2017911911010742,0.20076360702514648,0.20164735317230226,0.206697416305542,0.19958057403564453,0.19517821073532104,0.19789319038391112,0.20485033988952636,0.19837844371795654,0.2066699504852295,0.2020942211151123,0.19771642684936525,0.1956185817718506,0.1964979887008667,0.19400473833084106,0.1914237141609192,0.19988001585006715,0.2030113458633423,0.20276122093200682,0.19895565509796143,0.19632117748260497,0.19734245538711548,0.20470285415649414,0.1978179097175598,0.20103821754455567,0.19632771015167236,0.19765535593032837,0.2012186050415039,0.19585078954696655,0.19364721775054933,0.19825904369354247,0.1990360736846924,0.19848482608795165,0.20110321044921875,0.19728924036026002,0.1997831106185913,0.19955991506576537,0.19685810804367065,0.19534575939178467,0.19496642351150512,0.1935938596725464,0.20096993446350098,0.19911539554595947,0.1973336458206177,0.20100102424621583,0.19560914039611815,0.19489824771881104,0.19880380630493164,0.19585399627685546,0.19771867990493774,0.19281296730041503,0.1944914698600769,0.20048186779022217,0.19665337800979615,0.19587682485580443,0.1961282730102539,0.19752519130706786,0.2037343978881836,0.20153000354766845,0.19574649333953859,0.20079994201660156,0.20215086936950682,0.19815162420272828,0.20195789337158204,0.1945479154586792,0.19949334859848022,0.194929039478302,0.20466241836547852,0.1994682788848877,0.2007430076599121,0.20259582996368408,0.20086638927459716,0.199030601978302,0.1974153995513916,0.1936793565750122,0.19764890670776367,0.20022802352905272,0.19243829250335692,0.19634606838226318,0.19727034568786622,0.1916082262992859,0.1972744345664978,0.1961879014968872,0.1969621777534485,0.1923682928085327,0.19711663722991943,0.1952430486679077,0.19310357570648193,0.19397029876708985,0.19728842973709107,0.20754244327545165,0.19870022535324097,0.20247290134429932,0.19935420751571656,0.19988722801208497,0.1986431360244751,0.19986212253570557,0.19484119415283202,0.1983615279197693,0.19610047340393066,0.1916250228881836,0.19577149152755738,0.19404276609420776,0.19519623517990112,0.19907323122024537,0.1972040891647339,0.19705214500427246,0.1930802583694458,0.19581956863403321,0.20158216953277588,0.1965059757232666,0.19983725547790526,0.19697856903076172,0.20110783576965333,0.19798572063446046,0.18974945545196534,0.1985707998275757,0.19426414966583253,0.19882779121398925,0.19572880268096923,0.19196443557739257,0.19688767194747925,0.19858224391937257,0.1978306293487549,0.19447230100631713,0.19845478534698485,0.1962105393409729,0.1969773530960083,0.1970372200012207,0.19581842422485352,0.19895546436309813,0.1963498115539551,0.1997887372970581,0.1967022657394409,0.19489961862564087,0.19409056901931762,0.19279255867004394,0.1943673610687256,0.19989670515060426,0.20063431262969972,0.19981184005737304,0.19645462036132813,0.1914312481880188,0.19116822481155396,0.19327412843704223,0.19399189949035645,0.19984033107757568,0.1976354718208313,0.2021190643310547,0.19739949703216553,0.18889069557189941,0.19436998367309571,0.18815965652465821,0.19122483730316162,0.20218498706817628,0.19593807458877563,0.19083700180053711,0.19440817832946777,0.19105193614959717,0.1930363416671753,0.19345898628234864,0.19912033081054686,0.1930980682373047,0.19784078598022461,0.19376157522201537,0.20285332202911377,0.19540170431137086,0.19685835838317872,0.19687358140945435,0.18929920196533204,0.19379692077636718,0.18905867338180543,0.19509296417236327,0.19957646131515502,0.20124130249023436,0.200028133392334,0.19526319503784179,0.19605196714401246,0.19300737380981445,0.19396238327026366,0.1968587875366211,0.19336433410644532,0.19540274143218994,0.1989771842956543,0.19241042137145997,0.19090999364852906,0.1960237741470337,0.1948567271232605,0.19727754592895508,0.1897389531135559,0.19560956954956055,0.1961157202720642,0.18995800018310546,0.1923382043838501,0.19762330055236815,0.18856077194213866,0.1968323826789856,0.19996485710144044,0.1915809392929077,0.19719878435134888,0.19510014057159425,0.19473379850387573,0.1914563298225403,0.19576728343963623,0.19242632389068604,0.1892325758934021,0.19783177375793456,0.20309715270996093,0.20045454502105714,0.19173049926757812,0.19630157947540283,0.19615933895111085,0.19551749229431153,0.1905044436454773,0.1960163712501526,0.19240214824676513,0.19631333351135255,0.19448016881942748,0.1996880888938904,0.1959836006164551,0.19632242918014525,0.19477790594100952,0.19229716062545776,0.1958696126937866,0.19634108543395995,0.19402928352355958,0.1951427936553955,0.19861972332000732,0.1949515700340271,0.19473891258239745,0.1909637451171875,0.19742302894592284,0.20079660415649414,0.19172580242156984,0.19936269521713257,0.20148420333862305,0.19800209999084473,0.1938995122909546,0.19400124549865722,0.1941571831703186,0.19504722356796264,0.19441742897033693,0.19167020320892333,0.1935657501220703,0.19837827682495118,0.19607930183410643,0.19376821517944337,0.1922650694847107,0.1947110652923584,0.19048227071762086,0.193693208694458,0.18942773342132568,0.19788578748703003,0.19857864379882811,0.1980919599533081,0.19478565454483032,0.19398659467697144,0.19617443084716796,0.1928034782409668,0.18337844610214232,0.202886962890625,0.19086791276931764,0.19224473237991332,0.19494287967681884,0.19499013423919678,0.19098989963531493,0.19674015045166016,0.19607640504837037,0.19282763004302977,0.1973675847053528,0.19217088222503662,0.19931178092956542,0.186879563331604,0.20194652080535888,0.18761072158813477,0.1945359468460083,0.19452743530273436,0.1926392436027527,0.19105385541915892,0.19711816310882568,0.19593405723571777,0.19763792753219606,0.19903759956359862,0.1981420636177063,0.18902047872543334,0.19695589542388917,0.19488991498947145,0.19466925859451295,0.1889951705932617,0.18894728422164916,0.19635080099105834,0.19117882251739501,0.20331864356994628,0.19120612144470214,0.19655725955963135,0.19022884368896484,0.18910932540893555,0.19610666036605834,0.2001175880432129,0.19475786685943602,0.19456708431243896,0.18837069272994994,0.19583917856216432,0.19475328922271729,0.19836385250091554,0.19356672763824462,0.19158053398132324,0.18992276191711427,0.19715421199798583,0.19512557983398438,0.19353235960006715,0.193152117729187,0.1974405288696289,0.19434913396835327,0.1937305212020874,0.19563595056533814,0.19710586071014405,0.1994672656059265,0.19851081371307372,0.1940707802772522,0.1953045129776001,0.19074971675872804,0.19032458066940308,0.1886146068572998,0.19528114795684814,0.1950908660888672,0.19408044815063477,0.19075820446014405,0.19112194776535035,0.19314876794815064,0.19544553756713867,0.19814703464508057,0.1951266646385193,0.19527788162231446,0.18986352682113647,0.19352309703826903,0.19710071086883546,0.19435977935791016,0.18604037761688233,0.198423171043396,0.18826236724853515,0.1897595167160034,0.18761153221130372,0.19612796306610109,0.18638348579406738,0.19588409662246703,0.1933137893676758,0.19411466121673585,0.1911036729812622,0.19453327655792235,0.19397823810577391,0.18840762376785278,0.1965220093727112,0.1856871008872986,0.18490395545959473,0.1929114818572998,0.19100066423416137,0.19346117973327637,0.19090923070907592,0.19708733558654784,0.19836962223052979,0.19771169424057006,0.19658081531524657,0.19105305671691894,0.1866983652114868,0.1911684274673462,0.19650094509124755,0.19368468523025512,0.193528151512146,0.1970961570739746,0.19821065664291382,0.19633642435073853,0.1881986141204834,0.187498939037323,0.19622888565063476,0.18874194622039794,0.19216470718383788,0.19849483966827391,0.19600315093994142,0.19440774917602538,0.1940719962120056,0.1916489601135254,0.19027529954910277,0.18952362537384032,0.20005531311035157,0.19182331562042237,0.19978756904602052,0.193223237991333,0.19489378929138185,0.1934342861175537,0.18897807598114014,0.19319969415664673,0.19673290252685546,0.18931431770324708,0.18951187133789063,0.19244948625564576,0.18986259698867797,0.1889573335647583,0.19875626564025878,0.18383642435073852,0.18905724287033082,0.18768762350082396,0.187833833694458,0.19697773456573486,0.18474192619323732,0.1905425786972046,0.19162708520889282,0.19364341497421264,0.191499924659729,0.19926350116729735,0.19933726787567138,0.19144591093063354,0.19238311052322388,0.19624168872833253,0.18669782876968383,0.18806049823760987,0.18732563257217408,0.19177713394165039,0.19140963554382323,0.1913989782333374,0.19200756549835205,0.19670259952545166,0.18978490829467773,0.18837546110153197,0.186192524433136,0.1921160936355591,0.19191781282424927,0.1864067554473877,0.19048237800598145,0.19392653703689575,0.19725611209869384,0.19759669303894042,0.18590747117996215,0.19314701557159425,0.18848536014556885,0.19496326446533202,0.1899126648902893,0.18632819652557372,0.19077107906341553,0.19462279081344605,0.18898658752441405,0.18760762214660645,0.19140310287475587,0.19265322685241698,0.19908065795898439,0.19318301677703859,0.193461012840271,0.1948906421661377,0.18996000289916992,0.19182176589965821,0.18807640075683593,0.19051551818847656,0.19534611701965332,0.18924829959869385,0.19175410270690918,0.1956761121749878,0.19482940435409546,0.1872843861579895,0.18992286920547485,0.19121687412261962,0.19744123220443727,0.19560095071792602,0.19189963340759278,0.19701845645904542,0.19954190254211426,0.19151010513305664,0.19190341234207153,0.19512442350387574,0.19435570240020753,0.1876556396484375,0.18977725505828857,0.1870429754257202,0.1891481399536133,0.18732688426971436,0.1928340196609497,0.19750072956085205,0.19433486461639404,0.1889542818069458,0.19424681663513182,0.192240571975708,0.18881011009216309,0.18885500431060792,0.1893290877342224,0.19321508407592775,0.2107818603515625,0.19280648231506348,0.1961057662963867,0.19027965068817138,0.19014278650283814,0.18803725242614747,0.19252510070800782,0.19359588623046875,0.19042272567749025,0.19142669439315796,0.19403963088989257,0.19092442989349365,0.19923421144485473,0.18699731826782226,0.18748444318771362,0.19751135110855103,0.18527294397354127,0.19247310161590575,0.18540788888931276,0.19033952951431274,0.1901885986328125,0.1905538558959961,0.18743114471435546,0.19724100828170776,0.18912365436553955,0.1895226001739502,0.19617533683776855,0.192130708694458,0.18511710166931153,0.19055182933807374,0.19376327991485595,0.19012328386306762,0.19049980640411376,0.186474609375,0.19011378288269043,0.19188301563262938,0.20305285453796387,0.19047946929931642,0.1925666093826294,0.1869558334350586,0.18972713947296144,0.19093735218048097,0.18821710348129272,0.18746129274368287,0.18654762506484984,0.1960749387741089,0.1848301410675049,0.19477931261062623,0.19400105476379395,0.19374678134918213,0.1886260986328125,0.1918300747871399,0.18497169017791748,0.1905156135559082,0.19549590349197388,0.19145489931106568,0.196943998336792,0.19130046367645265,0.1861910939216614,0.1929474353790283,0.18237360715866088,0.19864245653152465,0.18493611812591554,0.19344699382781982,0.19782919883728028,0.19189584255218506,0.19187555313110352,0.1917797803878784,0.18910061120986937,0.19683276414871215,0.19216612577438355,0.18759459257125854,0.19815220832824706,0.1908401370048523,0.20274879932403564,0.1866111636161804,0.19284477233886718,0.18629196882247925,0.1959204912185669,0.181020188331604,0.1868902325630188,0.19047535657882692,0.19688307046890258,0.19029006958007813,0.192139732837677,0.18864946365356444,0.1892363667488098,0.19470558166503907,0.19562101364135742,0.1996188759803772,0.1951403260231018,0.18855006694793702,0.19408864974975587,0.187045156955719,0.19572644233703612,0.1902460813522339,0.19627783298492432,0.19290190935134888,0.1910369873046875,0.19384628534317017,0.19455018043518066,0.19355591535568237,0.1932801127433777,0.1909063220024109,0.188620924949646,0.1890727162361145,0.19039597511291503,0.1842146635055542,0.19756097793579103,0.18753571510314943,0.1926479697227478,0.19094185829162597,0.18859831094741822,0.19975180625915528,0.19321409463882447,0.19119361639022828,0.19501372575759887,0.19229602813720703,0.19010252952575685,0.1860712766647339,0.19243732690811158,0.18876903057098388,0.18736393451690675,0.1831509590148926,0.19114294052124023,0.18773359060287476,0.19093308448791504,0.19568963050842286,0.19086132049560547,0.1890718698501587,0.19525326490402223,0.19217513799667357,0.18935770988464357,0.18641210794448854,0.19422827959060668,0.1948331117630005,0.19653401374816895,0.18661398887634278,0.19440919160842896,0.19056522846221924,0.18693937063217164,0.18703479766845704,0.1913419246673584,0.19331530332565308,0.18961867094039916,0.1828679323196411,0.1888163924217224,0.19239132404327391,0.19319878816604613,0.19114986658096314,0.18783221244812012,0.18826133012771606,0.19326882362365722,0.1881479501724243,0.19579366445541382,0.19078164100646972,0.19250669479370117,0.19396203756332397,0.193625009059906,0.18606412410736084,0.1911618232727051,0.19117747545242308,0.19137537479400635,0.1896953582763672,0.1931161403656006,0.19148683547973633,0.19237239360809327,0.17859210968017578,0.1883041501045227,0.18385441303253175,0.19479087591171265,0.1966903805732727,0.19189506769180298,0.1926991581916809,0.19235763549804688,0.19160767793655395,0.19391335248947145,0.18623260259628296,0.18765461444854736,0.1939820647239685,0.18816120624542237,0.18999173641204833,0.1898794412612915,0.19013116359710694,0.19211676120758056,0.1908287525177002,0.1908551573753357,0.1904591679573059,0.19104433059692383,0.19164574146270752,0.1930872917175293,0.1909133553504944,0.1900578260421753,0.19044978618621827,0.19162834882736207,0.1957160711288452,0.1843319058418274,0.19367679357528686,0.18575314283370972,0.19041664600372316,0.19961432218551636,0.1959398627281189,0.1851353406906128,0.1911402463912964,0.18452270030975343,0.1861637592315674,0.18903563022613526,0.18925042152404786,0.19485756158828735,0.18700888156890869,0.19674631357192993,0.18927310705184935,0.19178653955459596,0.1828504681587219,0.1935439109802246,0.18914334774017333,0.18698145151138307,0.18642197847366332,0.1882317304611206,0.19204680919647216,0.19192947149276735,0.1917118549346924,0.18480262756347657,0.18656508922576903,0.19878199100494384,0.19523322582244873,0.19020457267761232,0.19974814653396605,0.18651285171508789,0.19077163934707642,0.1931499123573303,0.1900359034538269,0.19416685104370118,0.19373297691345215,0.19589030742645264,0.1838115334510803,0.19455380439758302,0.1923760175704956,0.18945786952972413,0.18798599243164063,0.19293121099472046,0.18537395000457763,0.18697447776794435,0.19149178266525269,0.18987762928009033,0.1936277389526367,0.19215332269668578,0.18248108625411988,0.18918302059173583,0.19199447631835936,0.18518922328948975,0.18333567380905152,0.17881572246551514,0.18950290679931642,0.18897457122802735,0.1939988613128662,0.18903239965438842,0.18572421073913575,0.1918264150619507,0.1826311707496643,0.1939762830734253,0.19526456594467162,0.18276678323745726,0.18419125080108642,0.19214508533477784,0.19542778730392457,0.19485357999801636,0.18111566305160523,0.18695077896118165,0.19248138666152953,0.1980891704559326,0.18576598167419434,0.19203567504882812,0.19195523262023925,0.18812066316604614,0.18877074718475342,0.18832483291625976,0.18967318534851074,0.1948625087738037,0.1877284049987793,0.1938166379928589,0.19119490385055543,0.18261337280273438,0.1901031732559204,0.18265073299407958,0.1970759153366089,0.19177987575531005,0.18803880214691163,0.19069727659225463,0.18405430316925048,0.19621629714965821,0.1828635573387146,0.18654017448425292,0.19124480485916137,0.18921422958374023,0.19030311107635497,0.19167202711105347,0.19466670751571655,0.18587710857391357,0.18839671611785888,0.18937708139419557,0.185044002532959,0.18699464797973633,0.18981869220733644,0.19403040409088135,0.1971163868904114,0.19168561697006226,0.19071354866027831,0.18446611166000365,0.1831258773803711,0.18740744590759278,0.17899515628814697,0.1898226857185364,0.19470996856689454,0.180134117603302,0.18885130882263185,0.19262430667877198,0.19153523445129395,0.18949649333953858,0.19004523754119873,0.1850944995880127,0.18744610548019408,0.18955148458480836,0.1904157042503357,0.19107102155685424,0.18624731302261352,0.18800946474075317,0.1916213035583496,0.18505518436431884,0.1935424327850342,0.19561995267868043,0.18542635440826416,0.19675912857055664,0.18458476066589355,0.1931331515312195,0.190176522731781,0.1824120283126831,0.18698477745056152,0.18948405981063843,0.18350434303283691,0.18623629808425904,0.19486366510391234,0.19524223804473878,0.18569658994674682,0.19105665683746337,0.1821288824081421,0.18775471448898315,0.18827733993530274,0.1919501543045044,0.19091837406158446,0.19594779014587402,0.1954747200012207,0.19088995456695557,0.19079203605651857,0.1972133755683899,0.18342015743255616,0.18788435459136962,0.18236083984375,0.19614176750183104,0.1878216028213501,0.18294310569763184,0.19160009622573854,0.19430327415466309,0.19354751110076904,0.18830587863922119,0.19086141586303712,0.19498214721679688,0.20057437419891358,0.19818755388259887,0.1941293239593506,0.1837711215019226,0.19704656600952147,0.1877846598625183,0.18326094150543212,0.18684579133987428,0.19228265285491944,0.18751517534255982,0.1876363754272461,0.19157177209854126,0.18975266218185424,0.184650719165802,0.18419355154037476,0.18525584936141967,0.1919865369796753,0.1892560601234436,0.1864023208618164,0.1872548818588257,0.18366410732269287,0.18651440143585205,0.18594456911087037,0.19280529022216797,0.18982548713684083,0.18850840330123902,0.1887340784072876,0.17840851545333863,0.19110989570617676,0.18968591690063477,0.1852973461151123,0.18493447303771973,0.1855319619178772,0.18622710704803466,0.19248450994491578,0.18913646936416625,0.18709194660186768,0.19750072956085205,0.18249634504318238,0.19033621549606322,0.19383058547973633,0.1865147113800049,0.18900082111358643,0.18890037536621093,0.18550348281860352,0.19515998363494874,0.19241305589675903,0.1846078634262085,0.19308624267578126,0.1916326880455017,0.1846708297729492,0.187951397895813,0.1874943971633911,0.18705353736877442,0.19167468547821045,0.18922312259674073,0.18749254941940308,0.18555049896240233,0.19185945987701417,0.18853704929351806,0.19178813695907593,0.1869104266166687,0.18791069984436035,0.18966012001037597,0.1794463038444519,0.19053503274917602,0.18596856594085692,0.18921887874603271,0.18702093362808228,0.18743107318878174,0.1887684464454651,0.1840407967567444,0.19179248809814453,0.18809797763824462,0.19136512279510498,0.19075063467025757,0.1882038116455078,0.18848978281021117,0.18958243131637573,0.1923130750656128,0.18793716430664062,0.18691513538360596,0.18674514293670655,0.1866842031478882,0.19454816579818726,0.1866887927055359,0.1874564528465271,0.18626418113708496,0.19684451818466187,0.19212768077850342,0.18794654607772826,0.18078383207321166,0.18924190998077392,0.18216460943222046,0.1926555037498474,0.19377127885818482,0.19398505687713624,0.18395522832870484,0.19747387170791625,0.190595805644989,0.18940894603729247,0.18825981616973878,0.18949713706970214,0.18657801151275635,0.1917734384536743,0.17855145931243896,0.18728262186050415,0.18657946586608887,0.1936877489089966,0.18770145177841185,0.189650297164917,0.18165009021759032,0.1861378073692322,0.18698763847351074,0.1851770043373108,0.1855539083480835,0.19001121520996095,0.19596067667007447,0.1857028603553772,0.18219555616378785,0.18866403102874757,0.1878186583518982,0.18937308788299562,0.18639073371887208,0.18527460098266602,0.1885823130607605,0.1879873275756836,0.18835823535919188,0.18587920665740967,0.1922709822654724,0.18830320835113526,0.18288326263427734,0.19404138326644899,0.18255763053894042,0.19572780132293702,0.18650386333465577,0.19068846702575684,0.19349191188812256,0.18159788846969604,0.18314404487609864,0.18626248836517334,0.18133130073547363,0.17993682622909546,0.18323943614959717,0.18680475950241088,0.190335214138031,0.1913634181022644,0.1919248580932617,0.18576184511184693,0.19154874086380005,0.1835999846458435,0.1916811466217041,0.18928685188293456,0.19238533973693847,0.1903978705406189,0.19321694374084472,0.1874847412109375,0.187072229385376,0.18613553047180176,0.1842414140701294,0.19198496341705323,0.19682488441467286,0.18464717864990235,0.19293012619018554,0.18965933322906495,0.19283418655395507,0.18890870809555055,0.1840693712234497,0.1910482883453369,0.18526642322540282,0.18518385887145997,0.1888486385345459,0.1999263048171997,0.18305832147598267,0.18904112577438353,0.1907423734664917,0.1850066900253296,0.19221291542053223,0.18492624759674073,0.18773694038391114,0.19159127473831178,0.1870542049407959,0.18640336990356446,0.19059103727340698,0.18732364177703859,0.17785193920135497,0.1852344512939453,0.18207099437713622,0.18672139644622804,0.18805150985717772,0.18155333995819092,0.1824584722518921,0.18751788139343262,0.1899914860725403,0.1901382565498352,0.1893749475479126,0.18112140893936157,0.19117798805236816,0.18946938514709472,0.19186872243881226,0.18709027767181396,0.1949455976486206,0.1906861186027527,0.197057044506073,0.19561362266540527,0.1935080647468567,0.19208451509475707,0.19327933788299562,0.18832193613052367,0.19273757934570312,0.19097254276275635,0.18781137466430664,0.19232850074768065,0.1833409547805786,0.19366855621337892,0.18545540571212768,0.191670823097229,0.19828057289123535,0.18546364307403565,0.1852238655090332,0.18755239248275757,0.1940720796585083,0.18262848854064942,0.19269222021102905,0.18292468786239624,0.18576903343200685,0.18807305097579957,0.1863077163696289,0.18667979240417482,0.18838609457015992,0.1823594093322754,0.18081928491592408,0.18335654735565185,0.18560636043548584,0.1844485282897949,0.1731666922569275,0.18252856731414796,0.18575562238693238,0.1955513834953308,0.18618905544281006,0.18713150024414063,0.18845465183258056,0.19366384744644166,0.18577196598052978,0.18809067010879515,0.18191059827804565,0.18840013742446898,0.18704593181610107,0.19199752807617188,0.18607856035232545,0.1847764492034912,0.1841585397720337,0.18433471918106079,0.18633770942687988,0.18526415824890136,0.19126797914505006,0.19372618198394775,0.1870396375656128,0.1868033766746521,0.18757680654525757,0.18351309299468993,0.1890251398086548,0.1894575834274292,0.18455076217651367,0.18210954666137696,0.18859806060791015,0.18369213342666627,0.17937248945236206,0.19099518060684204,0.18770413398742675,0.19157860279083253,0.19109508991241456,0.1790350317955017,0.1852356195449829,0.19036871194839478,0.18517670631408692,0.19615558385849,0.1879021406173706,0.18777554035186766,0.1869547963142395,0.1900683879852295,0.1877583622932434,0.18720335960388185,0.1882184386253357,0.195500648021698,0.1863226056098938,0.18685390949249267,0.1791716694831848,0.194863760471344,0.19827618598937988,0.19404199123382568,0.18741737604141234,0.19433799982070923,0.19289138317108154,0.19229592084884645,0.18573176860809326,0.18462884426116943,0.1956777334213257,0.1819985628128052,0.18041772842407228,0.1824256658554077,0.18933141231536865,0.18487107753753662,0.1959443211555481,0.19237878322601318,0.1848085880279541,0.18433624505996704,0.194826340675354,0.19525456428527832,0.19764877557754518,0.18944356441497803,0.19039957523345946,0.18244196176528932,0.1843029260635376,0.18572120666503905,0.18382996320724487,0.1871003031730652,0.1857091188430786,0.18674415349960327,0.18725844621658325,0.18698996305465698,0.1874879002571106,0.17821874618530273,0.18811894655227662,0.19667016267776488,0.19440509080886842,0.1917941927909851,0.18912005424499512,0.19463913440704345,0.19363219738006593,0.1937311053276062,0.19142836332321167,0.18022838830947877,0.19158625602722168,0.18311280012130737,0.1797553777694702,0.18683799505233764,0.1767444133758545,0.18535771369934081,0.19126315116882325,0.1822238087654114,0.18603569269180298,0.18558521270751954,0.18497463464736938,0.18446459770202636,0.18897961378097533,0.18662126064300538,0.1880239486694336,0.19160280227661133,0.1912304162979126,0.18597607612609862,0.1848289966583252,0.18358774185180665,0.1869118928909302,0.18146467208862305,0.18189654350280762,0.1762758255004883,0.1874366283416748,0.19157222509384156,0.19001373052597045,0.1970095753669739,0.18665807247161864,0.18666276931762696,0.19440393447875975,0.1880800485610962,0.18328561782836914,0.1932775378227234,0.1852056384086609,0.19322125911712645,0.1910884737968445,0.18553422689437865,0.19224107265472412,0.17804735898971558,0.18657989501953126,0.19145612716674804,0.18585110902786256,0.1811484932899475,0.18606081008911132,0.1796025037765503,0.1925894021987915,0.18440918922424315,0.19001421928405762,0.18885265588760375,0.1945134401321411,0.18638935089111328,0.19155184030532837,0.18748691082000732,0.18893046379089357,0.18845876455307006,0.18115544319152832,0.18940587043762208,0.18794379234313965,0.1888332724571228,0.18045215606689452,0.1894906997680664,0.18367528915405273,0.1930704355239868,0.1887161135673523,0.1866304397583008,0.1855273127555847,0.17998871803283692,0.1833803415298462,0.18261573314666749,0.18433840274810792,0.1845582127571106,0.19312487840652465,0.19282073974609376,0.18329181671142578,0.1834214448928833,0.18004406690597535,0.19208364486694335,0.19019485712051393,0.1861208438873291,0.1925356864929199,0.1899272918701172,0.1881101369857788,0.17922699451446533,0.18263672590255736,0.18306396007537842,0.18082453012466432,0.18724178075790404,0.18004913330078126,0.18234605789184571,0.18771448135375976,0.19542531967163085,0.18163059949874877,0.18681211471557618,0.17928287982940674,0.182546865940094,0.19234172105789185,0.18724353313446046,0.19144721031188966,0.19218666553497316,0.18249940872192383,0.19109835624694824,0.1847182035446167,0.17932029962539672,0.19028868675231933,0.18500056266784667,0.18047564029693602,0.18586366176605223,0.18851151466369628,0.18225641250610353,0.17903378009796142,0.18553198575973512,0.18127647638320923,0.18396848440170288,0.1911829710006714,0.19384909868240358,0.1826666235923767,0.1805223345756531,0.18851699829101562,0.17874343395233155,0.18752520084381102,0.1830212116241455,0.18481113910675048,0.18269094228744506,0.19096779823303223,0.1891412615776062,0.17779183387756348,0.1892082691192627,0.1859383463859558,0.17897932529449462,0.18814122676849365,0.1833552360534668,0.18129609823226928,0.19074633121490478,0.18742889165878296,0.1856417179107666,0.18704792261123657,0.19439780712127686,0.18495326042175292,0.19864953756332399,0.1961613655090332,0.18991986513137818,0.18263144493103028,0.18796219825744628,0.18084228038787842,0.18200209140777587,0.1921204924583435,0.19748069047927858,0.17904791831970215,0.18547961711883545,0.19548381567001344,0.18392823934555053,0.1871715784072876,0.1815093755722046,0.18907009363174437,0.1865551471710205,0.1847382068634033,0.18665852546691894,0.18160171508789064,0.19141614437103271,0.18232563734054566,0.17580156326293944,0.1903796076774597,0.1894822597503662,0.1878448486328125,0.19393755197525026,0.1888329029083252,0.18468976020812988,0.17809094190597535,0.18426792621612548,0.18870854377746582,0.19133970737457276,0.18861281871795654,0.19349576234817506,0.18480066061019898,0.1896551012992859,0.19637022018432618,0.1877496838569641,0.1930107593536377,0.18786282539367677,0.18517012596130372,0.1847091794013977,0.18409086465835572,0.18481099605560303,0.18621302843093873,0.18408478498458863,0.1891712188720703,0.19515492916107177,0.1908641576766968,0.19204949140548705,0.18646284341812133,0.19295569658279418,0.18477988243103027,0.18817706108093263,0.18348941802978516,0.18645341396331788,0.19321168661117555,0.18706369400024414,0.18677160739898682,0.1873821496963501,0.18261549472808838,0.1903247833251953,0.19867157936096191,0.18785345554351807,0.1851351261138916,0.18154579401016235,0.18096141815185546,0.18583229780197144,0.18612470626831054,0.19075727462768555,0.18553295135498046,0.1915760040283203,0.18186688423156738,0.1881159543991089,0.19291243553161622,0.19378528594970704,0.19334195852279662,0.1872033953666687,0.19268633127212526,0.18204922676086427,0.18460384607315064,0.18764839172363282,0.19191331863403321,0.18357315063476562,0.1852329730987549,0.18492900133132933,0.1876429796218872,0.18022977113723754,0.19055140018463135,0.18129210472106932,0.18251068592071534,0.18558740615844727,0.18409205675125123,0.18726322650909424,0.1843062996864319,0.1847271203994751,0.18648693561553956,0.18233754634857177,0.18865106105804444,0.18157414197921753,0.18618645668029785,0.18289147615432738,0.18839746713638306,0.18401753902435303,0.18480386734008789,0.18954917192459106,0.18227131366729737,0.1860125780105591,0.19440174102783203,0.1853433609008789,0.19809975624084472,0.18742952346801758,0.19091858863830566,0.1868281841278076,0.1828702926635742,0.18334743976593018,0.18310167789459228,0.18761229515075684,0.18711053133010863,0.1860891342163086,0.18570610284805297,0.20030369758605956,0.1806489944458008,0.1850113868713379,0.18661463260650635,0.1831912398338318,0.18593988418579102,0.18533389568328856,0.18018953800201415,0.18675320148468016,0.18814637660980224,0.18309224843978883,0.18914501667022704,0.17307753562927247,0.18845973014831544,0.18555495738983155,0.18007256984710693,0.1830942749977112,0.18984777927398683,0.189388644695282,0.18541816473007203,0.1819467306137085,0.1835179328918457,0.1816352128982544,0.19451507329940795,0.18146278858184814,0.17902793884277343,0.18433911800384523,0.18679803609848022,0.1839076519012451,0.1904909610748291,0.18394442796707153,0.1820732593536377,0.18006540536880494,0.18464868068695067,0.1847563624382019,0.17979238033294678,0.19112465381622315,0.18440158367156984,0.19187890291213988,0.1847402811050415,0.18341355323791503,0.17854316234588624,0.18559021949768068,0.18415913581848145,0.18509920835494995,0.18253446817398072,0.19067952632904053,0.177125883102417,0.19094748497009278,0.18838627338409425,0.18384681940078734,0.18138872385025023,0.1902800679206848,0.18503384590148925,0.18862125873565674,0.18546451330184938,0.18739602565765381,0.18537987470626832,0.18732360601425171,0.1891002655029297,0.1852677583694458,0.17690937519073485,0.17547273635864258,0.18699407577514648,0.1903218984603882,0.18623459339141846,0.17397855520248412,0.18437793254852294,0.1771925687789917,0.18392786979675294,0.18025445938110352,0.17990598678588868,0.19807653427124022,0.18118789196014404,0.18032431602478027,0.19079614877700807,0.19293107986450195,0.1832227110862732,0.18405299186706542,0.18780198097229003,0.18397458791732788,0.17984321117401122,0.18578484058380126,0.18058629035949708,0.18177815675735473,0.1728561282157898,0.18645333051681517,0.18490052223205566,0.18532674312591552,0.18114712238311767,0.1797979712486267,0.18020522594451904,0.17706433534622193]};

  var data = [data0];
  var layout = {"title":"loss by time"};

  Plotly.plot('plot-1915618569', data, layout);
})();
});
      
</script>
</div>

</div>

<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>The accuracy is 37.0%
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Summary">Summary<a class="anchor-link" href="#Summary">&#182;</a></h2><p>We have learned the follows in this article:</p>
<ul>
<li>Mini-Batch Gradient Descent</li>
<li>epoch</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://github.com/izhangzhihao/deeplearning-tutorial/blob/2.0.x/src/main/scala/com/github/izhangzhihao/MiniBatchGradientDescent.scala">Source code</a></p>

</div>
</div>
</div>
 


                        <div class="spacing"></div>
                    </div>
                </section>
            </div>

        </div>
    </div>
</div>


	<!-- Tweet -->
	<div id="tweet">
		<div class="container">
			<section>
				<blockquote>
					DeepLearning.scala is an open source deep-learning toolkit in Scala created by our colleagues at ThoughtWorks.
					We're excited about this project because it uses differentiable functional programming to create and compose neural networks; a developer simply writes code in Scala with static typing.
				</blockquote>
			</section>
		</div>
	</div>


<!-- Footer -->
<div id="footer">
	<div class="container">
		<section>
			<header>
				<h2>Get in touch</h2>
				<!-- <span class="byline">Integer sit amet pede vel arcu aliquet pretium</span> -->
			</header>
			<ul class="contact">
				<li><a href="https://github.com/ThoughtWorksInc/DeepLearning.scala" title="Github" class="fa fa-github"><span>Github</span></a></li>
				<li><a href="https://gitter.im/ThoughtWorksInc/DeepLearning.scala" title="Gitter" style="width: 20px">
					<svg
						version="1.1" 
						xmlns="http://www.w3.org/2000/svg"
						xmlns:xlink="http://www.w3.org/1999/xlink"
						viewBox="0 0 14 20"
					>
						<rect x="12" y="4"width="2" height="8"/>
						<rect x="8" y="4" width="2" height="16"/>
						<rect x="4" y="4"width="2" height="16"/>
						<rect width="2" height="12"/>
					</svg>
					<span>Gitter</span>
				</a></li>
				<li><a href="mailto:tw-data-china@thoughtworks.com" title="Email" class="fa fa-envelope-o"><span>Email</span></a></li>
			</ul>
		</section>
	</div>
</div>

<!-- Copyright -->
<div id="copyright">
	<div class="container">
		© 2017 ThoughtWorks, Inc.
	</div>
</div>

</body>
</html>
