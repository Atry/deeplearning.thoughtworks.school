{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is a great tool that helps us efficiently summarize inherent patterns from tons of input data. I'd like to introduce DeepLearning.scala by letting the framework learn the common difference from Arithmetic progression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "\n",
    "**Input**:\n",
    " Arithmetic progression(AP) as:\n",
    "``` val input: INDArray = Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray``` \n",
    "\n",
    "\n",
    "**Output**: \n",
    " Common Difference of the certain AP as: \n",
    "```val expectedOutput: INDArray = Array(Array(1), Array(3), Array(2)).toNDArray```\n",
    "\n",
    "So here we want DeepLearning.scala to learn the common difference from the AP, i.e. ```{1} from {0, 1, 2} ``` \n",
    "in which `2-1 = 1-0 = 1 `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install DeepLearning.scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepLearning.scala is hosted on Maven Central repository.\n",
    "\n",
    "You can use magic imports in [jupyter-scala](https://github.com/alexarchambault/jupyter-scala) or [Ammonite-REPL](http://www.lihaoyi.com/Ammonite/#Ammonite-REPL) to download DeepLearning.scala and its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$plugin.$                                                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                       \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                       \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                          \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                         \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                         \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                            \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $plugin.$ivy.`com.thoughtworks.implicit-dependent-type::implicit-dependent-type:2.0.0`\n",
    "\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableany:1.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablenothing:1.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableseq:1.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiabledouble:1.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablefloat:1.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablehlist:1.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablecoproduct:1.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableindarray:1.0.0`\n",
    "\n",
    "import $ivy.`org.nd4j:nd4j-native-platform:0.7.2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use [sbt](http://www.scala-sbt.org), please add the following settings into your `build.sbt`:\n",
    "\n",
    "```\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiableany\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiablenothing\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiableseq\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiabledouble\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiablefloat\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiablehlist\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiablecoproduct\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiableindarray\" % \"latest.release\"\n",
    "\n",
    "addCompilerPlugin(\"com.thoughtworks.implicit-dependent-type\" %% \"implicit-dependent-type\" % \"latest.release\")\n",
    "\n",
    "addCompilerPlugin(\"org.scalamacros\" % \"paradise\" % \"2.1.0\" cross CrossVersion.full)\n",
    "\n",
    "fork := true\n",
    "\n",
    "scalaVersion := \"2.11.8\"\n",
    "```\n",
    "\n",
    "Note that this example must run on Scala 2.11.8 or Scala 2.10.6 because [nd4s](http://nd4j.org/scala) does not support Scala 2.12. Make sure there is not a setting like `scalaVersion := \"2.12.1\"` in your `build.sbt`.\n",
    "\n",
    "See [Scaladex](https://index.scala-lang.org/thoughtworksinc/deeplearning.scala) to install DeepLearning.scala in other build tools!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you may want to import classes in DeepLearning.scala and its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableHList._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableDouble._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableAny._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableHList\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Layer\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathFunctions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathOps\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.factory.Nd4j\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4s.Implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mshapeless._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.thoughtworks.deeplearning.DifferentiableHList._\n",
    "import com.thoughtworks.deeplearning.DifferentiableDouble._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray._\n",
    "import com.thoughtworks.deeplearning.DifferentiableAny._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
    "import com.thoughtworks.deeplearning.Symbolic._\n",
    "import com.thoughtworks.deeplearning.DifferentiableHList\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray\n",
    "import com.thoughtworks.deeplearning.Layer\n",
    "import com.thoughtworks.deeplearning.Symbolic\n",
    "import com.thoughtworks.deeplearning.Poly.MathFunctions._\n",
    "import com.thoughtworks.deeplearning.Poly.MathOps\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.nd4s.Implicits._\n",
    "import shapeless._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design the neural network\n",
    "\n",
    "DeepLearning.scala is also a language that we can use to create complex neural networks.\n",
    "\n",
    "In the following sections, you will learn:\n",
    " * how to define types for a neural network\n",
    " * how to use a neural network as a predictor\n",
    " * how to create a neural network\n",
    " * how to train a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The type of neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like a `scala.Function`, a neural network has its own input types and output types.\n",
    "\n",
    "For example, the type of the neural network that accepts an N-dimensional array and returns another N-dimensional array is `(INDArray => INDArray)@Symbolic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmyNeuralNetwork\u001b[39m: (\u001b[32mSymbolic\u001b[39m.\u001b[32mFromTo\u001b[39m[\u001b[32mINDArray\u001b[39m, \u001b[32mINDArray\u001b[39m]{type InputData = org.nd4j.linalg.api.ndarray.INDArray;type InputDelta = org.nd4j.linalg.api.ndarray.INDArray;type OutputData = org.nd4j.linalg.api.ndarray.INDArray;type OutputDelta = org.nd4j.linalg.api.ndarray.INDArray})#\u001b[32m@\u001b[39m = null"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var myNeuralNetwork: (INDArray => INDArray)@Symbolic = _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `(A => B)@Symbolic`, A is the input type, and B is the output type. For the example above, both the input type and the output type are `INDArray`.\n",
    "\n",
    "\n",
    "`@Symbolic` is a syntactic sugar to create implicit dependent types. See [implicit-dependent-type](https://github.com/ThoughtWorksInc/implicit-dependent-type) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a neural network as a predictor\n",
    "\n",
    "Like a normal `scala.Function`, if you pass the input data to the neural network, it will return some results.\n",
    "You can use the `predict` method to invoke a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31mjava.lang.NullPointerException\u001b[39m",
      "  com.thoughtworks.deeplearning.DifferentiableAny$AnyLayerOps$$anonfun$predict$1.apply(\u001b[32mDifferentiableAny.scala\u001b[39m:\u001b[32m107\u001b[39m)",
      "  com.thoughtworks.deeplearning.DifferentiableAny$AnyLayerOps$$anonfun$predict$1.apply(\u001b[32mDifferentiableAny.scala\u001b[39m:\u001b[32m107\u001b[39m)",
      "  resource.DefaultManagedResource.open(\u001b[32mAbstractManagedResource.scala\u001b[39m:\u001b[32m110\u001b[39m)",
      "  resource.AbstractManagedResource.acquireFor(\u001b[32mAbstractManagedResource.scala\u001b[39m:\u001b[32m87\u001b[39m)",
      "  resource.ManagedResourceOperations$class.apply(\u001b[32mManagedResourceOperations.scala\u001b[39m:\u001b[32m26\u001b[39m)",
      "  resource.AbstractManagedResource.apply(\u001b[32mAbstractManagedResource.scala\u001b[39m:\u001b[32m50\u001b[39m)",
      "  resource.ManagedResourceOperations$class.acquireAndGet(\u001b[32mManagedResourceOperations.scala\u001b[39m:\u001b[32m25\u001b[39m)",
      "  resource.AbstractManagedResource.acquireAndGet(\u001b[32mAbstractManagedResource.scala\u001b[39m:\u001b[32m50\u001b[39m)",
      "  com.thoughtworks.deeplearning.DifferentiableAny$AnyLayerOps.predict(\u001b[32mDifferentiableAny.scala\u001b[39m:\u001b[32m107\u001b[39m)",
      "  $sess.cmd3Wrapper$Helper.<init>(\u001b[32mcmd3.sc\u001b[39m:\u001b[32m2\u001b[39m)",
      "  $sess.cmd3Wrapper.<init>(\u001b[32mcmd3.sc\u001b[39m:\u001b[32m510\u001b[39m)",
      "  $sess.cmd3$.<init>(\u001b[32mcmd3.sc\u001b[39m:\u001b[32m487\u001b[39m)",
      "  $sess.cmd3$.<clinit>(\u001b[32mcmd3.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "val input: INDArray = Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray\n",
    "val predictionResult: INDArray = myNeuralNetwork.predict(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code throws a `NullPointerException` because `myNeuralNetwork` has not been initialized.\n",
    "We will fix the problem by creating a valid neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a neural network\n",
    "\n",
    "Same as the definition of a normal Scala function, the definition of neural network consists of a type definition for its parameter, a type definition for its return value, and a body that contains mathematical formulas, function-calls, and control flows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight Intialization \n",
    "\n",
    "A neural network is trainable.\n",
    "It means that some variables in the neural network can be changed automatically according to some goals. Those variables are called `weight`.\n",
    "You can create weight variables via `toWeight` method, given its initial value.\n",
    "\n",
    "In order to create a weight, you must create an `Optimizer`, which contains the rule that manages how the weight changes. See [Scaladoc](https://javadoc.io/page/com.thoughtworks.deeplearning/unidoc_2.11/latest/com/thoughtworks/deeplearning/DifferentiableINDArray$$Optimizers$.html) for a list of built-in optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36moptimizer\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit def optimizer: Optimizer = new LearningRate {\n",
    "  def currentLearningRate() = 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `@Symbolic` placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcreateMyNeuralNetwork\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createMyNeuralNetwork(implicit input: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val initialValueOfWeight: INDArray = Nd4j.randn(3, 1) / math.sqrt(3.0)\n",
    "  val weight: INDArray @Symbolic = initialValueOfWeight.toWeight\n",
    "  input dot weight\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `createMyNeuralNetwork` method is a **symbolic method**.\n",
    "When you call the `createMyNeuralNetwork` method, you have not actually evaluated it yet.\n",
    "In fact, you only build its structure.\n",
    "Variables in the neural network are placeholders,\n",
    "which will be replaced with actual values in the future training or prediction process.\n",
    "\n",
    "Generally, any method that contains an `implicit` parameter that has a `@Symbolic` type is a **symbolic method**.\n",
    "You can create symbolic methods in the following form:\n",
    "\n",
    "```\n",
    "def yourSymbolicMethod(implicit input: InputType @Symbolic): OutputType @Symbolic = {\n",
    "  ???\n",
    "}\n",
    "```\n",
    "\n",
    "The `@Symbolic` annotations in symbolic methods create placeholder types for parameters, return values and other local variables.\n",
    "\n",
    "Note that the input parameter must be `implicit` so that it is automatically generated when you invoke the the symbolic method. Otherwise, you have to manually pass the `@Symbolic` annotated placeholder to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a new neural network and assign it to `myNeuralNetwork`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myNeuralNetwork = createMyNeuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your Neural Network\n",
    "\n",
    "You have learned that weight will be automatically changed due to some goals.\n",
    "\n",
    "In DeepLearning.scala, when we train a neural network, our goal should always be minimizing the absolute of the return value.\n",
    "\n",
    "For example, if someone repeatedly call `myNeuralNetwork.train(Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray)`,\n",
    "the neural network would try to minimize `input dot weight`.\n",
    "Soon `weight` would become an array of zeros in order to make `input dot weight` zeros,\n",
    "and `myNeuralNetwork.predict(Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray)` would return `Array(Array(0), Array(0), Array(0)).toNDArray`.\n",
    "\n",
    "What if you expect `myNeuralNetwork.predict(Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray)` to return `Array(Array(1), Array(3), Array(2)).toNDArray`?\n",
    "\n",
    "You can create another neural network that evaluates how far between the result of `myNeuralNetwork` and your expectation. The new neural network is usually called **loss function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mlossFunction\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lossFunction(implicit pair: (INDArray :: INDArray :: HNil) @Symbolic): Double @Symbolic = {\n",
    "  val input = pair.head\n",
    "  val expectedOutput = pair.tail.head\n",
    "  abs(myNeuralNetwork.compose(input) - expectedOutput).sum\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the `lossFunction` get trained continuously, its return value will be close to zero, and the result of  `myNeuralNetwork` must be close to the expected result at the same time.\n",
    "\n",
    "Note the `lossFunction` accepts a placehold of `INDArray :: INDArray :: HNil` as its parameter, which is  a [shapeless](https://github.com/milessabin/shapeless)'s `HList` type.\n",
    "The `HList` consists of two N-dimensional arrays.\n",
    "The first array is the input data used to train the neural network, and the second array is the expected output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we hard-code some data to train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36minput\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 1.00, 2.00],\n",
       " [3.00, 6.00, 9.00],\n",
       " [13.00, 15.00, 17.00]]\n",
       "\u001b[36mexpectedOutput\u001b[39m: \u001b[32mINDArray\u001b[39m = [1.00, 3.00, 2.00]\n",
       "\u001b[36mNumberOfIterations\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m400\u001b[39m\n",
       "\u001b[36mlossSeq\u001b[39m: \u001b[32mcollection\u001b[39m.\u001b[32mimmutable\u001b[39m.\u001b[32mIndexedSeq\u001b[39m[\u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m.\u001b[32m<refinement>\u001b[39m.this.type.\u001b[32mOutputData\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m11.161251068115234\u001b[39m,\n",
       "  \u001b[32m9.637250900268555\u001b[39m,\n",
       "  \u001b[32m8.113250732421875\u001b[39m,\n",
       "  \u001b[32m6.589250564575195\u001b[39m,\n",
       "  \u001b[32m5.065250396728516\u001b[39m,\n",
       "  \u001b[32m3.541250467300415\u001b[39m,\n",
       "  \u001b[32m2.1384034156799316\u001b[39m,\n",
       "  \u001b[32m2.5212504863739014\u001b[39m,\n",
       "  \u001b[32m2.4424033164978027\u001b[39m,\n",
       "  \u001b[32m2.242403507232666\u001b[39m,\n",
       "  \u001b[32m2.042403221130371\u001b[39m,\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val input: INDArray = Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray\n",
    "val expectedOutput: INDArray = Array(Array(1), Array(3), Array(2)).toNDArray\n",
    "\n",
    "val NumberOfIterations = 400\n",
    "\n",
    "val lossSeq = for (iteration <- 0 until NumberOfIterations) yield {\n",
    "  lossFunction.train(input :: expectedOutput :: HNil)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After those iterations, the loss should close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.1507331132888794\n"
     ]
    }
   ],
   "source": [
    "println(s\"loss: ${ lossFunction.predict(input :: expectedOutput :: HNil) }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction result should close to Array(Array(0), Array(3), Array(1)).toNDArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: [1.00, 3.22, 2.93]\n"
     ]
    }
   ],
   "source": [
    "println(s\"result: ${ myNeuralNetwork.predict(input) }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a plot showing the loss changes during iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.plotly-scala::plotly-jupyter-scala:0.3.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.JupyterScala._\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.JupyterScala._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <script type=\"text/javascript\">\n",
       "        require.config({\n",
       "  paths: {\n",
       "    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',\n",
       "    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min'\n",
       "  },\n",
       "\n",
       "  shim: {\n",
       "    plotly: {\n",
       "      deps: ['d3', 'jquery'],\n",
       "      exports: 'plotly'\n",
       "    }\n",
       "  }\n",
       "});\n",
       "        \n",
       "\n",
       "        require(['plotly'], function(Plotly) {\n",
       "          window.Plotly = Plotly;\n",
       "        });\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-1668384299\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "requirejs([\"plotly\"], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"type\":\"scatter\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0],\"y\":[11.161251068115234,9.637250900268555,8.113250732421875,6.589250564575195,5.065250396728516,3.541250467300415,2.1384034156799316,2.5212504863739014,2.4424033164978027,2.242403507232666,2.042403221130371,2.5092501640319824,2.346403121948242,2.1464033126831055,1.993249773979187,2.4504034519195557,2.250403642654419,2.050403594970703,1.9812511205673218,2.354403018951416,2.1544029712677,1.9544029235839844,1.969252109527588,2.2584028244018555,2.0584030151367188,1.8584028482437134,1.9572535753250122,2.162402629852295,1.962402582168579,1.7624022960662842,1.9452550411224365,2.0664021968841553,1.866402268409729,1.6664021015167236,1.9332560300827026,1.9704017639160156,1.7704018354415894,1.570401668548584,1.921257734298706,1.874401569366455,1.6744015216827393,1.474401593208313,1.9092587232589722,1.7784011363983154,1.5784008502960205,1.3932600021362305,1.882400631904602,1.6824007034301758,1.4824004173278809,1.3812615871429443,1.7864006757736206,1.5864003896713257,1.3864002227783203,1.3692622184753418,1.6904003620147705,1.4904004335403442,1.2904000282287598,1.3572635650634766,1.59440016746521,1.3944001197814941,1.1943999528884888,1.3452637195587158,1.4984002113342285,1.2984001636505127,1.0983999967575073,1.3332644701004028,1.402400016784668,1.2024002075195312,1.0024006366729736,1.3212651014328003,1.3064005374908447,1.1064002513885498,0.9064002633094788,1.3092663288116455,1.210400104522705,1.0103998184204102,0.8104007244110107,1.297266960144043,1.1144005060195923,0.9144002795219421,0.7812671661376953,1.218400239944458,1.0184003114700317,0.8184002637863159,0.7692682147026062,1.1224007606506348,0.9224002957344055,0.7224005460739136,0.7572686076164246,1.0264008045196533,0.8264008164405823,0.62639981508255,0.7452691793441772,0.9330591559410095,0.5892683267593384,1.069059133529663,0.5064006447792053,0.937268853187561,0.8104004263877869,0.6104001998901367,0.4212701916694641,1.1810590028762817,0.45040106773376465,0.7692697644233704,0.8530584573745728,0.6132698655128479,0.9890583753585815,0.45726990699768066,1.1250594854354858,0.37040066719055176,0.8052693009376526,0.7970595955848694,0.6492681503295898,0.9330604672431946,0.4932681918144226,1.0690603256225586,0.33726853132247925,1.2050600051879883,0.33040130138397217,0.6852678656578064,0.8770601153373718,0.5292679071426392,1.013061285018921,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973,1.1507331132888794,0.3732665181159973]};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"loss by time\"};\n",
       "\n",
       "  Plotly.plot('plot-1668384299', data, layout);\n",
       "})();\n",
       "});\n",
       "      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mplot\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mScatter\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mScatter\u001b[39m(\n",
       "    \u001b[33mSome\u001b[39m(\n",
       "      \u001b[33mDoubles\u001b[39m(\n",
       "        \u001b[33mVector\u001b[39m(\n",
       "          \u001b[32m0.0\u001b[39m,\n",
       "          \u001b[32m1.0\u001b[39m,\n",
       "          \u001b[32m2.0\u001b[39m,\n",
       "          \u001b[32m3.0\u001b[39m,\n",
       "          \u001b[32m4.0\u001b[39m,\n",
       "          \u001b[32m5.0\u001b[39m,\n",
       "          \u001b[32m6.0\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mres13_2\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-1668384299\"\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotly.JupyterScala.init()\n",
    "val plot = Seq(\n",
    "  Scatter(\n",
    "   0 until NumberOfIterations,\n",
    "   lossSeq\n",
    "  )\n",
    ")\n",
    "plot.plot(\n",
    "  title = \"loss by time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this article, you have learned:\n",
    "* to create neural networks dealing with complex data structures like `Double`, `INDArray` and `HList` like ordinary programming language\n",
    "* to compose a neural network into a larger neural network\n",
    "* to train a neural network\n",
    "* to use a neural network as a predictor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
